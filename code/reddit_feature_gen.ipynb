{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare entity-tagged Reddit posts into features for disease prediction. Walk through `reddit_validation_embed.ipynb` first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "import pandas as pd\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # included for convenience to help find correct paths\n",
    "# import os\n",
    "# os.getcwd()\n",
    "# os.listdir(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "MEDRED_REPRODUCIBLE_DIR = \"../\"\n",
    "SYMPTOMS_IN = MEDRED_REPRODUCIBLE_DIR + \"data/validation/Reddit/NER_Reddit_pred_dis.csv\" # sym_file\n",
    "DRUGS_IN = MEDRED_REPRODUCIBLE_DIR + \"data/validation/Reddit/NER_Reddit_pred_drug.csv\" # drug_file\n",
    "# are features for the DL implementation or MetaMap?\n",
    "EMBEDDING_TYPE = \"DL\"\n",
    "# output path\n",
    "FEATURES_OUT = MEDRED_REPRODUCIBLE_DIR + \"data/validation/Reddit/\" + EMBEDDING_TYPE + \"_embedded_features.pckl\"\n",
    "# test run? T/F\n",
    "IS_SAMPLE_RUN = False\n",
    "\n",
    "\n",
    "\n",
    "# sym_file = \"data/entities/{}/{}_symptom_mappings.csv\".format(etype, etype)\n",
    "# drug_file = \"data/entities/{}/{}_drugs_mappings.csv\".format(etype, etype)\n",
    "# features_file = \"data/features/{}_embdedded_features{}.pckl\".format(etype, sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity-tagged posts loaded: 446202\n"
     ]
    }
   ],
   "source": [
    "all_sr = ['bpd', 'cfs','crohnsdisease', 'dementia',  'depression',\\\n",
    "                    'diabetes', 'dysautonomia', 'gastroparesis','hypothyroidism', 'ibs', \\\n",
    "                    'interstitialcystitis', 'kidneystones', 'menieres', 'multiplesclerosis',\\\n",
    "                    'parkinsons', 'psoriasis', 'rheumatoid', 'sleepapnea']\n",
    "\n",
    "sym = pd.read_csv(SYMPTOMS_IN) # ,subreddit,matched,UID,norm_UID,post_index,score\n",
    "drug = pd.read_csv(DRUGS_IN) # ,post_index,subreddit,matched,norm_UID,UID,score\n",
    "\n",
    "if IS_SAMPLE_RUN:\n",
    "    df = sym.append(drug).sample(n=1000, random_state=7)\n",
    "else:\n",
    "    df = sym.append(drug)\n",
    "print (\"Entity-tagged posts loaded:\", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_from_tokens(row):\n",
    "    '''\n",
    "    Get average embedding for all tokens in a post\n",
    "    '''\n",
    "    # get list of term-tagged tokens\n",
    "    tokens = nlp(row)\n",
    "    # init\n",
    "    vec = []\n",
    "    \n",
    "    # removed, since not used anywhere else; scalar to halve the mean? why?\n",
    "    #   word_emb_len initally set to 300, matches # embeddings per post\n",
    "    #   set as a constant right above original function - a parameter from data prep?\n",
    "    # vec.append(np.zeros(word_emb_len))\n",
    "\n",
    "    # for each token, append a list of its embeddings\n",
    "    for token in tokens:\n",
    "        if token.has_vector:\n",
    "            vec.append(token.vector)\n",
    "    # then make a list of mean embedding values fo reach token\n",
    "    vec = np.mean(vec, axis=0)\n",
    "    # handlign for no embedding cases\n",
    "    if isinstance(vec, np.ndarray):\n",
    "        vec = vec.tolist()\n",
    "    else:\n",
    "        vec = []\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define feature preparation code for different cutoffs of embedding fit. Fit one set at 0.9, as the value used in the original work is not specified. Datasets based on other values (as seen in the original feature preparation code) are derived after.\n",
    "\n",
    "Runtime for this one ~12 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total posts with entities over threshold 136480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alex\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>entities</th>\n",
       "      <th>vec</th>\n",
       "      <th>disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>diabetes</td>\n",
       "      <td>CGM</td>\n",
       "      <td>[-0.36256998777389526, -0.002186200115829706, ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>depression</td>\n",
       "      <td>depression, depression</td>\n",
       "      <td>[-0.170890673995018, 0.7456066608428955, 0.023...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bpd</td>\n",
       "      <td>BPD, BPD</td>\n",
       "      <td>[-0.20788399875164032, 0.8302733302116394, -0....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>depression</td>\n",
       "      <td>depression, depression, depression, depression</td>\n",
       "      <td>[-0.15829943120479584, 0.7350971102714539, -0....</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>depression</td>\n",
       "      <td>broken arm</td>\n",
       "      <td>[-0.041669994592666626, -0.1625255048274994, -...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136475</th>\n",
       "      <td>ibs</td>\n",
       "      <td>IBS</td>\n",
       "      <td>[-0.645550012588501, 1.003600001335144, -0.554...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136476</th>\n",
       "      <td>bpd</td>\n",
       "      <td>paranoid delusions, paranoia</td>\n",
       "      <td>[-0.29793548583984375, -0.1916699856519699, 0....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136477</th>\n",
       "      <td>depression</td>\n",
       "      <td>clonazepam, clonazepam</td>\n",
       "      <td>[-0.13348400592803955, 0.420906662940979, -0.1...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136478</th>\n",
       "      <td>depression</td>\n",
       "      <td>stress</td>\n",
       "      <td>[-0.5926600098609924, 0.8686400055885315, -0.2...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136479</th>\n",
       "      <td>depression</td>\n",
       "      <td>depressed</td>\n",
       "      <td>[0.11964999884366989, 0.5301600098609924, -0.0...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136480 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         subreddit                                        entities  \\\n",
       "0         diabetes                                             CGM   \n",
       "1       depression                          depression, depression   \n",
       "2              bpd                                        BPD, BPD   \n",
       "3       depression  depression, depression, depression, depression   \n",
       "4       depression                                      broken arm   \n",
       "...            ...                                             ...   \n",
       "136475         ibs                                             IBS   \n",
       "136476         bpd                    paranoid delusions, paranoia   \n",
       "136477  depression                          clonazepam, clonazepam   \n",
       "136478  depression                                          stress   \n",
       "136479  depression                                       depressed   \n",
       "\n",
       "                                                      vec  disease  \n",
       "0       [-0.36256998777389526, -0.002186200115829706, ...        5  \n",
       "1       [-0.170890673995018, 0.7456066608428955, 0.023...        4  \n",
       "2       [-0.20788399875164032, 0.8302733302116394, -0....        0  \n",
       "3       [-0.15829943120479584, 0.7350971102714539, -0....        4  \n",
       "4       [-0.041669994592666626, -0.1625255048274994, -...        4  \n",
       "...                                                   ...      ...  \n",
       "136475  [-0.645550012588501, 1.003600001335144, -0.554...        9  \n",
       "136476  [-0.29793548583984375, -0.1916699856519699, 0....        0  \n",
       "136477  [-0.13348400592803955, 0.420906662940979, -0.1...        4  \n",
       "136478  [-0.5926600098609924, 0.8686400055885315, -0.2...        4  \n",
       "136479  [0.11964999884366989, 0.5301600098609924, -0.0...        4  \n",
       "\n",
       "[136480 rows x 4 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def save_features_with_certainty(df, certainty, features_file=FEATURES_OUT, embedding_type=EMBEDDING_TYPE):\n",
    "    '''\n",
    "    Process features at different score (certainty) cutoffs.\n",
    "    '''\n",
    "    raw_features = df[[\"subreddit\", \"matched\", \"post_index\", \"score\"]]\n",
    "    raw_features = raw_features[ (raw_features[\"score\"].astype(float) > certainty) ]\n",
    "    # If no values above cutoff, stop early - nothign to write\n",
    "    if not len(raw_features):\n",
    "        return\n",
    "\n",
    "    # additional cleanup based on NER model type\n",
    "    if embedding_type == \"DL\":\n",
    "        #raw_features['matched'] = raw_features['matched'].apply(','.join)\n",
    "        pass\n",
    "    elif embedding_type == \"MM\":\n",
    "        raw_features['matched'] = raw_features['matched'].apply(ast.literal_eval)\n",
    "        raw_features['matched'] = raw_features['matched'].apply(' '.join)\n",
    "    else:\n",
    "        print (\"Non-existent entitiy type, please try again. \")\n",
    "        sys.exit()\n",
    "\n",
    "    # format, aggregating entities to post level (comma separated)\n",
    "    raw_features = raw_features.rename(columns={'matched':'entities'})\n",
    "    #   format, aggregating only when there are any tags\n",
    "    def join_entities(df):\n",
    "        if df.shape[0] > 1:\n",
    "            df = df.dropna(inplace=False)\n",
    "        return ', '.join(df)\n",
    "    raw_features = raw_features.groupby(['post_index','subreddit'])['entities'].apply(join_entities).reset_index()\n",
    "    raw_features = raw_features.drop(columns=['post_index'])\n",
    "    print(\"Total posts with entities over threshold\", len(raw_features))\n",
    "    \n",
    "    # append file name with certainty level\n",
    "    features_file = features_file.replace(\".pckl\", \"_{:.2f}.pckl\".format(certainty))\n",
    "\n",
    "    # cast, in order to add vectors to cells\n",
    "    object_features = raw_features.astype(object)\n",
    "\n",
    "    # attach embeddings\n",
    "    object_features['vec'] = object_features['entities'].apply(embedding_from_tokens)\n",
    "    embedding_vec_list = object_features['vec'].tolist()\n",
    "    embedding_vec_list = pd.DataFrame(embedding_vec_list)\n",
    "    features = object_features.copy()\n",
    "\n",
    "    # tag post with associated disease based on subreddit\n",
    "    disease_values_dict = {el:i for i, el in enumerate(all_sr)}\n",
    "    # # these will be used to take disease names for each prediction task\n",
    "    # disease_names = list(disease_values_dict.keys())\n",
    "    # disease_labels = list(disease_values_dict.values())\n",
    "    s = pd.DataFrame()\n",
    "    s['disease'] = features.apply(lambda x: disease_values_dict[x['subreddit']], axis=1)\n",
    "\n",
    "    features = features.join(s)\n",
    "    features.to_pickle(features_file)\n",
    "    return features\n",
    "\n",
    "out = save_features_with_certainty(df, 0.9, features_file=FEATURES_OUT, embedding_type=EMBEDDING_TYPE)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive the rest. Lower iterations will take longer, since less terms will be filtered out at lower certainty thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total posts with entities over threshold 171000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alex\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total posts with entities over threshold 170998\n",
      "Total posts with entities over threshold 170841\n",
      "Total posts with entities over threshold 170088\n",
      "Total posts with entities over threshold 168303\n",
      "Total posts with entities over threshold 164717\n",
      "Total posts with entities over threshold 159297\n",
      "Total posts with entities over threshold 149941\n",
      "Total posts with entities over threshold 136480\n"
     ]
    }
   ],
   "source": [
    "# 5-10 minutes each\n",
    "for certainty in np.linspace(0.1,1,9, endpoint=False):\n",
    "    save_features_with_certainty(df, certainty, features_file=FEATURES_OUT, embedding_type=EMBEDDING_TYPE)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9c1d87ea85a731d0848cf4e607dbb57360506d747d84c6cb1b30d4e34e6c1fb7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
