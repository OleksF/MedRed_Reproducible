{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98011dd5",
   "metadata": {},
   "source": [
    "Datasets\n",
    "\n",
    "- MicroMed: https://github.com/IBMMRL/medinfo2015\n",
    "- CADEC: https://data.csiro.au/collection/csiro:10948?q=CADEC&_st=keyword&_str=1&_si=1\n",
    "- MedRed and Reddit 500k: https://figshare.com/articles/dataset/MedRed/12039609/1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d498bf",
   "metadata": {},
   "source": [
    "Train models used in the paper.\n",
    "\n",
    "**Before running** make sure to go through data preparation notebooks `micromed_prep.ipynb`, `cadec_prep.ipynb`, and `NER_prep.ipynb` to obtain and preprocess data into correct inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d1fbdd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ad9635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import ColumnCorpus\n",
    "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, PooledFlairEmbeddings, TransformerWordEmbeddings\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer\n",
    "from typing import List\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0ed324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # included for convenience to help find correct paths\n",
    "# import os\n",
    "# os.getcwd()\n",
    "# os.listdir(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be4c718",
   "metadata": {},
   "source": [
    "Constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "857740f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEDRED_REPRODUCIBLE_DIR = \"../\"\n",
    "MEDRED_LABELS_DIR = MEDRED_REPRODUCIBLE_DIR + \"data/AMT/labels\"\n",
    "MICROMED_LABELS_DIR = MEDRED_REPRODUCIBLE_DIR + \"data/Micromed\"\n",
    "CADEC_LABELS_DIR = MEDRED_REPRODUCIBLE_DIR + \"data/cadec\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d0d70d",
   "metadata": {},
   "source": [
    "Model training code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b301f7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_data(path):\n",
    "\n",
    "\tfiles = os.listdir(path)\n",
    "\tregex = re.compile(\".*(_train|_dev|_test).*\")\n",
    "\tfiles = list(filter(regex.match, files))\n",
    "\n",
    "\t# define columns\n",
    "\tcolumns = {0: 'text', 1: 'ner'}\n",
    "\n",
    "\t# init a corpus using column format, data folder and the names of the train, dev and test files\n",
    "\tcorpus: Corpus = ColumnCorpus(path, columns,\n",
    "\t                              train_file=files[2],\n",
    "\t                              test_file=files[1],\n",
    "\t                              dev_file=files[0])\n",
    "\treturn corpus\n",
    "\n",
    "\n",
    "\n",
    "def train(model, selected_embeddings):\n",
    "\t# 1. load data\n",
    "\tif model == \"MedRed\":\n",
    "\t\tcorpus = read_in_data(MEDRED_LABELS_DIR)\n",
    "\telif model == \"Micromed\":\n",
    "\t\tcorpus = read_in_data(MICROMED_LABELS_DIR)\n",
    "\telif model == \"CADEC\":\n",
    "\t\tcorpus = read_in_data(CADEC_LABELS_DIR)\n",
    "\telse:\n",
    "\t\traise ValueError(\"`model` must be one of MedRec, Micromed, CADEC.\")\t\n",
    "\n",
    "\t# 2. what tag do we want to predict?\n",
    "\ttag_type = 'ner'\n",
    "\n",
    "\t# 3. make the tag dictionary from the corpus\n",
    "\t# modified call, replaced deprecated\n",
    "\ttag_dictionary = corpus.make_label_dictionary(label_type=tag_type)\n",
    "\t# tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
    "\n",
    "\t# 4. select embedding types\n",
    "\tembedding_types: List[TokenEmbeddings] = []\n",
    "\tif selected_embeddings['glove']:\n",
    "\t\tembedding_types.append(WordEmbeddings('glove'))\n",
    "\t# RoBERTa via TransformerWordEmbeddings\n",
    "\tif selected_embeddings['roberta']:\n",
    "\t\tembedding_types.append(TransformerWordEmbeddings('roberta-base')) # modified from old version\n",
    "\t# PooledFlairEmbeddings\n",
    "\tif selected_embeddings['pooled-flair']:\n",
    "\t\tembedding_types.append(PooledFlairEmbeddings('news-forward', pooling='mean'))\n",
    "\t# PooledFlairEmbeddings\n",
    "\tif selected_embeddings['pooled-flair']:\n",
    "\t\tembedding_types.append(PooledFlairEmbeddings('news-backward', pooling='mean'))\n",
    "\t# StackedEmbeddings\n",
    "\tembeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
    "\n",
    "\t# 5. config tagger for sequences\n",
    "\ttagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
    "                                          embeddings=embeddings,\n",
    "                                          tag_dictionary=tag_dictionary,\n",
    "                                          tag_type=tag_type,\n",
    "                                          use_crf=True\n",
    "                                          )\n",
    "\t# 6. set up training object\n",
    "\ttrainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "\t# set up string to label model output, joining embedding types passed in\n",
    "\tselected_embeddings_text = [key  for key in selected_embeddings if selected_embeddings[key]]\n",
    "\tselected_embeddings_text = '_'.join(selected_embeddings_text)\n",
    "\tmodel_dir = MEDRED_REPRODUCIBLE_DIR + 'resources/taggers/FA_' + model + '_' + selected_embeddings_text\n",
    "\n",
    "\t# 7. start training\n",
    "\ttrainer.train(model_dir,\n",
    "\t\t\t\t  train_with_dev=True,\n",
    "\t\t\t\t  learning_rate=0.1,\n",
    "\t\t\t\t  patience=3,\n",
    "\t\t\t\t  anneal_factor=0.5,\n",
    "\t\t\t\t  min_learning_rate=0.0001,\n",
    "\t\t\t\t  mini_batch_size=4,\n",
    "\t\t\t\t  max_epochs=200,\n",
    "\t\t\t\t  write_weights=True,\n",
    "\t\t\t\t  create_loss_file=True,\n",
    "\t\t\t\t  checkpoint=True)\n",
    "\n",
    "\t# 8. plot training curves\n",
    "\tfrom flair.visual.training_curves import Plotter\n",
    "\tplotter = Plotter()\n",
    "\tplotter.plot_training_curves(model_dir + '/loss.tsv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72fb0a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-06 20:52:13,057 Reading data from ..\\data\\AMT\\labels\n",
      "2022-05-06 20:52:13,058 Train: ..\\data\\AMT\\labels\\NER_Reddit_AMT_labels_href_train.csv\n",
      "2022-05-06 20:52:13,059 Dev: ..\\data\\AMT\\labels\\NER_Reddit_AMT_labels_href_dev.csv\n",
      "2022-05-06 20:52:13,059 Test: ..\\data\\AMT\\labels\\NER_Reddit_AMT_labels_href_test.csv\n",
      "2022-05-06 20:52:15,125 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "968it [00:00, 49645.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-06 20:52:15,146 Dictionary created for label 'ner' with 3 values: DIS (seen 1428 times), DRUG (seen 403 times)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-06 20:52:24,203 SequenceTagger predicts: Dictionary with 9 tags: O, S-DIS, B-DIS, E-DIS, I-DIS, S-DRUG, B-DRUG, E-DRUG, I-DRUG\n",
      "2022-05-06 20:52:24,241 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 20:52:24,244 Model: \"SequenceTagger(\n",
      "  (embeddings): StackedEmbeddings(\n",
      "    (list_embedding_0): WordEmbeddings(\n",
      "      'glove'\n",
      "      (embedding): Embedding(400001, 100)\n",
      "    )\n",
      "    (list_embedding_1): TransformerWordEmbeddings(\n",
      "      (model): RobertaModel(\n",
      "        (embeddings): RobertaEmbeddings(\n",
      "          (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "          (token_type_embeddings): Embedding(1, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): RobertaEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (3): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (4): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (5): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (6): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (7): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (8): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (9): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (10): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (11): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (pooler): RobertaPooler(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (activation): Tanh()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=868, out_features=868, bias=True)\n",
      "  (rnn): LSTM(868, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=11, bias=True)\n",
      "  (loss_function): ViterbiLoss()\n",
      "  (crf): CRF()\n",
      ")\"\n",
      "2022-05-06 20:52:24,245 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 20:52:24,247 Corpus: \"Corpus: 968 train + 523 dev + 537 test sentences\"\n",
      "2022-05-06 20:52:24,248 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 20:52:24,249 Parameters:\n",
      "2022-05-06 20:52:24,250  - learning_rate: \"0.100000\"\n",
      "2022-05-06 20:52:24,250  - mini_batch_size: \"4\"\n",
      "2022-05-06 20:52:24,251  - patience: \"3\"\n",
      "2022-05-06 20:52:24,252  - anneal_factor: \"0.5\"\n",
      "2022-05-06 20:52:24,253  - max_epochs: \"200\"\n",
      "2022-05-06 20:52:24,254  - shuffle: \"True\"\n",
      "2022-05-06 20:52:24,255  - train_with_dev: \"True\"\n",
      "2022-05-06 20:52:24,255  - batch_growth_annealing: \"False\"\n",
      "2022-05-06 20:52:24,256 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 20:52:24,257 Model training base path: \"..\\resources\\taggers\\FA_MedRed_glove_roberta\"\n",
      "2022-05-06 20:52:24,258 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 20:52:24,259 Device: cuda:0\n",
      "2022-05-06 20:52:24,260 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 20:52:24,261 Embeddings storage mode: cpu\n",
      "2022-05-06 20:52:24,262 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 20:52:33,946 epoch 1 - iter 37/373 - loss 0.36022785 - samples/sec: 15.29 - lr: 0.100000\n",
      "2022-05-06 20:52:42,941 epoch 1 - iter 74/373 - loss 0.30432361 - samples/sec: 17.50 - lr: 0.100000\n",
      "2022-05-06 20:52:52,974 epoch 1 - iter 111/373 - loss 0.27676199 - samples/sec: 15.53 - lr: 0.100000\n",
      "2022-05-06 20:53:03,835 epoch 1 - iter 148/373 - loss 0.27724496 - samples/sec: 14.27 - lr: 0.100000\n",
      "2022-05-06 20:53:13,438 epoch 1 - iter 185/373 - loss 0.26546302 - samples/sec: 16.26 - lr: 0.100000\n",
      "2022-05-06 20:53:23,761 epoch 1 - iter 222/373 - loss 0.25993299 - samples/sec: 15.05 - lr: 0.100000\n",
      "2022-05-06 20:53:32,958 epoch 1 - iter 259/373 - loss 0.25019141 - samples/sec: 17.09 - lr: 0.100000\n",
      "2022-05-06 20:53:42,600 epoch 1 - iter 296/373 - loss 0.23784730 - samples/sec: 16.20 - lr: 0.100000\n",
      "2022-05-06 20:53:51,056 epoch 1 - iter 333/373 - loss 0.23040934 - samples/sec: 18.65 - lr: 0.100000\n",
      "2022-05-06 20:54:01,357 epoch 1 - iter 370/373 - loss 0.22350734 - samples/sec: 15.12 - lr: 0.100000\n",
      "2022-05-06 20:54:02,241 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 20:54:02,242 EPOCH 1 done: loss 0.2230 - lr 0.100000\n",
      "2022-05-06 20:54:02,243 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 20:54:03,620 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 20:54:12,963 epoch 2 - iter 37/373 - loss 0.13596297 - samples/sec: 15.85 - lr: 0.100000\n",
      "2022-05-06 20:54:22,649 epoch 2 - iter 74/373 - loss 0.15322673 - samples/sec: 16.11 - lr: 0.100000\n",
      "2022-05-06 20:54:31,711 epoch 2 - iter 111/373 - loss 0.14703724 - samples/sec: 17.38 - lr: 0.100000\n",
      "2022-05-06 20:54:41,158 epoch 2 - iter 148/373 - loss 0.15766792 - samples/sec: 16.57 - lr: 0.100000\n",
      "2022-05-06 20:54:50,239 epoch 2 - iter 185/373 - loss 0.15353944 - samples/sec: 17.24 - lr: 0.100000\n",
      "2022-05-06 20:54:59,364 epoch 2 - iter 222/373 - loss 0.14870100 - samples/sec: 17.16 - lr: 0.100000\n",
      "2022-05-06 20:55:07,986 epoch 2 - iter 259/373 - loss 0.14782000 - samples/sec: 18.23 - lr: 0.100000\n",
      "2022-05-06 20:55:16,733 epoch 2 - iter 296/373 - loss 0.14974071 - samples/sec: 17.96 - lr: 0.100000\n",
      "2022-05-06 20:55:26,323 epoch 2 - iter 333/373 - loss 0.14952422 - samples/sec: 16.34 - lr: 0.100000\n",
      "2022-05-06 20:55:34,209 epoch 2 - iter 370/373 - loss 0.14857788 - samples/sec: 20.03 - lr: 0.100000\n",
      "2022-05-06 20:55:35,348 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 20:55:35,349 EPOCH 2 done: loss 0.1483 - lr 0.100000\n",
      "2022-05-06 20:55:35,350 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 20:55:36,799 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 20:55:46,425 epoch 3 - iter 37/373 - loss 0.12433360 - samples/sec: 15.38 - lr: 0.100000\n",
      "2022-05-06 20:55:55,792 epoch 3 - iter 74/373 - loss 0.13355064 - samples/sec: 16.80 - lr: 0.100000\n",
      "2022-05-06 20:56:04,953 epoch 3 - iter 111/373 - loss 0.13478735 - samples/sec: 17.11 - lr: 0.100000\n",
      "2022-05-06 20:56:13,471 epoch 3 - iter 148/373 - loss 0.13468954 - samples/sec: 18.48 - lr: 0.100000\n",
      "2022-05-06 20:56:23,335 epoch 3 - iter 185/373 - loss 0.13144803 - samples/sec: 15.91 - lr: 0.100000\n",
      "2022-05-06 20:56:32,639 epoch 3 - iter 222/373 - loss 0.12856827 - samples/sec: 16.86 - lr: 0.100000\n",
      "2022-05-06 20:56:41,735 epoch 3 - iter 259/373 - loss 0.12741696 - samples/sec: 17.23 - lr: 0.100000\n",
      "2022-05-06 20:56:50,744 epoch 3 - iter 296/373 - loss 0.12529523 - samples/sec: 17.42 - lr: 0.100000\n",
      "2022-05-06 20:56:59,380 epoch 3 - iter 333/373 - loss 0.12605213 - samples/sec: 18.24 - lr: 0.100000\n",
      "2022-05-06 20:57:08,767 epoch 3 - iter 370/373 - loss 0.12574130 - samples/sec: 16.73 - lr: 0.100000\n",
      "2022-05-06 20:57:10,032 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 20:57:10,032 EPOCH 3 done: loss 0.1258 - lr 0.100000\n",
      "2022-05-06 20:57:10,033 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 20:57:11,444 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 20:57:20,090 epoch 4 - iter 37/373 - loss 0.11929924 - samples/sec: 17.13 - lr: 0.100000\n",
      "2022-05-06 20:57:29,642 epoch 4 - iter 74/373 - loss 0.12629722 - samples/sec: 16.37 - lr: 0.100000\n",
      "2022-05-06 20:57:39,408 epoch 4 - iter 111/373 - loss 0.12128579 - samples/sec: 16.01 - lr: 0.100000\n",
      "2022-05-06 20:57:49,221 epoch 4 - iter 148/373 - loss 0.11632357 - samples/sec: 15.94 - lr: 0.100000\n",
      "2022-05-06 20:57:57,835 epoch 4 - iter 185/373 - loss 0.11474154 - samples/sec: 18.29 - lr: 0.100000\n",
      "2022-05-06 20:58:07,114 epoch 4 - iter 222/373 - loss 0.11415221 - samples/sec: 16.86 - lr: 0.100000\n",
      "2022-05-06 20:58:16,914 epoch 4 - iter 259/373 - loss 0.11960915 - samples/sec: 15.94 - lr: 0.100000\n",
      "2022-05-06 20:58:26,161 epoch 4 - iter 296/373 - loss 0.11888152 - samples/sec: 16.94 - lr: 0.100000\n",
      "2022-05-06 20:58:34,485 epoch 4 - iter 333/373 - loss 0.11873839 - samples/sec: 18.92 - lr: 0.100000\n",
      "2022-05-06 20:58:43,546 epoch 4 - iter 370/373 - loss 0.11881150 - samples/sec: 17.31 - lr: 0.100000\n",
      "2022-05-06 20:58:44,813 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 20:58:44,814 EPOCH 4 done: loss 0.1195 - lr 0.100000\n",
      "2022-05-06 20:58:44,815 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 20:58:46,271 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 20:58:54,467 epoch 5 - iter 37/373 - loss 0.13230631 - samples/sec: 18.07 - lr: 0.100000\n",
      "2022-05-06 20:59:03,205 epoch 5 - iter 74/373 - loss 0.11033051 - samples/sec: 18.02 - lr: 0.100000\n",
      "2022-05-06 20:59:12,668 epoch 5 - iter 111/373 - loss 0.10414020 - samples/sec: 16.49 - lr: 0.100000\n",
      "2022-05-06 20:59:21,436 epoch 5 - iter 148/373 - loss 0.10701234 - samples/sec: 17.89 - lr: 0.100000\n",
      "2022-05-06 20:59:31,432 epoch 5 - iter 185/373 - loss 0.10830087 - samples/sec: 15.63 - lr: 0.100000\n",
      "2022-05-06 20:59:39,880 epoch 5 - iter 222/373 - loss 0.10955151 - samples/sec: 18.69 - lr: 0.100000\n",
      "2022-05-06 20:59:50,288 epoch 5 - iter 259/373 - loss 0.10941686 - samples/sec: 14.97 - lr: 0.100000\n",
      "2022-05-06 20:59:58,807 epoch 5 - iter 296/373 - loss 0.11330412 - samples/sec: 18.46 - lr: 0.100000\n",
      "2022-05-06 21:00:08,417 epoch 5 - iter 333/373 - loss 0.11217724 - samples/sec: 16.26 - lr: 0.100000\n",
      "2022-05-06 21:00:16,774 epoch 5 - iter 370/373 - loss 0.11184860 - samples/sec: 18.91 - lr: 0.100000\n",
      "2022-05-06 21:00:17,970 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:00:17,971 EPOCH 5 done: loss 0.1117 - lr 0.100000\n",
      "2022-05-06 21:00:17,972 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 21:00:19,355 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:00:26,769 epoch 6 - iter 37/373 - loss 0.11427171 - samples/sec: 19.97 - lr: 0.100000\n",
      "2022-05-06 21:00:35,247 epoch 6 - iter 74/373 - loss 0.11735444 - samples/sec: 18.55 - lr: 0.100000\n",
      "2022-05-06 21:00:44,554 epoch 6 - iter 111/373 - loss 0.11251991 - samples/sec: 16.85 - lr: 0.100000\n",
      "2022-05-06 21:00:53,473 epoch 6 - iter 148/373 - loss 0.10868672 - samples/sec: 17.63 - lr: 0.100000\n",
      "2022-05-06 21:01:02,760 epoch 6 - iter 185/373 - loss 0.10679186 - samples/sec: 16.89 - lr: 0.100000\n",
      "2022-05-06 21:01:12,273 epoch 6 - iter 222/373 - loss 0.10480846 - samples/sec: 16.48 - lr: 0.100000\n",
      "2022-05-06 21:01:21,906 epoch 6 - iter 259/373 - loss 0.10761639 - samples/sec: 16.23 - lr: 0.100000\n",
      "2022-05-06 21:01:30,066 epoch 6 - iter 296/373 - loss 0.10903919 - samples/sec: 19.37 - lr: 0.100000\n",
      "2022-05-06 21:01:39,403 epoch 6 - iter 333/373 - loss 0.10916366 - samples/sec: 16.74 - lr: 0.100000\n",
      "2022-05-06 21:01:49,378 epoch 6 - iter 370/373 - loss 0.10881873 - samples/sec: 15.61 - lr: 0.100000\n",
      "2022-05-06 21:01:50,678 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:01:50,679 EPOCH 6 done: loss 0.1098 - lr 0.100000\n",
      "2022-05-06 21:01:50,679 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 21:01:52,142 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:01:59,915 epoch 7 - iter 37/373 - loss 0.09952411 - samples/sec: 19.05 - lr: 0.100000\n",
      "2022-05-06 21:02:09,151 epoch 7 - iter 74/373 - loss 0.09480419 - samples/sec: 16.98 - lr: 0.100000\n",
      "2022-05-06 21:02:19,427 epoch 7 - iter 111/373 - loss 0.10364711 - samples/sec: 15.17 - lr: 0.100000\n",
      "2022-05-06 21:02:28,899 epoch 7 - iter 148/373 - loss 0.10589309 - samples/sec: 16.53 - lr: 0.100000\n",
      "2022-05-06 21:02:37,782 epoch 7 - iter 185/373 - loss 0.10622826 - samples/sec: 17.70 - lr: 0.100000\n",
      "2022-05-06 21:02:47,520 epoch 7 - iter 222/373 - loss 0.10561308 - samples/sec: 16.04 - lr: 0.100000\n",
      "2022-05-06 21:02:56,863 epoch 7 - iter 259/373 - loss 0.10625268 - samples/sec: 16.74 - lr: 0.100000\n",
      "2022-05-06 21:03:05,897 epoch 7 - iter 296/373 - loss 0.10568087 - samples/sec: 17.35 - lr: 0.100000\n",
      "2022-05-06 21:03:14,860 epoch 7 - iter 333/373 - loss 0.10443365 - samples/sec: 17.53 - lr: 0.100000\n",
      "2022-05-06 21:03:24,009 epoch 7 - iter 370/373 - loss 0.10484084 - samples/sec: 17.14 - lr: 0.100000\n",
      "2022-05-06 21:03:25,122 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:03:25,123 EPOCH 7 done: loss 0.1050 - lr 0.100000\n",
      "2022-05-06 21:03:25,124 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 21:03:26,570 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:03:34,080 epoch 8 - iter 37/373 - loss 0.10375830 - samples/sec: 19.72 - lr: 0.100000\n",
      "2022-05-06 21:03:43,193 epoch 8 - iter 74/373 - loss 0.10109661 - samples/sec: 17.25 - lr: 0.100000\n",
      "2022-05-06 21:03:52,793 epoch 8 - iter 111/373 - loss 0.09757934 - samples/sec: 16.30 - lr: 0.100000\n",
      "2022-05-06 21:04:02,435 epoch 8 - iter 148/373 - loss 0.10532800 - samples/sec: 16.21 - lr: 0.100000\n",
      "2022-05-06 21:04:11,382 epoch 8 - iter 185/373 - loss 0.10529607 - samples/sec: 17.53 - lr: 0.100000\n",
      "2022-05-06 21:04:21,491 epoch 8 - iter 222/373 - loss 0.10448119 - samples/sec: 15.42 - lr: 0.100000\n",
      "2022-05-06 21:04:30,536 epoch 8 - iter 259/373 - loss 0.10329923 - samples/sec: 17.36 - lr: 0.100000\n",
      "2022-05-06 21:04:39,973 epoch 8 - iter 296/373 - loss 0.10056258 - samples/sec: 16.59 - lr: 0.100000\n",
      "2022-05-06 21:04:49,061 epoch 8 - iter 333/373 - loss 0.10074598 - samples/sec: 17.26 - lr: 0.100000\n",
      "2022-05-06 21:04:57,971 epoch 8 - iter 370/373 - loss 0.10123645 - samples/sec: 17.64 - lr: 0.100000\n",
      "2022-05-06 21:04:59,282 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:04:59,283 EPOCH 8 done: loss 0.1015 - lr 0.100000\n",
      "2022-05-06 21:04:59,283 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 21:05:00,792 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:05:08,810 epoch 9 - iter 37/373 - loss 0.08633977 - samples/sec: 18.47 - lr: 0.100000\n",
      "2022-05-06 21:05:18,554 epoch 9 - iter 74/373 - loss 0.08842227 - samples/sec: 16.02 - lr: 0.100000\n",
      "2022-05-06 21:05:27,429 epoch 9 - iter 111/373 - loss 0.08962825 - samples/sec: 17.67 - lr: 0.100000\n",
      "2022-05-06 21:05:36,566 epoch 9 - iter 148/373 - loss 0.09652384 - samples/sec: 17.13 - lr: 0.100000\n",
      "2022-05-06 21:05:45,998 epoch 9 - iter 185/373 - loss 0.09580759 - samples/sec: 16.63 - lr: 0.100000\n",
      "2022-05-06 21:05:54,455 epoch 9 - iter 222/373 - loss 0.09530654 - samples/sec: 18.64 - lr: 0.100000\n",
      "2022-05-06 21:06:03,678 epoch 9 - iter 259/373 - loss 0.09424371 - samples/sec: 17.00 - lr: 0.100000\n",
      "2022-05-06 21:06:12,624 epoch 9 - iter 296/373 - loss 0.09555761 - samples/sec: 17.57 - lr: 0.100000\n",
      "2022-05-06 21:06:21,762 epoch 9 - iter 333/373 - loss 0.09838750 - samples/sec: 17.14 - lr: 0.100000\n",
      "2022-05-06 21:06:32,032 epoch 9 - iter 370/373 - loss 0.09909928 - samples/sec: 15.17 - lr: 0.100000\n",
      "2022-05-06 21:06:33,304 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:06:33,305 EPOCH 9 done: loss 0.0988 - lr 0.100000\n",
      "2022-05-06 21:06:33,306 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 21:06:34,684 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:06:43,802 epoch 10 - iter 37/373 - loss 0.09702005 - samples/sec: 16.24 - lr: 0.100000\n",
      "2022-05-06 21:06:52,633 epoch 10 - iter 74/373 - loss 0.09772657 - samples/sec: 17.74 - lr: 0.100000\n",
      "2022-05-06 21:07:01,582 epoch 10 - iter 111/373 - loss 0.09573355 - samples/sec: 17.56 - lr: 0.100000\n",
      "2022-05-06 21:07:10,750 epoch 10 - iter 148/373 - loss 0.09517212 - samples/sec: 17.09 - lr: 0.100000\n",
      "2022-05-06 21:07:20,286 epoch 10 - iter 185/373 - loss 0.09833017 - samples/sec: 16.40 - lr: 0.100000\n",
      "2022-05-06 21:07:28,515 epoch 10 - iter 222/373 - loss 0.10308587 - samples/sec: 19.18 - lr: 0.100000\n",
      "2022-05-06 21:07:38,225 epoch 10 - iter 259/373 - loss 0.10361440 - samples/sec: 16.11 - lr: 0.100000\n",
      "2022-05-06 21:07:47,988 epoch 10 - iter 296/373 - loss 0.10257892 - samples/sec: 15.95 - lr: 0.100000\n",
      "2022-05-06 21:07:56,472 epoch 10 - iter 333/373 - loss 0.10124913 - samples/sec: 18.54 - lr: 0.100000\n",
      "2022-05-06 21:08:05,184 epoch 10 - iter 370/373 - loss 0.09925272 - samples/sec: 18.00 - lr: 0.100000\n",
      "2022-05-06 21:08:06,379 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:08:06,380 EPOCH 10 done: loss 0.0990 - lr 0.100000\n",
      "2022-05-06 21:08:06,380 BAD EPOCHS (no improvement): 1\n",
      "2022-05-06 21:08:07,811 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:08:16,740 epoch 11 - iter 37/373 - loss 0.09507860 - samples/sec: 16.58 - lr: 0.100000\n",
      "2022-05-06 21:08:26,620 epoch 11 - iter 74/373 - loss 0.09488171 - samples/sec: 15.80 - lr: 0.100000\n",
      "2022-05-06 21:08:35,759 epoch 11 - iter 111/373 - loss 0.09831880 - samples/sec: 17.15 - lr: 0.100000\n",
      "2022-05-06 21:08:44,862 epoch 11 - iter 148/373 - loss 0.09895493 - samples/sec: 17.21 - lr: 0.100000\n",
      "2022-05-06 21:08:53,602 epoch 11 - iter 185/373 - loss 0.10053152 - samples/sec: 17.99 - lr: 0.100000\n",
      "2022-05-06 21:09:02,773 epoch 11 - iter 222/373 - loss 0.09943153 - samples/sec: 17.06 - lr: 0.100000\n",
      "2022-05-06 21:09:11,187 epoch 11 - iter 259/373 - loss 0.09914404 - samples/sec: 18.71 - lr: 0.100000\n",
      "2022-05-06 21:09:19,979 epoch 11 - iter 296/373 - loss 0.09761497 - samples/sec: 17.83 - lr: 0.100000\n",
      "2022-05-06 21:09:29,025 epoch 11 - iter 333/373 - loss 0.09753486 - samples/sec: 17.34 - lr: 0.100000\n",
      "2022-05-06 21:09:39,197 epoch 11 - iter 370/373 - loss 0.09531009 - samples/sec: 15.33 - lr: 0.100000\n",
      "2022-05-06 21:09:40,270 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:09:40,270 EPOCH 11 done: loss 0.0949 - lr 0.100000\n",
      "2022-05-06 21:09:40,271 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 21:09:41,682 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:09:50,509 epoch 12 - iter 37/373 - loss 0.09350470 - samples/sec: 16.78 - lr: 0.100000\n",
      "2022-05-06 21:09:59,599 epoch 12 - iter 74/373 - loss 0.09694882 - samples/sec: 17.25 - lr: 0.100000\n",
      "2022-05-06 21:10:08,325 epoch 12 - iter 111/373 - loss 0.09717401 - samples/sec: 18.04 - lr: 0.100000\n",
      "2022-05-06 21:10:16,774 epoch 12 - iter 148/373 - loss 0.09515952 - samples/sec: 18.58 - lr: 0.100000\n",
      "2022-05-06 21:10:25,291 epoch 12 - iter 185/373 - loss 0.09657003 - samples/sec: 18.48 - lr: 0.100000\n",
      "2022-05-06 21:10:34,701 epoch 12 - iter 222/373 - loss 0.09840215 - samples/sec: 16.61 - lr: 0.100000\n",
      "2022-05-06 21:10:44,225 epoch 12 - iter 259/373 - loss 0.09738207 - samples/sec: 16.42 - lr: 0.100000\n",
      "2022-05-06 21:10:52,797 epoch 12 - iter 296/373 - loss 0.09840022 - samples/sec: 18.37 - lr: 0.100000\n",
      "2022-05-06 21:11:03,177 epoch 12 - iter 333/373 - loss 0.09921393 - samples/sec: 15.00 - lr: 0.100000\n",
      "2022-05-06 21:11:12,795 epoch 12 - iter 370/373 - loss 0.09734152 - samples/sec: 16.28 - lr: 0.100000\n",
      "2022-05-06 21:11:14,223 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:11:14,224 EPOCH 12 done: loss 0.0969 - lr 0.100000\n",
      "2022-05-06 21:11:14,224 BAD EPOCHS (no improvement): 1\n",
      "2022-05-06 21:11:15,642 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:11:24,969 epoch 13 - iter 37/373 - loss 0.09075537 - samples/sec: 15.87 - lr: 0.100000\n",
      "2022-05-06 21:11:33,950 epoch 13 - iter 74/373 - loss 0.09158141 - samples/sec: 17.44 - lr: 0.100000\n",
      "2022-05-06 21:11:43,191 epoch 13 - iter 111/373 - loss 0.08938170 - samples/sec: 16.93 - lr: 0.100000\n",
      "2022-05-06 21:11:51,756 epoch 13 - iter 148/373 - loss 0.08869734 - samples/sec: 18.32 - lr: 0.100000\n",
      "2022-05-06 21:12:00,503 epoch 13 - iter 185/373 - loss 0.08723429 - samples/sec: 17.96 - lr: 0.100000\n",
      "2022-05-06 21:12:09,464 epoch 13 - iter 222/373 - loss 0.09008534 - samples/sec: 17.51 - lr: 0.100000\n",
      "2022-05-06 21:12:18,507 epoch 13 - iter 259/373 - loss 0.09113294 - samples/sec: 17.35 - lr: 0.100000\n",
      "2022-05-06 21:12:27,490 epoch 13 - iter 296/373 - loss 0.09190065 - samples/sec: 17.55 - lr: 0.100000\n",
      "2022-05-06 21:12:35,617 epoch 13 - iter 333/373 - loss 0.09094239 - samples/sec: 19.44 - lr: 0.100000\n",
      "2022-05-06 21:12:45,673 epoch 13 - iter 370/373 - loss 0.09353040 - samples/sec: 15.56 - lr: 0.100000\n",
      "2022-05-06 21:12:47,017 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:12:47,018 EPOCH 13 done: loss 0.0940 - lr 0.100000\n",
      "2022-05-06 21:12:47,018 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 21:12:48,450 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:12:57,490 epoch 14 - iter 37/373 - loss 0.09384460 - samples/sec: 16.38 - lr: 0.100000\n",
      "2022-05-06 21:13:06,061 epoch 14 - iter 74/373 - loss 0.09836522 - samples/sec: 18.31 - lr: 0.100000\n",
      "2022-05-06 21:13:15,853 epoch 14 - iter 111/373 - loss 0.10005119 - samples/sec: 15.99 - lr: 0.100000\n",
      "2022-05-06 21:13:23,977 epoch 14 - iter 148/373 - loss 0.09664158 - samples/sec: 19.43 - lr: 0.100000\n",
      "2022-05-06 21:13:33,258 epoch 14 - iter 185/373 - loss 0.09505584 - samples/sec: 16.89 - lr: 0.100000\n",
      "2022-05-06 21:13:42,306 epoch 14 - iter 222/373 - loss 0.09439141 - samples/sec: 17.33 - lr: 0.100000\n",
      "2022-05-06 21:13:52,307 epoch 14 - iter 259/373 - loss 0.09425530 - samples/sec: 15.60 - lr: 0.100000\n",
      "2022-05-06 21:14:01,634 epoch 14 - iter 296/373 - loss 0.09417680 - samples/sec: 16.80 - lr: 0.100000\n",
      "2022-05-06 21:14:10,661 epoch 14 - iter 333/373 - loss 0.09313740 - samples/sec: 17.38 - lr: 0.100000\n",
      "2022-05-06 21:14:19,652 epoch 14 - iter 370/373 - loss 0.09188279 - samples/sec: 17.47 - lr: 0.100000\n",
      "2022-05-06 21:14:20,793 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:14:20,794 EPOCH 14 done: loss 0.0918 - lr 0.100000\n",
      "2022-05-06 21:14:20,794 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 21:14:22,196 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:14:30,016 epoch 15 - iter 37/373 - loss 0.08423552 - samples/sec: 18.93 - lr: 0.100000\n",
      "2022-05-06 21:14:38,854 epoch 15 - iter 74/373 - loss 0.09630680 - samples/sec: 17.78 - lr: 0.100000\n",
      "2022-05-06 21:14:48,258 epoch 15 - iter 111/373 - loss 0.09389932 - samples/sec: 16.64 - lr: 0.100000\n",
      "2022-05-06 21:14:57,110 epoch 15 - iter 148/373 - loss 0.08964258 - samples/sec: 17.75 - lr: 0.100000\n",
      "2022-05-06 21:15:06,791 epoch 15 - iter 185/373 - loss 0.08897031 - samples/sec: 16.14 - lr: 0.100000\n",
      "2022-05-06 21:15:16,869 epoch 15 - iter 222/373 - loss 0.09227968 - samples/sec: 15.49 - lr: 0.100000\n",
      "2022-05-06 21:15:25,845 epoch 15 - iter 259/373 - loss 0.09089523 - samples/sec: 17.51 - lr: 0.100000\n",
      "2022-05-06 21:15:34,501 epoch 15 - iter 296/373 - loss 0.09056877 - samples/sec: 18.14 - lr: 0.100000\n",
      "2022-05-06 21:15:43,564 epoch 15 - iter 333/373 - loss 0.09124436 - samples/sec: 17.26 - lr: 0.100000\n",
      "2022-05-06 21:15:53,724 epoch 15 - iter 370/373 - loss 0.09144636 - samples/sec: 15.38 - lr: 0.100000\n",
      "2022-05-06 21:15:54,842 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:15:54,843 EPOCH 15 done: loss 0.0912 - lr 0.100000\n",
      "2022-05-06 21:15:54,844 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 21:15:56,294 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:16:05,565 epoch 16 - iter 37/373 - loss 0.10595036 - samples/sec: 15.97 - lr: 0.100000\n",
      "2022-05-06 21:16:14,185 epoch 16 - iter 74/373 - loss 0.09417663 - samples/sec: 18.27 - lr: 0.100000\n",
      "2022-05-06 21:16:22,712 epoch 16 - iter 111/373 - loss 0.09263410 - samples/sec: 18.46 - lr: 0.100000\n",
      "2022-05-06 21:16:32,550 epoch 16 - iter 148/373 - loss 0.09346818 - samples/sec: 15.89 - lr: 0.100000\n",
      "2022-05-06 21:16:42,145 epoch 16 - iter 185/373 - loss 0.09085247 - samples/sec: 16.28 - lr: 0.100000\n",
      "2022-05-06 21:16:51,909 epoch 16 - iter 222/373 - loss 0.09070670 - samples/sec: 15.96 - lr: 0.100000\n",
      "2022-05-06 21:17:00,352 epoch 16 - iter 259/373 - loss 0.08987820 - samples/sec: 18.63 - lr: 0.100000\n",
      "2022-05-06 21:17:09,378 epoch 16 - iter 296/373 - loss 0.08946305 - samples/sec: 17.40 - lr: 0.100000\n",
      "2022-05-06 21:17:18,452 epoch 16 - iter 333/373 - loss 0.09032365 - samples/sec: 17.32 - lr: 0.100000\n",
      "2022-05-06 21:17:26,972 epoch 16 - iter 370/373 - loss 0.08967416 - samples/sec: 18.51 - lr: 0.100000\n",
      "2022-05-06 21:17:28,123 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:17:28,123 EPOCH 16 done: loss 0.0897 - lr 0.100000\n",
      "2022-05-06 21:17:28,124 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 21:17:29,543 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:17:37,845 epoch 17 - iter 37/373 - loss 0.07270451 - samples/sec: 17.84 - lr: 0.100000\n",
      "2022-05-06 21:17:47,110 epoch 17 - iter 74/373 - loss 0.08751967 - samples/sec: 16.94 - lr: 0.100000\n",
      "2022-05-06 21:17:55,374 epoch 17 - iter 111/373 - loss 0.08849375 - samples/sec: 19.04 - lr: 0.100000\n",
      "2022-05-06 21:18:04,100 epoch 17 - iter 148/373 - loss 0.08730962 - samples/sec: 18.00 - lr: 0.100000\n",
      "2022-05-06 21:18:13,813 epoch 17 - iter 185/373 - loss 0.09068040 - samples/sec: 16.06 - lr: 0.100000\n",
      "2022-05-06 21:18:23,556 epoch 17 - iter 222/373 - loss 0.09097695 - samples/sec: 16.03 - lr: 0.100000\n",
      "2022-05-06 21:18:32,664 epoch 17 - iter 259/373 - loss 0.08952228 - samples/sec: 17.29 - lr: 0.100000\n",
      "2022-05-06 21:18:42,142 epoch 17 - iter 296/373 - loss 0.08955243 - samples/sec: 16.51 - lr: 0.100000\n",
      "2022-05-06 21:18:51,179 epoch 17 - iter 333/373 - loss 0.08929928 - samples/sec: 17.35 - lr: 0.100000\n",
      "2022-05-06 21:19:00,972 epoch 17 - iter 370/373 - loss 0.09034942 - samples/sec: 15.94 - lr: 0.100000\n",
      "2022-05-06 21:19:01,971 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:19:01,972 EPOCH 17 done: loss 0.0907 - lr 0.100000\n",
      "2022-05-06 21:19:01,973 BAD EPOCHS (no improvement): 1\n",
      "2022-05-06 21:19:03,394 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:19:11,018 epoch 18 - iter 37/373 - loss 0.08891354 - samples/sec: 19.42 - lr: 0.100000\n",
      "2022-05-06 21:19:19,943 epoch 18 - iter 74/373 - loss 0.08255349 - samples/sec: 17.55 - lr: 0.100000\n",
      "2022-05-06 21:19:28,722 epoch 18 - iter 111/373 - loss 0.08294426 - samples/sec: 17.88 - lr: 0.100000\n",
      "2022-05-06 21:19:37,255 epoch 18 - iter 148/373 - loss 0.08626894 - samples/sec: 18.41 - lr: 0.100000\n",
      "2022-05-06 21:19:45,539 epoch 18 - iter 185/373 - loss 0.08982560 - samples/sec: 19.02 - lr: 0.100000\n",
      "2022-05-06 21:19:54,668 epoch 18 - iter 222/373 - loss 0.08732489 - samples/sec: 17.21 - lr: 0.100000\n",
      "2022-05-06 21:20:04,309 epoch 18 - iter 259/373 - loss 0.08802429 - samples/sec: 16.19 - lr: 0.100000\n",
      "2022-05-06 21:20:14,109 epoch 18 - iter 296/373 - loss 0.08884152 - samples/sec: 15.94 - lr: 0.100000\n",
      "2022-05-06 21:20:22,654 epoch 18 - iter 333/373 - loss 0.09038639 - samples/sec: 18.42 - lr: 0.100000\n",
      "2022-05-06 21:20:31,674 epoch 18 - iter 370/373 - loss 0.08965477 - samples/sec: 17.38 - lr: 0.100000\n",
      "2022-05-06 21:20:32,641 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:20:32,641 EPOCH 18 done: loss 0.0896 - lr 0.100000\n",
      "2022-05-06 21:20:32,642 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 21:20:34,039 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:20:42,282 epoch 19 - iter 37/373 - loss 0.07976414 - samples/sec: 17.96 - lr: 0.100000\n",
      "2022-05-06 21:20:51,178 epoch 19 - iter 74/373 - loss 0.08098529 - samples/sec: 17.57 - lr: 0.100000\n",
      "2022-05-06 21:21:00,188 epoch 19 - iter 111/373 - loss 0.08557973 - samples/sec: 17.40 - lr: 0.100000\n",
      "2022-05-06 21:21:09,658 epoch 19 - iter 148/373 - loss 0.08820264 - samples/sec: 16.55 - lr: 0.100000\n",
      "2022-05-06 21:21:20,333 epoch 19 - iter 185/373 - loss 0.08930924 - samples/sec: 14.53 - lr: 0.100000\n",
      "2022-05-06 21:21:29,480 epoch 19 - iter 222/373 - loss 0.08783779 - samples/sec: 17.09 - lr: 0.100000\n",
      "2022-05-06 21:21:38,506 epoch 19 - iter 259/373 - loss 0.08810959 - samples/sec: 17.35 - lr: 0.100000\n",
      "2022-05-06 21:21:46,966 epoch 19 - iter 296/373 - loss 0.08993606 - samples/sec: 18.56 - lr: 0.100000\n",
      "2022-05-06 21:21:54,065 epoch 19 - iter 333/373 - loss 0.08909639 - samples/sec: 22.44 - lr: 0.100000\n",
      "2022-05-06 21:22:02,624 epoch 19 - iter 370/373 - loss 0.08887470 - samples/sec: 18.37 - lr: 0.100000\n",
      "2022-05-06 21:22:03,886 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:22:03,887 EPOCH 19 done: loss 0.0889 - lr 0.100000\n",
      "2022-05-06 21:22:03,887 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 21:22:05,274 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:22:13,839 epoch 20 - iter 37/373 - loss 0.08467709 - samples/sec: 17.29 - lr: 0.100000\n",
      "2022-05-06 21:22:21,965 epoch 20 - iter 74/373 - loss 0.08571936 - samples/sec: 19.38 - lr: 0.100000\n",
      "2022-05-06 21:22:30,808 epoch 20 - iter 111/373 - loss 0.08952231 - samples/sec: 17.71 - lr: 0.100000\n",
      "2022-05-06 21:22:39,796 epoch 20 - iter 148/373 - loss 0.08961348 - samples/sec: 17.44 - lr: 0.100000\n",
      "2022-05-06 21:22:48,916 epoch 20 - iter 185/373 - loss 0.09129335 - samples/sec: 17.15 - lr: 0.100000\n",
      "2022-05-06 21:22:56,797 epoch 20 - iter 222/373 - loss 0.09137112 - samples/sec: 20.03 - lr: 0.100000\n",
      "2022-05-06 21:23:05,845 epoch 20 - iter 259/373 - loss 0.08983051 - samples/sec: 17.29 - lr: 0.100000\n",
      "2022-05-06 21:23:15,338 epoch 20 - iter 296/373 - loss 0.08960431 - samples/sec: 16.42 - lr: 0.100000\n",
      "2022-05-06 21:23:24,537 epoch 20 - iter 333/373 - loss 0.08997413 - samples/sec: 17.07 - lr: 0.100000\n",
      "2022-05-06 21:23:33,525 epoch 20 - iter 370/373 - loss 0.08933494 - samples/sec: 17.45 - lr: 0.100000\n",
      "2022-05-06 21:23:34,668 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:23:34,669 EPOCH 20 done: loss 0.0889 - lr 0.100000\n",
      "2022-05-06 21:23:34,670 BAD EPOCHS (no improvement): 1\n",
      "2022-05-06 21:23:36,069 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:23:44,337 epoch 21 - iter 37/373 - loss 0.08860141 - samples/sec: 17.91 - lr: 0.100000\n",
      "2022-05-06 21:23:53,452 epoch 21 - iter 74/373 - loss 0.09044906 - samples/sec: 17.17 - lr: 0.100000\n",
      "2022-05-06 21:24:01,865 epoch 21 - iter 111/373 - loss 0.09166544 - samples/sec: 18.68 - lr: 0.100000\n",
      "2022-05-06 21:24:09,771 epoch 21 - iter 148/373 - loss 0.08988727 - samples/sec: 20.02 - lr: 0.100000\n",
      "2022-05-06 21:24:19,331 epoch 21 - iter 185/373 - loss 0.08958643 - samples/sec: 16.33 - lr: 0.100000\n",
      "2022-05-06 21:24:29,551 epoch 21 - iter 222/373 - loss 0.08792230 - samples/sec: 15.21 - lr: 0.100000\n",
      "2022-05-06 21:24:37,660 epoch 21 - iter 259/373 - loss 0.08716762 - samples/sec: 19.41 - lr: 0.100000\n",
      "2022-05-06 21:24:47,754 epoch 21 - iter 296/373 - loss 0.08618731 - samples/sec: 15.47 - lr: 0.100000\n",
      "2022-05-06 21:24:56,349 epoch 21 - iter 333/373 - loss 0.08542280 - samples/sec: 18.29 - lr: 0.100000\n",
      "2022-05-06 21:25:05,413 epoch 21 - iter 370/373 - loss 0.08512037 - samples/sec: 17.32 - lr: 0.100000\n",
      "2022-05-06 21:25:06,517 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:25:06,518 EPOCH 21 done: loss 0.0851 - lr 0.100000\n",
      "2022-05-06 21:25:06,519 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 21:25:07,966 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:25:16,317 epoch 22 - iter 37/373 - loss 0.09230728 - samples/sec: 17.73 - lr: 0.100000\n",
      "2022-05-06 21:25:25,086 epoch 22 - iter 74/373 - loss 0.09014218 - samples/sec: 17.90 - lr: 0.100000\n",
      "2022-05-06 21:25:35,559 epoch 22 - iter 111/373 - loss 0.08458344 - samples/sec: 14.83 - lr: 0.100000\n",
      "2022-05-06 21:25:43,499 epoch 22 - iter 148/373 - loss 0.08530745 - samples/sec: 19.84 - lr: 0.100000\n",
      "2022-05-06 21:25:52,410 epoch 22 - iter 185/373 - loss 0.08417447 - samples/sec: 17.55 - lr: 0.100000\n",
      "2022-05-06 21:26:00,907 epoch 22 - iter 222/373 - loss 0.08475817 - samples/sec: 18.52 - lr: 0.100000\n",
      "2022-05-06 21:26:09,750 epoch 22 - iter 259/373 - loss 0.08432568 - samples/sec: 17.74 - lr: 0.100000\n",
      "2022-05-06 21:26:18,067 epoch 22 - iter 296/373 - loss 0.08513678 - samples/sec: 18.91 - lr: 0.100000\n",
      "2022-05-06 21:26:26,838 epoch 22 - iter 333/373 - loss 0.08530300 - samples/sec: 17.87 - lr: 0.100000\n",
      "2022-05-06 21:26:36,051 epoch 22 - iter 370/373 - loss 0.08433330 - samples/sec: 16.96 - lr: 0.100000\n",
      "2022-05-06 21:26:36,934 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:26:36,935 EPOCH 22 done: loss 0.0843 - lr 0.100000\n",
      "2022-05-06 21:26:36,936 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 21:26:38,297 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:26:46,574 epoch 23 - iter 37/373 - loss 0.08442887 - samples/sec: 17.89 - lr: 0.100000\n",
      "2022-05-06 21:26:55,289 epoch 23 - iter 74/373 - loss 0.08985076 - samples/sec: 18.01 - lr: 0.100000\n",
      "2022-05-06 21:27:03,732 epoch 23 - iter 111/373 - loss 0.08585123 - samples/sec: 18.63 - lr: 0.100000\n",
      "2022-05-06 21:27:11,447 epoch 23 - iter 148/373 - loss 0.08500387 - samples/sec: 20.52 - lr: 0.100000\n",
      "2022-05-06 21:27:20,217 epoch 23 - iter 185/373 - loss 0.08306544 - samples/sec: 17.90 - lr: 0.100000\n",
      "2022-05-06 21:27:30,270 epoch 23 - iter 222/373 - loss 0.08444427 - samples/sec: 15.51 - lr: 0.100000\n",
      "2022-05-06 21:27:37,987 epoch 23 - iter 259/373 - loss 0.08331010 - samples/sec: 20.49 - lr: 0.100000\n",
      "2022-05-06 21:27:47,531 epoch 23 - iter 296/373 - loss 0.08323016 - samples/sec: 16.43 - lr: 0.100000\n",
      "2022-05-06 21:27:57,304 epoch 23 - iter 333/373 - loss 0.08400990 - samples/sec: 15.98 - lr: 0.100000\n",
      "2022-05-06 21:28:06,860 epoch 23 - iter 370/373 - loss 0.08474524 - samples/sec: 16.33 - lr: 0.100000\n",
      "2022-05-06 21:28:08,013 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:28:08,013 EPOCH 23 done: loss 0.0845 - lr 0.100000\n",
      "2022-05-06 21:28:08,014 BAD EPOCHS (no improvement): 1\n",
      "2022-05-06 21:28:09,390 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:28:18,043 epoch 24 - iter 37/373 - loss 0.07898728 - samples/sec: 17.11 - lr: 0.100000\n",
      "2022-05-06 21:28:26,249 epoch 24 - iter 74/373 - loss 0.08438954 - samples/sec: 19.19 - lr: 0.100000\n",
      "2022-05-06 21:28:35,047 epoch 24 - iter 111/373 - loss 0.08782499 - samples/sec: 17.84 - lr: 0.100000\n",
      "2022-05-06 21:28:44,063 epoch 24 - iter 148/373 - loss 0.08614431 - samples/sec: 17.37 - lr: 0.100000\n",
      "2022-05-06 21:28:53,579 epoch 24 - iter 185/373 - loss 0.08697926 - samples/sec: 16.43 - lr: 0.100000\n",
      "2022-05-06 21:29:02,642 epoch 24 - iter 222/373 - loss 0.08648944 - samples/sec: 17.36 - lr: 0.100000\n",
      "2022-05-06 21:29:12,392 epoch 24 - iter 259/373 - loss 0.08800013 - samples/sec: 15.98 - lr: 0.100000\n",
      "2022-05-06 21:29:21,295 epoch 24 - iter 296/373 - loss 0.08781992 - samples/sec: 17.57 - lr: 0.100000\n",
      "2022-05-06 21:29:30,007 epoch 24 - iter 333/373 - loss 0.08627846 - samples/sec: 18.04 - lr: 0.100000\n",
      "2022-05-06 21:29:38,638 epoch 24 - iter 370/373 - loss 0.08609758 - samples/sec: 18.19 - lr: 0.100000\n",
      "2022-05-06 21:29:39,704 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:29:39,704 EPOCH 24 done: loss 0.0862 - lr 0.100000\n",
      "2022-05-06 21:29:39,705 BAD EPOCHS (no improvement): 2\n",
      "2022-05-06 21:29:41,169 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:29:49,099 epoch 25 - iter 37/373 - loss 0.07596649 - samples/sec: 18.67 - lr: 0.100000\n",
      "2022-05-06 21:29:58,253 epoch 25 - iter 74/373 - loss 0.08042990 - samples/sec: 17.10 - lr: 0.100000\n",
      "2022-05-06 21:30:07,190 epoch 25 - iter 111/373 - loss 0.08093197 - samples/sec: 17.56 - lr: 0.100000\n",
      "2022-05-06 21:30:15,873 epoch 25 - iter 148/373 - loss 0.08574911 - samples/sec: 18.13 - lr: 0.100000\n",
      "2022-05-06 21:30:24,542 epoch 25 - iter 185/373 - loss 0.08866762 - samples/sec: 18.18 - lr: 0.100000\n",
      "2022-05-06 21:30:33,968 epoch 25 - iter 222/373 - loss 0.08851378 - samples/sec: 16.55 - lr: 0.100000\n",
      "2022-05-06 21:30:43,281 epoch 25 - iter 259/373 - loss 0.08765978 - samples/sec: 16.81 - lr: 0.100000\n",
      "2022-05-06 21:30:51,906 epoch 25 - iter 296/373 - loss 0.08574852 - samples/sec: 18.18 - lr: 0.100000\n",
      "2022-05-06 21:31:01,818 epoch 25 - iter 333/373 - loss 0.08485142 - samples/sec: 15.70 - lr: 0.100000\n",
      "2022-05-06 21:31:10,841 epoch 25 - iter 370/373 - loss 0.08545228 - samples/sec: 17.33 - lr: 0.100000\n",
      "2022-05-06 21:31:12,194 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:31:12,195 EPOCH 25 done: loss 0.0852 - lr 0.100000\n",
      "2022-05-06 21:31:12,195 BAD EPOCHS (no improvement): 3\n",
      "2022-05-06 21:31:13,642 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:31:22,264 epoch 26 - iter 37/373 - loss 0.07842030 - samples/sec: 17.17 - lr: 0.100000\n",
      "2022-05-06 21:31:30,766 epoch 26 - iter 74/373 - loss 0.08524095 - samples/sec: 18.54 - lr: 0.100000\n",
      "2022-05-06 21:31:39,683 epoch 26 - iter 111/373 - loss 0.08114696 - samples/sec: 17.68 - lr: 0.100000\n",
      "2022-05-06 21:31:48,901 epoch 26 - iter 148/373 - loss 0.08151308 - samples/sec: 17.10 - lr: 0.100000\n",
      "2022-05-06 21:31:58,665 epoch 26 - iter 185/373 - loss 0.08372870 - samples/sec: 15.98 - lr: 0.100000\n",
      "2022-05-06 21:32:06,790 epoch 26 - iter 222/373 - loss 0.08363669 - samples/sec: 19.36 - lr: 0.100000\n",
      "2022-05-06 21:32:15,218 epoch 26 - iter 259/373 - loss 0.08288160 - samples/sec: 18.63 - lr: 0.100000\n",
      "2022-05-06 21:32:24,479 epoch 26 - iter 296/373 - loss 0.08370477 - samples/sec: 16.86 - lr: 0.100000\n",
      "2022-05-06 21:32:32,887 epoch 26 - iter 333/373 - loss 0.08327724 - samples/sec: 18.72 - lr: 0.100000\n",
      "2022-05-06 21:32:41,884 epoch 26 - iter 370/373 - loss 0.08367875 - samples/sec: 17.42 - lr: 0.100000\n",
      "2022-05-06 21:32:42,942 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:32:42,943 EPOCH 26 done: loss 0.0838 - lr 0.100000\n",
      "2022-05-06 21:32:42,944 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 21:32:44,352 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:32:54,010 epoch 27 - iter 37/373 - loss 0.10565748 - samples/sec: 15.33 - lr: 0.100000\n",
      "2022-05-06 21:33:03,907 epoch 27 - iter 74/373 - loss 0.09166304 - samples/sec: 15.76 - lr: 0.100000\n",
      "2022-05-06 21:33:12,057 epoch 27 - iter 111/373 - loss 0.09019293 - samples/sec: 19.33 - lr: 0.100000\n",
      "2022-05-06 21:33:21,793 epoch 27 - iter 148/373 - loss 0.08356760 - samples/sec: 16.00 - lr: 0.100000\n",
      "2022-05-06 21:33:30,229 epoch 27 - iter 185/373 - loss 0.08660768 - samples/sec: 18.66 - lr: 0.100000\n",
      "2022-05-06 21:33:38,581 epoch 27 - iter 222/373 - loss 0.08506053 - samples/sec: 18.88 - lr: 0.100000\n",
      "2022-05-06 21:33:48,487 epoch 27 - iter 259/373 - loss 0.08440687 - samples/sec: 15.82 - lr: 0.100000\n",
      "2022-05-06 21:33:57,439 epoch 27 - iter 296/373 - loss 0.08510609 - samples/sec: 17.57 - lr: 0.100000\n",
      "2022-05-06 21:34:05,959 epoch 27 - iter 333/373 - loss 0.08344724 - samples/sec: 18.49 - lr: 0.100000\n",
      "2022-05-06 21:34:13,781 epoch 27 - iter 370/373 - loss 0.08316587 - samples/sec: 20.23 - lr: 0.100000\n",
      "2022-05-06 21:34:14,834 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:34:14,835 EPOCH 27 done: loss 0.0833 - lr 0.100000\n",
      "2022-05-06 21:34:14,835 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 21:34:16,286 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:34:23,870 epoch 28 - iter 37/373 - loss 0.07527478 - samples/sec: 19.52 - lr: 0.100000\n",
      "2022-05-06 21:34:33,007 epoch 28 - iter 74/373 - loss 0.07505347 - samples/sec: 17.16 - lr: 0.100000\n",
      "2022-05-06 21:34:40,994 epoch 28 - iter 111/373 - loss 0.08329265 - samples/sec: 19.75 - lr: 0.100000\n",
      "2022-05-06 21:34:50,136 epoch 28 - iter 148/373 - loss 0.08045921 - samples/sec: 17.09 - lr: 0.100000\n",
      "2022-05-06 21:34:59,314 epoch 28 - iter 185/373 - loss 0.08289009 - samples/sec: 17.06 - lr: 0.100000\n",
      "2022-05-06 21:35:08,483 epoch 28 - iter 222/373 - loss 0.08346208 - samples/sec: 17.07 - lr: 0.100000\n",
      "2022-05-06 21:35:17,898 epoch 28 - iter 259/373 - loss 0.08231534 - samples/sec: 16.60 - lr: 0.100000\n",
      "2022-05-06 21:35:27,334 epoch 28 - iter 296/373 - loss 0.08280267 - samples/sec: 16.56 - lr: 0.100000\n",
      "2022-05-06 21:35:36,784 epoch 28 - iter 333/373 - loss 0.08168772 - samples/sec: 16.55 - lr: 0.100000\n",
      "2022-05-06 21:35:45,898 epoch 28 - iter 370/373 - loss 0.08109497 - samples/sec: 17.21 - lr: 0.100000\n",
      "2022-05-06 21:35:46,752 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:35:46,752 EPOCH 28 done: loss 0.0812 - lr 0.100000\n",
      "2022-05-06 21:35:46,753 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 21:35:48,177 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:35:56,890 epoch 29 - iter 37/373 - loss 0.07681429 - samples/sec: 17.00 - lr: 0.100000\n",
      "2022-05-06 21:36:06,950 epoch 29 - iter 74/373 - loss 0.08489765 - samples/sec: 15.46 - lr: 0.100000\n",
      "2022-05-06 21:36:16,271 epoch 29 - iter 111/373 - loss 0.08594184 - samples/sec: 16.77 - lr: 0.100000\n",
      "2022-05-06 21:36:24,382 epoch 29 - iter 148/373 - loss 0.08250897 - samples/sec: 19.51 - lr: 0.100000\n",
      "2022-05-06 21:36:32,322 epoch 29 - iter 185/373 - loss 0.08336597 - samples/sec: 19.87 - lr: 0.100000\n",
      "2022-05-06 21:36:41,024 epoch 29 - iter 222/373 - loss 0.08326176 - samples/sec: 18.07 - lr: 0.100000\n",
      "2022-05-06 21:36:49,855 epoch 29 - iter 259/373 - loss 0.08350350 - samples/sec: 17.79 - lr: 0.100000\n",
      "2022-05-06 21:36:58,748 epoch 29 - iter 296/373 - loss 0.08375698 - samples/sec: 17.63 - lr: 0.100000\n",
      "2022-05-06 21:37:06,961 epoch 29 - iter 333/373 - loss 0.08277466 - samples/sec: 19.21 - lr: 0.100000\n",
      "2022-05-06 21:37:16,703 epoch 29 - iter 370/373 - loss 0.08281744 - samples/sec: 15.98 - lr: 0.100000\n",
      "2022-05-06 21:37:17,616 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:37:17,617 EPOCH 29 done: loss 0.0826 - lr 0.100000\n",
      "2022-05-06 21:37:17,617 BAD EPOCHS (no improvement): 1\n",
      "2022-05-06 21:37:19,047 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:37:27,878 epoch 30 - iter 37/373 - loss 0.05636381 - samples/sec: 16.77 - lr: 0.100000\n",
      "2022-05-06 21:37:36,356 epoch 30 - iter 74/373 - loss 0.06747764 - samples/sec: 18.62 - lr: 0.100000\n",
      "2022-05-06 21:37:44,827 epoch 30 - iter 111/373 - loss 0.07572542 - samples/sec: 18.57 - lr: 0.100000\n",
      "2022-05-06 21:37:54,536 epoch 30 - iter 148/373 - loss 0.07561950 - samples/sec: 16.07 - lr: 0.100000\n",
      "2022-05-06 21:38:02,454 epoch 30 - iter 185/373 - loss 0.07692286 - samples/sec: 19.93 - lr: 0.100000\n",
      "2022-05-06 21:38:11,047 epoch 30 - iter 222/373 - loss 0.07652679 - samples/sec: 18.29 - lr: 0.100000\n",
      "2022-05-06 21:38:19,776 epoch 30 - iter 259/373 - loss 0.07522998 - samples/sec: 17.91 - lr: 0.100000\n",
      "2022-05-06 21:38:28,726 epoch 30 - iter 296/373 - loss 0.07754950 - samples/sec: 17.48 - lr: 0.100000\n",
      "2022-05-06 21:38:37,837 epoch 30 - iter 333/373 - loss 0.07704840 - samples/sec: 17.16 - lr: 0.100000\n",
      "2022-05-06 21:38:47,328 epoch 30 - iter 370/373 - loss 0.07956138 - samples/sec: 16.46 - lr: 0.100000\n",
      "2022-05-06 21:38:48,720 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:38:48,721 EPOCH 30 done: loss 0.0801 - lr 0.100000\n",
      "2022-05-06 21:38:48,722 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 21:38:50,168 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:38:58,961 epoch 31 - iter 37/373 - loss 0.08363993 - samples/sec: 16.84 - lr: 0.100000\n",
      "2022-05-06 21:39:08,657 epoch 31 - iter 74/373 - loss 0.08546692 - samples/sec: 16.08 - lr: 0.100000\n",
      "2022-05-06 21:39:17,489 epoch 31 - iter 111/373 - loss 0.08157609 - samples/sec: 17.76 - lr: 0.100000\n",
      "2022-05-06 21:39:25,758 epoch 31 - iter 148/373 - loss 0.08583332 - samples/sec: 19.06 - lr: 0.100000\n",
      "2022-05-06 21:39:34,400 epoch 31 - iter 185/373 - loss 0.08296925 - samples/sec: 18.13 - lr: 0.100000\n",
      "2022-05-06 21:39:42,751 epoch 31 - iter 222/373 - loss 0.08454646 - samples/sec: 18.82 - lr: 0.100000\n",
      "2022-05-06 21:39:52,208 epoch 31 - iter 259/373 - loss 0.08234933 - samples/sec: 16.51 - lr: 0.100000\n",
      "2022-05-06 21:40:01,271 epoch 31 - iter 296/373 - loss 0.08108162 - samples/sec: 17.32 - lr: 0.100000\n",
      "2022-05-06 21:40:09,773 epoch 31 - iter 333/373 - loss 0.08192462 - samples/sec: 18.48 - lr: 0.100000\n",
      "2022-05-06 21:40:19,297 epoch 31 - iter 370/373 - loss 0.07985145 - samples/sec: 16.39 - lr: 0.100000\n",
      "2022-05-06 21:40:20,328 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:40:20,329 EPOCH 31 done: loss 0.0799 - lr 0.100000\n",
      "2022-05-06 21:40:20,330 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 21:40:21,740 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:40:30,449 epoch 32 - iter 37/373 - loss 0.08150042 - samples/sec: 17.00 - lr: 0.100000\n",
      "2022-05-06 21:40:38,944 epoch 32 - iter 74/373 - loss 0.08334513 - samples/sec: 18.50 - lr: 0.100000\n",
      "2022-05-06 21:40:48,346 epoch 32 - iter 111/373 - loss 0.08512961 - samples/sec: 16.68 - lr: 0.100000\n",
      "2022-05-06 21:40:56,140 epoch 32 - iter 148/373 - loss 0.08265278 - samples/sec: 20.29 - lr: 0.100000\n",
      "2022-05-06 21:41:05,499 epoch 32 - iter 185/373 - loss 0.08001882 - samples/sec: 16.66 - lr: 0.100000\n",
      "2022-05-06 21:41:15,142 epoch 32 - iter 222/373 - loss 0.07784640 - samples/sec: 16.17 - lr: 0.100000\n",
      "2022-05-06 21:41:23,414 epoch 32 - iter 259/373 - loss 0.07683762 - samples/sec: 19.02 - lr: 0.100000\n",
      "2022-05-06 21:41:32,910 epoch 32 - iter 296/373 - loss 0.07710985 - samples/sec: 16.43 - lr: 0.100000\n",
      "2022-05-06 21:41:41,520 epoch 32 - iter 333/373 - loss 0.07819594 - samples/sec: 18.23 - lr: 0.100000\n",
      "2022-05-06 21:41:50,330 epoch 32 - iter 370/373 - loss 0.07958275 - samples/sec: 17.80 - lr: 0.100000\n",
      "2022-05-06 21:41:51,368 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:41:51,369 EPOCH 32 done: loss 0.0795 - lr 0.100000\n",
      "2022-05-06 21:41:51,370 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 21:41:52,769 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:42:01,818 epoch 33 - iter 37/373 - loss 0.06959014 - samples/sec: 16.36 - lr: 0.100000\n",
      "2022-05-06 21:42:09,439 epoch 33 - iter 74/373 - loss 0.07483378 - samples/sec: 20.83 - lr: 0.100000\n",
      "2022-05-06 21:42:18,763 epoch 33 - iter 111/373 - loss 0.07511073 - samples/sec: 16.75 - lr: 0.100000\n",
      "2022-05-06 21:42:27,602 epoch 33 - iter 148/373 - loss 0.07613503 - samples/sec: 17.72 - lr: 0.100000\n",
      "2022-05-06 21:42:37,025 epoch 33 - iter 185/373 - loss 0.07663546 - samples/sec: 16.58 - lr: 0.100000\n",
      "2022-05-06 21:42:45,739 epoch 33 - iter 222/373 - loss 0.07871254 - samples/sec: 18.01 - lr: 0.100000\n",
      "2022-05-06 21:42:54,667 epoch 33 - iter 259/373 - loss 0.07727276 - samples/sec: 17.55 - lr: 0.100000\n",
      "2022-05-06 21:43:03,351 epoch 33 - iter 296/373 - loss 0.07855433 - samples/sec: 18.29 - lr: 0.100000\n",
      "2022-05-06 21:43:12,038 epoch 33 - iter 333/373 - loss 0.07848088 - samples/sec: 18.08 - lr: 0.100000\n",
      "2022-05-06 21:43:21,339 epoch 33 - iter 370/373 - loss 0.07844669 - samples/sec: 16.81 - lr: 0.100000\n",
      "2022-05-06 21:43:22,642 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:43:22,643 EPOCH 33 done: loss 0.0789 - lr 0.100000\n",
      "2022-05-06 21:43:22,644 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 21:43:24,057 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:43:32,532 epoch 34 - iter 37/373 - loss 0.08126107 - samples/sec: 17.47 - lr: 0.100000\n",
      "2022-05-06 21:43:41,838 epoch 34 - iter 74/373 - loss 0.08120792 - samples/sec: 16.88 - lr: 0.100000\n",
      "2022-05-06 21:43:51,963 epoch 34 - iter 111/373 - loss 0.08029056 - samples/sec: 15.36 - lr: 0.100000\n",
      "2022-05-06 21:44:01,605 epoch 34 - iter 148/373 - loss 0.08087435 - samples/sec: 16.20 - lr: 0.100000\n",
      "2022-05-06 21:44:10,252 epoch 34 - iter 185/373 - loss 0.08016102 - samples/sec: 18.15 - lr: 0.100000\n",
      "2022-05-06 21:44:19,266 epoch 34 - iter 222/373 - loss 0.07920229 - samples/sec: 17.36 - lr: 0.100000\n",
      "2022-05-06 21:44:27,463 epoch 34 - iter 259/373 - loss 0.07891390 - samples/sec: 19.26 - lr: 0.100000\n",
      "2022-05-06 21:44:35,615 epoch 34 - iter 296/373 - loss 0.07962025 - samples/sec: 19.37 - lr: 0.100000\n",
      "2022-05-06 21:44:44,248 epoch 34 - iter 333/373 - loss 0.07820905 - samples/sec: 18.17 - lr: 0.100000\n",
      "2022-05-06 21:44:53,231 epoch 34 - iter 370/373 - loss 0.08026953 - samples/sec: 17.43 - lr: 0.100000\n",
      "2022-05-06 21:44:54,354 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:44:54,355 EPOCH 34 done: loss 0.0805 - lr 0.100000\n",
      "2022-05-06 21:44:54,355 BAD EPOCHS (no improvement): 1\n",
      "2022-05-06 21:44:55,721 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:45:04,369 epoch 35 - iter 37/373 - loss 0.08294014 - samples/sec: 17.12 - lr: 0.100000\n",
      "2022-05-06 21:45:14,324 epoch 35 - iter 74/373 - loss 0.08587550 - samples/sec: 15.68 - lr: 0.100000\n",
      "2022-05-06 21:45:22,678 epoch 35 - iter 111/373 - loss 0.08776070 - samples/sec: 18.85 - lr: 0.100000\n",
      "2022-05-06 21:45:31,543 epoch 35 - iter 148/373 - loss 0.08163615 - samples/sec: 17.70 - lr: 0.100000\n",
      "2022-05-06 21:45:40,227 epoch 35 - iter 185/373 - loss 0.08052485 - samples/sec: 18.09 - lr: 0.100000\n",
      "2022-05-06 21:45:50,695 epoch 35 - iter 222/373 - loss 0.08089516 - samples/sec: 14.97 - lr: 0.100000\n",
      "2022-05-06 21:45:59,163 epoch 35 - iter 259/373 - loss 0.08077729 - samples/sec: 18.61 - lr: 0.100000\n",
      "2022-05-06 21:46:08,147 epoch 35 - iter 296/373 - loss 0.08005694 - samples/sec: 17.41 - lr: 0.100000\n",
      "2022-05-06 21:46:17,253 epoch 35 - iter 333/373 - loss 0.07960428 - samples/sec: 17.15 - lr: 0.100000\n",
      "2022-05-06 21:46:25,990 epoch 35 - iter 370/373 - loss 0.08003836 - samples/sec: 17.93 - lr: 0.100000\n",
      "2022-05-06 21:46:27,175 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:46:27,176 EPOCH 35 done: loss 0.0799 - lr 0.100000\n",
      "2022-05-06 21:46:27,176 BAD EPOCHS (no improvement): 2\n",
      "2022-05-06 21:46:28,639 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:46:36,614 epoch 36 - iter 37/373 - loss 0.07699764 - samples/sec: 18.57 - lr: 0.100000\n",
      "2022-05-06 21:46:46,811 epoch 36 - iter 74/373 - loss 0.07401379 - samples/sec: 15.24 - lr: 0.100000\n",
      "2022-05-06 21:46:55,680 epoch 36 - iter 111/373 - loss 0.07651576 - samples/sec: 17.68 - lr: 0.100000\n",
      "2022-05-06 21:47:05,733 epoch 36 - iter 148/373 - loss 0.07636887 - samples/sec: 15.47 - lr: 0.100000\n",
      "2022-05-06 21:47:14,635 epoch 36 - iter 185/373 - loss 0.07742333 - samples/sec: 17.59 - lr: 0.100000\n",
      "2022-05-06 21:47:23,400 epoch 36 - iter 222/373 - loss 0.07988494 - samples/sec: 17.88 - lr: 0.100000\n",
      "2022-05-06 21:47:31,313 epoch 36 - iter 259/373 - loss 0.08075655 - samples/sec: 19.93 - lr: 0.100000\n",
      "2022-05-06 21:47:39,849 epoch 36 - iter 296/373 - loss 0.08023205 - samples/sec: 18.38 - lr: 0.100000\n",
      "2022-05-06 21:47:48,646 epoch 36 - iter 333/373 - loss 0.07992283 - samples/sec: 17.82 - lr: 0.100000\n",
      "2022-05-06 21:47:57,719 epoch 36 - iter 370/373 - loss 0.07877994 - samples/sec: 17.26 - lr: 0.100000\n",
      "2022-05-06 21:47:59,029 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:47:59,029 EPOCH 36 done: loss 0.0786 - lr 0.100000\n",
      "2022-05-06 21:47:59,030 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 21:48:01,034 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:48:09,459 epoch 37 - iter 37/373 - loss 0.06909103 - samples/sec: 17.58 - lr: 0.100000\n",
      "2022-05-06 21:48:17,723 epoch 37 - iter 74/373 - loss 0.06964748 - samples/sec: 19.05 - lr: 0.100000\n",
      "2022-05-06 21:48:26,404 epoch 37 - iter 111/373 - loss 0.07283942 - samples/sec: 18.08 - lr: 0.100000\n",
      "2022-05-06 21:48:35,302 epoch 37 - iter 148/373 - loss 0.07076209 - samples/sec: 17.66 - lr: 0.100000\n",
      "2022-05-06 21:48:45,262 epoch 37 - iter 185/373 - loss 0.07555840 - samples/sec: 15.66 - lr: 0.100000\n",
      "2022-05-06 21:48:54,401 epoch 37 - iter 222/373 - loss 0.07650581 - samples/sec: 17.10 - lr: 0.100000\n",
      "2022-05-06 21:49:02,298 epoch 37 - iter 259/373 - loss 0.07666936 - samples/sec: 20.01 - lr: 0.100000\n",
      "2022-05-06 21:49:11,207 epoch 37 - iter 296/373 - loss 0.07722226 - samples/sec: 17.61 - lr: 0.100000\n",
      "2022-05-06 21:49:19,336 epoch 37 - iter 333/373 - loss 0.07573067 - samples/sec: 19.37 - lr: 0.100000\n",
      "2022-05-06 21:49:28,536 epoch 37 - iter 370/373 - loss 0.07770061 - samples/sec: 17.02 - lr: 0.100000\n",
      "2022-05-06 21:49:29,922 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:49:29,922 EPOCH 37 done: loss 0.0777 - lr 0.100000\n",
      "2022-05-06 21:49:29,923 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 21:49:31,337 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:49:39,281 epoch 38 - iter 37/373 - loss 0.07443463 - samples/sec: 18.64 - lr: 0.100000\n",
      "2022-05-06 21:49:48,377 epoch 38 - iter 74/373 - loss 0.07850637 - samples/sec: 17.24 - lr: 0.100000\n",
      "2022-05-06 21:49:57,483 epoch 38 - iter 111/373 - loss 0.07849356 - samples/sec: 17.18 - lr: 0.100000\n",
      "2022-05-06 21:50:06,379 epoch 38 - iter 148/373 - loss 0.08070464 - samples/sec: 17.59 - lr: 0.100000\n",
      "2022-05-06 21:50:15,608 epoch 38 - iter 185/373 - loss 0.08046004 - samples/sec: 16.96 - lr: 0.100000\n",
      "2022-05-06 21:50:24,819 epoch 38 - iter 222/373 - loss 0.08165634 - samples/sec: 16.98 - lr: 0.100000\n",
      "2022-05-06 21:50:33,717 epoch 38 - iter 259/373 - loss 0.08063162 - samples/sec: 17.61 - lr: 0.100000\n",
      "2022-05-06 21:50:42,508 epoch 38 - iter 296/373 - loss 0.07950840 - samples/sec: 17.84 - lr: 0.100000\n",
      "2022-05-06 21:50:50,950 epoch 38 - iter 333/373 - loss 0.07853366 - samples/sec: 18.65 - lr: 0.100000\n",
      "2022-05-06 21:50:59,868 epoch 38 - iter 370/373 - loss 0.07950452 - samples/sec: 17.59 - lr: 0.100000\n",
      "2022-05-06 21:51:01,115 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:51:01,116 EPOCH 38 done: loss 0.0793 - lr 0.100000\n",
      "2022-05-06 21:51:01,117 BAD EPOCHS (no improvement): 1\n",
      "2022-05-06 21:51:02,542 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:51:10,837 epoch 39 - iter 37/373 - loss 0.08132418 - samples/sec: 17.85 - lr: 0.100000\n",
      "2022-05-06 21:51:19,645 epoch 39 - iter 74/373 - loss 0.08414249 - samples/sec: 17.77 - lr: 0.100000\n",
      "2022-05-06 21:51:28,153 epoch 39 - iter 111/373 - loss 0.08090455 - samples/sec: 18.47 - lr: 0.100000\n",
      "2022-05-06 21:51:37,721 epoch 39 - iter 148/373 - loss 0.07857673 - samples/sec: 16.29 - lr: 0.100000\n",
      "2022-05-06 21:51:46,660 epoch 39 - iter 185/373 - loss 0.07786045 - samples/sec: 17.52 - lr: 0.100000\n",
      "2022-05-06 21:51:56,062 epoch 39 - iter 222/373 - loss 0.07739470 - samples/sec: 16.66 - lr: 0.100000\n",
      "2022-05-06 21:52:05,197 epoch 39 - iter 259/373 - loss 0.07777917 - samples/sec: 17.14 - lr: 0.100000\n",
      "2022-05-06 21:52:14,842 epoch 39 - iter 296/373 - loss 0.07695703 - samples/sec: 16.17 - lr: 0.100000\n",
      "2022-05-06 21:52:23,355 epoch 39 - iter 333/373 - loss 0.07632339 - samples/sec: 18.48 - lr: 0.100000\n",
      "2022-05-06 21:52:31,795 epoch 39 - iter 370/373 - loss 0.07698343 - samples/sec: 18.63 - lr: 0.100000\n",
      "2022-05-06 21:52:32,984 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:52:32,984 EPOCH 39 done: loss 0.0770 - lr 0.100000\n",
      "2022-05-06 21:52:32,985 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 21:52:34,395 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:52:42,466 epoch 40 - iter 37/373 - loss 0.08127595 - samples/sec: 18.35 - lr: 0.100000\n",
      "2022-05-06 21:52:50,281 epoch 40 - iter 74/373 - loss 0.07254581 - samples/sec: 20.22 - lr: 0.100000\n",
      "2022-05-06 21:52:58,721 epoch 40 - iter 111/373 - loss 0.07397964 - samples/sec: 18.65 - lr: 0.100000\n",
      "2022-05-06 21:53:07,602 epoch 40 - iter 148/373 - loss 0.07504236 - samples/sec: 17.64 - lr: 0.100000\n",
      "2022-05-06 21:53:16,306 epoch 40 - iter 185/373 - loss 0.07766715 - samples/sec: 18.05 - lr: 0.100000\n",
      "2022-05-06 21:53:24,557 epoch 40 - iter 222/373 - loss 0.07738915 - samples/sec: 19.15 - lr: 0.100000\n",
      "2022-05-06 21:53:33,770 epoch 40 - iter 259/373 - loss 0.07700976 - samples/sec: 16.99 - lr: 0.100000\n",
      "2022-05-06 21:53:43,485 epoch 40 - iter 296/373 - loss 0.07547774 - samples/sec: 16.05 - lr: 0.100000\n",
      "2022-05-06 21:53:52,899 epoch 40 - iter 333/373 - loss 0.07648331 - samples/sec: 16.56 - lr: 0.100000\n",
      "2022-05-06 21:54:01,694 epoch 40 - iter 370/373 - loss 0.07709423 - samples/sec: 17.80 - lr: 0.100000\n",
      "2022-05-06 21:54:02,889 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:54:02,890 EPOCH 40 done: loss 0.0768 - lr 0.100000\n",
      "2022-05-06 21:54:02,891 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 21:54:04,252 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:54:13,502 epoch 41 - iter 37/373 - loss 0.06021062 - samples/sec: 16.01 - lr: 0.100000\n",
      "2022-05-06 21:54:21,467 epoch 41 - iter 74/373 - loss 0.06701946 - samples/sec: 19.84 - lr: 0.100000\n",
      "2022-05-06 21:54:31,247 epoch 41 - iter 111/373 - loss 0.07521032 - samples/sec: 15.93 - lr: 0.100000\n",
      "2022-05-06 21:54:40,226 epoch 41 - iter 148/373 - loss 0.07552959 - samples/sec: 17.47 - lr: 0.100000\n",
      "2022-05-06 21:54:48,521 epoch 41 - iter 185/373 - loss 0.07425427 - samples/sec: 19.00 - lr: 0.100000\n",
      "2022-05-06 21:54:57,065 epoch 41 - iter 222/373 - loss 0.07356369 - samples/sec: 18.40 - lr: 0.100000\n",
      "2022-05-06 21:55:06,221 epoch 41 - iter 259/373 - loss 0.07652650 - samples/sec: 17.10 - lr: 0.100000\n",
      "2022-05-06 21:55:15,185 epoch 41 - iter 296/373 - loss 0.07684805 - samples/sec: 17.49 - lr: 0.100000\n",
      "2022-05-06 21:55:23,761 epoch 41 - iter 333/373 - loss 0.07745735 - samples/sec: 18.31 - lr: 0.100000\n",
      "2022-05-06 21:55:33,756 epoch 41 - iter 370/373 - loss 0.07729948 - samples/sec: 15.57 - lr: 0.100000\n",
      "2022-05-06 21:55:34,757 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:55:34,758 EPOCH 41 done: loss 0.0775 - lr 0.100000\n",
      "2022-05-06 21:55:34,759 BAD EPOCHS (no improvement): 1\n",
      "2022-05-06 21:55:36,296 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:55:44,186 epoch 42 - iter 37/373 - loss 0.08543869 - samples/sec: 18.76 - lr: 0.100000\n",
      "2022-05-06 21:55:53,962 epoch 42 - iter 74/373 - loss 0.07844573 - samples/sec: 15.97 - lr: 0.100000\n",
      "2022-05-06 21:56:02,368 epoch 42 - iter 111/373 - loss 0.07614742 - samples/sec: 18.74 - lr: 0.100000\n",
      "2022-05-06 21:56:11,013 epoch 42 - iter 148/373 - loss 0.07507163 - samples/sec: 18.15 - lr: 0.100000\n",
      "2022-05-06 21:56:20,172 epoch 42 - iter 185/373 - loss 0.07402482 - samples/sec: 17.11 - lr: 0.100000\n",
      "2022-05-06 21:56:28,738 epoch 42 - iter 222/373 - loss 0.07546849 - samples/sec: 18.32 - lr: 0.100000\n",
      "2022-05-06 21:56:37,126 epoch 42 - iter 259/373 - loss 0.07605334 - samples/sec: 18.71 - lr: 0.100000\n",
      "2022-05-06 21:56:46,165 epoch 42 - iter 296/373 - loss 0.07651391 - samples/sec: 17.33 - lr: 0.100000\n",
      "2022-05-06 21:56:54,985 epoch 42 - iter 333/373 - loss 0.07702922 - samples/sec: 17.78 - lr: 0.100000\n",
      "2022-05-06 21:57:04,535 epoch 42 - iter 370/373 - loss 0.07742934 - samples/sec: 16.33 - lr: 0.100000\n",
      "2022-05-06 21:57:05,723 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:57:05,723 EPOCH 42 done: loss 0.0776 - lr 0.100000\n",
      "2022-05-06 21:57:05,724 BAD EPOCHS (no improvement): 2\n",
      "2022-05-06 21:57:07,149 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:57:16,443 epoch 43 - iter 37/373 - loss 0.08959094 - samples/sec: 15.93 - lr: 0.100000\n",
      "2022-05-06 21:57:26,184 epoch 43 - iter 74/373 - loss 0.08010855 - samples/sec: 16.05 - lr: 0.100000\n",
      "2022-05-06 21:57:34,680 epoch 43 - iter 111/373 - loss 0.07996803 - samples/sec: 18.52 - lr: 0.100000\n",
      "2022-05-06 21:57:42,811 epoch 43 - iter 148/373 - loss 0.07755423 - samples/sec: 19.39 - lr: 0.100000\n",
      "2022-05-06 21:57:51,777 epoch 43 - iter 185/373 - loss 0.07452806 - samples/sec: 17.49 - lr: 0.100000\n",
      "2022-05-06 21:58:00,549 epoch 43 - iter 222/373 - loss 0.07566648 - samples/sec: 17.87 - lr: 0.100000\n",
      "2022-05-06 21:58:09,723 epoch 43 - iter 259/373 - loss 0.07668261 - samples/sec: 17.05 - lr: 0.100000\n",
      "2022-05-06 21:58:18,729 epoch 43 - iter 296/373 - loss 0.07718194 - samples/sec: 17.42 - lr: 0.100000\n",
      "2022-05-06 21:58:27,219 epoch 43 - iter 333/373 - loss 0.07585585 - samples/sec: 18.50 - lr: 0.100000\n",
      "2022-05-06 21:58:36,212 epoch 43 - iter 370/373 - loss 0.07503060 - samples/sec: 17.44 - lr: 0.100000\n",
      "2022-05-06 21:58:37,238 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:58:37,239 EPOCH 43 done: loss 0.0749 - lr 0.100000\n",
      "2022-05-06 21:58:37,239 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 21:58:38,686 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 21:58:46,534 epoch 44 - iter 37/373 - loss 0.07341019 - samples/sec: 18.86 - lr: 0.100000\n",
      "2022-05-06 21:58:55,753 epoch 44 - iter 74/373 - loss 0.07580003 - samples/sec: 16.96 - lr: 0.100000\n",
      "2022-05-06 21:59:04,936 epoch 44 - iter 111/373 - loss 0.07330223 - samples/sec: 17.09 - lr: 0.100000\n",
      "2022-05-06 21:59:13,902 epoch 44 - iter 148/373 - loss 0.07295600 - samples/sec: 17.51 - lr: 0.100000\n",
      "2022-05-06 21:59:22,232 epoch 44 - iter 185/373 - loss 0.07282274 - samples/sec: 18.87 - lr: 0.100000\n",
      "2022-05-06 21:59:31,211 epoch 44 - iter 222/373 - loss 0.07503868 - samples/sec: 17.45 - lr: 0.100000\n",
      "2022-05-06 21:59:40,177 epoch 44 - iter 259/373 - loss 0.07727972 - samples/sec: 17.44 - lr: 0.100000\n",
      "2022-05-06 21:59:49,467 epoch 44 - iter 296/373 - loss 0.07683537 - samples/sec: 16.82 - lr: 0.100000\n",
      "2022-05-06 21:59:58,764 epoch 44 - iter 333/373 - loss 0.07500825 - samples/sec: 16.82 - lr: 0.100000\n",
      "2022-05-06 22:00:08,082 epoch 44 - iter 370/373 - loss 0.07630132 - samples/sec: 16.76 - lr: 0.100000\n",
      "2022-05-06 22:00:09,231 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:00:09,232 EPOCH 44 done: loss 0.0769 - lr 0.100000\n",
      "2022-05-06 22:00:09,232 BAD EPOCHS (no improvement): 1\n",
      "2022-05-06 22:00:10,673 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:00:19,724 epoch 45 - iter 37/373 - loss 0.07206506 - samples/sec: 16.36 - lr: 0.100000\n",
      "2022-05-06 22:00:29,070 epoch 45 - iter 74/373 - loss 0.07291349 - samples/sec: 16.73 - lr: 0.100000\n",
      "2022-05-06 22:00:37,741 epoch 45 - iter 111/373 - loss 0.07383099 - samples/sec: 18.18 - lr: 0.100000\n",
      "2022-05-06 22:00:46,240 epoch 45 - iter 148/373 - loss 0.07113424 - samples/sec: 18.51 - lr: 0.100000\n",
      "2022-05-06 22:00:55,447 epoch 45 - iter 185/373 - loss 0.07382473 - samples/sec: 17.01 - lr: 0.100000\n",
      "2022-05-06 22:01:05,499 epoch 45 - iter 222/373 - loss 0.07450505 - samples/sec: 15.47 - lr: 0.100000\n",
      "2022-05-06 22:01:13,169 epoch 45 - iter 259/373 - loss 0.07434320 - samples/sec: 20.61 - lr: 0.100000\n",
      "2022-05-06 22:01:22,883 epoch 45 - iter 296/373 - loss 0.07444489 - samples/sec: 16.05 - lr: 0.100000\n",
      "2022-05-06 22:01:32,147 epoch 45 - iter 333/373 - loss 0.07535228 - samples/sec: 16.87 - lr: 0.100000\n",
      "2022-05-06 22:01:40,643 epoch 45 - iter 370/373 - loss 0.07524924 - samples/sec: 18.55 - lr: 0.100000\n",
      "2022-05-06 22:01:41,907 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:01:41,908 EPOCH 45 done: loss 0.0754 - lr 0.100000\n",
      "2022-05-06 22:01:41,909 BAD EPOCHS (no improvement): 2\n",
      "2022-05-06 22:01:43,349 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:01:51,944 epoch 46 - iter 37/373 - loss 0.08559201 - samples/sec: 17.22 - lr: 0.100000\n",
      "2022-05-06 22:02:01,445 epoch 46 - iter 74/373 - loss 0.08960051 - samples/sec: 16.40 - lr: 0.100000\n",
      "2022-05-06 22:02:09,857 epoch 46 - iter 111/373 - loss 0.08720540 - samples/sec: 18.72 - lr: 0.100000\n",
      "2022-05-06 22:02:19,498 epoch 46 - iter 148/373 - loss 0.08287690 - samples/sec: 16.19 - lr: 0.100000\n",
      "2022-05-06 22:02:28,631 epoch 46 - iter 185/373 - loss 0.08083414 - samples/sec: 17.14 - lr: 0.100000\n",
      "2022-05-06 22:02:37,461 epoch 46 - iter 222/373 - loss 0.08008512 - samples/sec: 17.78 - lr: 0.100000\n",
      "2022-05-06 22:02:46,168 epoch 46 - iter 259/373 - loss 0.07896567 - samples/sec: 18.00 - lr: 0.100000\n",
      "2022-05-06 22:02:54,888 epoch 46 - iter 296/373 - loss 0.07762721 - samples/sec: 18.02 - lr: 0.100000\n",
      "2022-05-06 22:03:03,135 epoch 46 - iter 333/373 - loss 0.07698882 - samples/sec: 19.05 - lr: 0.100000\n",
      "2022-05-06 22:03:12,177 epoch 46 - iter 370/373 - loss 0.07570170 - samples/sec: 17.34 - lr: 0.100000\n",
      "2022-05-06 22:03:13,537 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:03:13,538 EPOCH 46 done: loss 0.0753 - lr 0.100000\n",
      "2022-05-06 22:03:13,539 BAD EPOCHS (no improvement): 3\n",
      "2022-05-06 22:03:14,925 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:03:22,656 epoch 47 - iter 37/373 - loss 0.08136917 - samples/sec: 19.15 - lr: 0.100000\n",
      "2022-05-06 22:03:32,719 epoch 47 - iter 74/373 - loss 0.07549185 - samples/sec: 15.48 - lr: 0.100000\n",
      "2022-05-06 22:03:41,148 epoch 47 - iter 111/373 - loss 0.07481470 - samples/sec: 18.68 - lr: 0.100000\n",
      "2022-05-06 22:03:50,638 epoch 47 - iter 148/373 - loss 0.07403028 - samples/sec: 16.47 - lr: 0.100000\n",
      "2022-05-06 22:03:58,958 epoch 47 - iter 185/373 - loss 0.07528086 - samples/sec: 18.96 - lr: 0.100000\n",
      "2022-05-06 22:04:07,954 epoch 47 - iter 222/373 - loss 0.07599907 - samples/sec: 17.41 - lr: 0.100000\n",
      "2022-05-06 22:04:17,180 epoch 47 - iter 259/373 - loss 0.07575197 - samples/sec: 16.95 - lr: 0.100000\n",
      "2022-05-06 22:04:25,571 epoch 47 - iter 296/373 - loss 0.07515111 - samples/sec: 18.73 - lr: 0.100000\n",
      "2022-05-06 22:04:35,092 epoch 47 - iter 333/373 - loss 0.07514938 - samples/sec: 16.37 - lr: 0.100000\n",
      "2022-05-06 22:04:44,416 epoch 47 - iter 370/373 - loss 0.07484030 - samples/sec: 16.73 - lr: 0.100000\n",
      "2022-05-06 22:04:45,527 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:04:45,528 EPOCH 47 done: loss 0.0753 - lr 0.100000\n",
      "2022-05-06 22:04:45,528 Epoch    47: reducing learning rate of group 0 to 5.0000e-02.\n",
      "2022-05-06 22:04:45,529 BAD EPOCHS (no improvement): 4\n",
      "2022-05-06 22:04:47,025 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:04:54,960 epoch 48 - iter 37/373 - loss 0.07087709 - samples/sec: 18.66 - lr: 0.050000\n",
      "2022-05-06 22:05:03,988 epoch 48 - iter 74/373 - loss 0.07073475 - samples/sec: 17.34 - lr: 0.050000\n",
      "2022-05-06 22:05:13,208 epoch 48 - iter 111/373 - loss 0.07252500 - samples/sec: 16.97 - lr: 0.050000\n",
      "2022-05-06 22:05:22,113 epoch 48 - iter 148/373 - loss 0.07250988 - samples/sec: 17.58 - lr: 0.050000\n",
      "2022-05-06 22:05:31,757 epoch 48 - iter 185/373 - loss 0.07137003 - samples/sec: 16.18 - lr: 0.050000\n",
      "2022-05-06 22:05:40,225 epoch 48 - iter 222/373 - loss 0.07096699 - samples/sec: 18.55 - lr: 0.050000\n",
      "2022-05-06 22:05:48,708 epoch 48 - iter 259/373 - loss 0.07198785 - samples/sec: 18.50 - lr: 0.050000\n",
      "2022-05-06 22:05:57,817 epoch 48 - iter 296/373 - loss 0.07011840 - samples/sec: 17.17 - lr: 0.050000\n",
      "2022-05-06 22:06:06,669 epoch 48 - iter 333/373 - loss 0.06923456 - samples/sec: 17.74 - lr: 0.050000\n",
      "2022-05-06 22:06:15,178 epoch 48 - iter 370/373 - loss 0.06929676 - samples/sec: 18.45 - lr: 0.050000\n",
      "2022-05-06 22:06:16,308 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:06:16,309 EPOCH 48 done: loss 0.0690 - lr 0.050000\n",
      "2022-05-06 22:06:16,310 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 22:06:17,715 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:06:25,932 epoch 49 - iter 37/373 - loss 0.06800419 - samples/sec: 18.02 - lr: 0.050000\n",
      "2022-05-06 22:06:35,418 epoch 49 - iter 74/373 - loss 0.06553529 - samples/sec: 16.46 - lr: 0.050000\n",
      "2022-05-06 22:06:44,384 epoch 49 - iter 111/373 - loss 0.06924241 - samples/sec: 17.46 - lr: 0.050000\n",
      "2022-05-06 22:06:53,392 epoch 49 - iter 148/373 - loss 0.06850453 - samples/sec: 17.39 - lr: 0.050000\n",
      "2022-05-06 22:07:01,784 epoch 49 - iter 185/373 - loss 0.06643409 - samples/sec: 18.71 - lr: 0.050000\n",
      "2022-05-06 22:07:10,133 epoch 49 - iter 222/373 - loss 0.06551063 - samples/sec: 18.84 - lr: 0.050000\n",
      "2022-05-06 22:07:20,058 epoch 49 - iter 259/373 - loss 0.06709964 - samples/sec: 15.67 - lr: 0.050000\n",
      "2022-05-06 22:07:28,814 epoch 49 - iter 296/373 - loss 0.06673869 - samples/sec: 17.93 - lr: 0.050000\n",
      "2022-05-06 22:07:37,388 epoch 49 - iter 333/373 - loss 0.06662169 - samples/sec: 18.35 - lr: 0.050000\n",
      "2022-05-06 22:07:46,099 epoch 49 - iter 370/373 - loss 0.06781510 - samples/sec: 18.04 - lr: 0.050000\n",
      "2022-05-06 22:07:47,356 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:07:47,357 EPOCH 49 done: loss 0.0680 - lr 0.050000\n",
      "2022-05-06 22:07:47,357 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 22:07:48,747 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:07:56,744 epoch 50 - iter 37/373 - loss 0.07391604 - samples/sec: 18.52 - lr: 0.050000\n",
      "2022-05-06 22:08:06,189 epoch 50 - iter 74/373 - loss 0.06566402 - samples/sec: 16.52 - lr: 0.050000\n",
      "2022-05-06 22:08:15,251 epoch 50 - iter 111/373 - loss 0.06601315 - samples/sec: 17.38 - lr: 0.050000\n",
      "2022-05-06 22:08:23,670 epoch 50 - iter 148/373 - loss 0.06609259 - samples/sec: 18.67 - lr: 0.050000\n",
      "2022-05-06 22:08:32,160 epoch 50 - iter 185/373 - loss 0.06416490 - samples/sec: 18.48 - lr: 0.050000\n",
      "2022-05-06 22:08:40,161 epoch 50 - iter 222/373 - loss 0.06470502 - samples/sec: 19.70 - lr: 0.050000\n",
      "2022-05-06 22:08:49,203 epoch 50 - iter 259/373 - loss 0.06685446 - samples/sec: 17.30 - lr: 0.050000\n",
      "2022-05-06 22:08:59,131 epoch 50 - iter 296/373 - loss 0.06481875 - samples/sec: 15.70 - lr: 0.050000\n",
      "2022-05-06 22:09:08,667 epoch 50 - iter 333/373 - loss 0.06587335 - samples/sec: 16.36 - lr: 0.050000\n",
      "2022-05-06 22:09:16,808 epoch 50 - iter 370/373 - loss 0.06652404 - samples/sec: 19.38 - lr: 0.050000\n",
      "2022-05-06 22:09:17,881 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:09:17,882 EPOCH 50 done: loss 0.0667 - lr 0.050000\n",
      "2022-05-06 22:09:17,882 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 22:09:19,314 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:09:27,952 epoch 51 - iter 37/373 - loss 0.05676528 - samples/sec: 17.14 - lr: 0.050000\n",
      "2022-05-06 22:09:36,654 epoch 51 - iter 74/373 - loss 0.06064605 - samples/sec: 18.05 - lr: 0.050000\n",
      "2022-05-06 22:09:45,487 epoch 51 - iter 111/373 - loss 0.06429933 - samples/sec: 17.76 - lr: 0.050000\n",
      "2022-05-06 22:09:53,360 epoch 51 - iter 148/373 - loss 0.06295521 - samples/sec: 20.03 - lr: 0.050000\n",
      "2022-05-06 22:10:03,053 epoch 51 - iter 185/373 - loss 0.06314720 - samples/sec: 16.06 - lr: 0.050000\n",
      "2022-05-06 22:10:12,555 epoch 51 - iter 222/373 - loss 0.06375842 - samples/sec: 16.43 - lr: 0.050000\n",
      "2022-05-06 22:10:20,297 epoch 51 - iter 259/373 - loss 0.06410411 - samples/sec: 20.44 - lr: 0.050000\n",
      "2022-05-06 22:10:28,618 epoch 51 - iter 296/373 - loss 0.06506387 - samples/sec: 18.92 - lr: 0.050000\n",
      "2022-05-06 22:10:37,533 epoch 51 - iter 333/373 - loss 0.06599280 - samples/sec: 17.58 - lr: 0.050000\n",
      "2022-05-06 22:10:47,057 epoch 51 - iter 370/373 - loss 0.06710777 - samples/sec: 16.39 - lr: 0.050000\n",
      "2022-05-06 22:10:48,225 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:10:48,226 EPOCH 51 done: loss 0.0670 - lr 0.050000\n",
      "2022-05-06 22:10:48,226 BAD EPOCHS (no improvement): 1\n",
      "2022-05-06 22:10:49,637 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:10:58,066 epoch 52 - iter 37/373 - loss 0.06527949 - samples/sec: 17.57 - lr: 0.050000\n",
      "2022-05-06 22:11:07,326 epoch 52 - iter 74/373 - loss 0.06778948 - samples/sec: 16.86 - lr: 0.050000\n",
      "2022-05-06 22:11:15,742 epoch 52 - iter 111/373 - loss 0.06400677 - samples/sec: 18.66 - lr: 0.050000\n",
      "2022-05-06 22:11:24,534 epoch 52 - iter 148/373 - loss 0.06797257 - samples/sec: 17.94 - lr: 0.050000\n",
      "2022-05-06 22:11:33,093 epoch 52 - iter 185/373 - loss 0.06501857 - samples/sec: 18.37 - lr: 0.050000\n",
      "2022-05-06 22:11:41,962 epoch 52 - iter 222/373 - loss 0.06436496 - samples/sec: 17.72 - lr: 0.050000\n",
      "2022-05-06 22:11:51,559 epoch 52 - iter 259/373 - loss 0.06269240 - samples/sec: 16.26 - lr: 0.050000\n",
      "2022-05-06 22:12:00,853 epoch 52 - iter 296/373 - loss 0.06307164 - samples/sec: 16.84 - lr: 0.050000\n",
      "2022-05-06 22:12:09,895 epoch 52 - iter 333/373 - loss 0.06332133 - samples/sec: 17.34 - lr: 0.050000\n",
      "2022-05-06 22:12:18,382 epoch 52 - iter 370/373 - loss 0.06457310 - samples/sec: 18.47 - lr: 0.050000\n",
      "2022-05-06 22:12:19,463 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:12:19,464 EPOCH 52 done: loss 0.0646 - lr 0.050000\n",
      "2022-05-06 22:12:19,465 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 22:12:20,843 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:12:29,068 epoch 53 - iter 37/373 - loss 0.06576562 - samples/sec: 18.00 - lr: 0.050000\n",
      "2022-05-06 22:12:37,276 epoch 53 - iter 74/373 - loss 0.06529967 - samples/sec: 19.18 - lr: 0.050000\n",
      "2022-05-06 22:12:46,528 epoch 53 - iter 111/373 - loss 0.06559749 - samples/sec: 16.90 - lr: 0.050000\n",
      "2022-05-06 22:12:56,382 epoch 53 - iter 148/373 - loss 0.06245582 - samples/sec: 15.81 - lr: 0.050000\n",
      "2022-05-06 22:13:05,136 epoch 53 - iter 185/373 - loss 0.06276978 - samples/sec: 17.89 - lr: 0.050000\n",
      "2022-05-06 22:13:14,387 epoch 53 - iter 222/373 - loss 0.06361949 - samples/sec: 16.90 - lr: 0.050000\n",
      "2022-05-06 22:13:23,176 epoch 53 - iter 259/373 - loss 0.06537340 - samples/sec: 17.86 - lr: 0.050000\n",
      "2022-05-06 22:13:32,866 epoch 53 - iter 296/373 - loss 0.06418970 - samples/sec: 16.12 - lr: 0.050000\n",
      "2022-05-06 22:13:42,029 epoch 53 - iter 333/373 - loss 0.06419384 - samples/sec: 17.07 - lr: 0.050000\n",
      "2022-05-06 22:13:50,173 epoch 53 - iter 370/373 - loss 0.06477132 - samples/sec: 19.32 - lr: 0.050000\n",
      "2022-05-06 22:13:51,076 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:13:51,078 EPOCH 53 done: loss 0.0647 - lr 0.050000\n",
      "2022-05-06 22:13:51,078 BAD EPOCHS (no improvement): 1\n",
      "2022-05-06 22:13:52,458 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:14:01,345 epoch 54 - iter 37/373 - loss 0.06314488 - samples/sec: 16.66 - lr: 0.050000\n",
      "2022-05-06 22:14:11,571 epoch 54 - iter 74/373 - loss 0.06067272 - samples/sec: 15.19 - lr: 0.050000\n",
      "2022-05-06 22:14:19,779 epoch 54 - iter 111/373 - loss 0.05962177 - samples/sec: 19.22 - lr: 0.050000\n",
      "2022-05-06 22:14:28,290 epoch 54 - iter 148/373 - loss 0.06083663 - samples/sec: 18.46 - lr: 0.050000\n",
      "2022-05-06 22:14:38,267 epoch 54 - iter 185/373 - loss 0.06052521 - samples/sec: 15.66 - lr: 0.050000\n",
      "2022-05-06 22:14:47,109 epoch 54 - iter 222/373 - loss 0.06017984 - samples/sec: 17.77 - lr: 0.050000\n",
      "2022-05-06 22:14:55,450 epoch 54 - iter 259/373 - loss 0.06086459 - samples/sec: 18.85 - lr: 0.050000\n",
      "2022-05-06 22:15:04,953 epoch 54 - iter 296/373 - loss 0.06205416 - samples/sec: 16.45 - lr: 0.050000\n",
      "2022-05-06 22:15:14,613 epoch 54 - iter 333/373 - loss 0.06289499 - samples/sec: 16.15 - lr: 0.050000\n",
      "2022-05-06 22:15:23,002 epoch 54 - iter 370/373 - loss 0.06371448 - samples/sec: 18.70 - lr: 0.050000\n",
      "2022-05-06 22:15:24,122 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:15:24,123 EPOCH 54 done: loss 0.0640 - lr 0.050000\n",
      "2022-05-06 22:15:24,123 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 22:15:25,543 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:15:34,175 epoch 55 - iter 37/373 - loss 0.06319744 - samples/sec: 17.15 - lr: 0.050000\n",
      "2022-05-06 22:15:43,436 epoch 55 - iter 74/373 - loss 0.05829340 - samples/sec: 16.95 - lr: 0.050000\n",
      "2022-05-06 22:15:51,722 epoch 55 - iter 111/373 - loss 0.06202855 - samples/sec: 18.98 - lr: 0.050000\n",
      "2022-05-06 22:16:01,629 epoch 55 - iter 148/373 - loss 0.06276962 - samples/sec: 15.73 - lr: 0.050000\n",
      "2022-05-06 22:16:10,640 epoch 55 - iter 185/373 - loss 0.06080940 - samples/sec: 17.37 - lr: 0.050000\n",
      "2022-05-06 22:16:18,695 epoch 55 - iter 222/373 - loss 0.06243991 - samples/sec: 19.58 - lr: 0.050000\n",
      "2022-05-06 22:16:27,687 epoch 55 - iter 259/373 - loss 0.06298293 - samples/sec: 17.37 - lr: 0.050000\n",
      "2022-05-06 22:16:36,289 epoch 55 - iter 296/373 - loss 0.06282582 - samples/sec: 18.23 - lr: 0.050000\n",
      "2022-05-06 22:16:45,436 epoch 55 - iter 333/373 - loss 0.06339915 - samples/sec: 17.13 - lr: 0.050000\n",
      "2022-05-06 22:16:54,239 epoch 55 - iter 370/373 - loss 0.06499568 - samples/sec: 17.80 - lr: 0.050000\n",
      "2022-05-06 22:16:55,399 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:16:55,400 EPOCH 55 done: loss 0.0651 - lr 0.050000\n",
      "2022-05-06 22:16:55,400 BAD EPOCHS (no improvement): 1\n",
      "2022-05-06 22:16:56,813 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:17:06,341 epoch 56 - iter 37/373 - loss 0.05727433 - samples/sec: 15.54 - lr: 0.050000\n",
      "2022-05-06 22:17:14,446 epoch 56 - iter 74/373 - loss 0.05504492 - samples/sec: 19.45 - lr: 0.050000\n",
      "2022-05-06 22:17:22,969 epoch 56 - iter 111/373 - loss 0.06487288 - samples/sec: 18.43 - lr: 0.050000\n",
      "2022-05-06 22:17:32,337 epoch 56 - iter 148/373 - loss 0.06338905 - samples/sec: 16.70 - lr: 0.050000\n",
      "2022-05-06 22:17:41,489 epoch 56 - iter 185/373 - loss 0.06084548 - samples/sec: 17.05 - lr: 0.050000\n",
      "2022-05-06 22:17:49,089 epoch 56 - iter 222/373 - loss 0.06186084 - samples/sec: 20.78 - lr: 0.050000\n",
      "2022-05-06 22:17:58,477 epoch 56 - iter 259/373 - loss 0.06092276 - samples/sec: 16.63 - lr: 0.050000\n",
      "2022-05-06 22:18:07,869 epoch 56 - iter 296/373 - loss 0.06267908 - samples/sec: 16.71 - lr: 0.050000\n",
      "2022-05-06 22:18:15,536 epoch 56 - iter 333/373 - loss 0.06408906 - samples/sec: 20.63 - lr: 0.050000\n",
      "2022-05-06 22:18:24,669 epoch 56 - iter 370/373 - loss 0.06507505 - samples/sec: 17.13 - lr: 0.050000\n",
      "2022-05-06 22:18:26,317 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:18:26,317 EPOCH 56 done: loss 0.0649 - lr 0.050000\n",
      "2022-05-06 22:18:26,318 BAD EPOCHS (no improvement): 2\n",
      "2022-05-06 22:18:27,767 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:18:36,222 epoch 57 - iter 37/373 - loss 0.07187375 - samples/sec: 17.51 - lr: 0.050000\n",
      "2022-05-06 22:18:44,879 epoch 57 - iter 74/373 - loss 0.06616213 - samples/sec: 18.13 - lr: 0.050000\n",
      "2022-05-06 22:18:53,109 epoch 57 - iter 111/373 - loss 0.06286112 - samples/sec: 19.20 - lr: 0.050000\n",
      "2022-05-06 22:19:02,110 epoch 57 - iter 148/373 - loss 0.06328932 - samples/sec: 17.38 - lr: 0.050000\n",
      "2022-05-06 22:19:11,066 epoch 57 - iter 185/373 - loss 0.06520476 - samples/sec: 17.49 - lr: 0.050000\n",
      "2022-05-06 22:19:19,559 epoch 57 - iter 222/373 - loss 0.06609692 - samples/sec: 18.48 - lr: 0.050000\n",
      "2022-05-06 22:19:28,156 epoch 57 - iter 259/373 - loss 0.06368359 - samples/sec: 18.25 - lr: 0.050000\n",
      "2022-05-06 22:19:37,705 epoch 57 - iter 296/373 - loss 0.06339906 - samples/sec: 16.37 - lr: 0.050000\n",
      "2022-05-06 22:19:46,740 epoch 57 - iter 333/373 - loss 0.06393395 - samples/sec: 17.35 - lr: 0.050000\n",
      "2022-05-06 22:19:56,141 epoch 57 - iter 370/373 - loss 0.06329625 - samples/sec: 16.64 - lr: 0.050000\n",
      "2022-05-06 22:19:57,569 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:19:57,570 EPOCH 57 done: loss 0.0636 - lr 0.050000\n",
      "2022-05-06 22:19:57,571 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 22:19:59,091 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:20:08,914 epoch 58 - iter 37/373 - loss 0.05544556 - samples/sec: 15.07 - lr: 0.050000\n",
      "2022-05-06 22:20:17,771 epoch 58 - iter 74/373 - loss 0.05605730 - samples/sec: 17.74 - lr: 0.050000\n",
      "2022-05-06 22:20:26,149 epoch 58 - iter 111/373 - loss 0.06160335 - samples/sec: 18.79 - lr: 0.050000\n",
      "2022-05-06 22:20:34,654 epoch 58 - iter 148/373 - loss 0.06493239 - samples/sec: 18.44 - lr: 0.050000\n",
      "2022-05-06 22:20:43,571 epoch 58 - iter 185/373 - loss 0.06605560 - samples/sec: 17.69 - lr: 0.050000\n",
      "2022-05-06 22:20:52,236 epoch 58 - iter 222/373 - loss 0.06733259 - samples/sec: 18.16 - lr: 0.050000\n",
      "2022-05-06 22:21:00,792 epoch 58 - iter 259/373 - loss 0.06617864 - samples/sec: 18.36 - lr: 0.050000\n",
      "2022-05-06 22:21:09,407 epoch 58 - iter 296/373 - loss 0.06550315 - samples/sec: 18.26 - lr: 0.050000\n",
      "2022-05-06 22:21:19,441 epoch 58 - iter 333/373 - loss 0.06518164 - samples/sec: 15.58 - lr: 0.050000\n",
      "2022-05-06 22:21:28,481 epoch 58 - iter 370/373 - loss 0.06523568 - samples/sec: 17.35 - lr: 0.050000\n",
      "2022-05-06 22:21:29,725 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:21:29,726 EPOCH 58 done: loss 0.0651 - lr 0.050000\n",
      "2022-05-06 22:21:29,726 BAD EPOCHS (no improvement): 1\n",
      "2022-05-06 22:21:31,214 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:21:39,573 epoch 59 - iter 37/373 - loss 0.06631918 - samples/sec: 17.71 - lr: 0.050000\n",
      "2022-05-06 22:21:48,124 epoch 59 - iter 74/373 - loss 0.06724559 - samples/sec: 18.38 - lr: 0.050000\n",
      "2022-05-06 22:21:57,683 epoch 59 - iter 111/373 - loss 0.06503719 - samples/sec: 16.30 - lr: 0.050000\n",
      "2022-05-06 22:22:07,178 epoch 59 - iter 148/373 - loss 0.06409984 - samples/sec: 16.46 - lr: 0.050000\n",
      "2022-05-06 22:22:16,970 epoch 59 - iter 185/373 - loss 0.06461061 - samples/sec: 15.94 - lr: 0.050000\n",
      "2022-05-06 22:22:25,754 epoch 59 - iter 222/373 - loss 0.06352375 - samples/sec: 17.87 - lr: 0.050000\n",
      "2022-05-06 22:22:34,393 epoch 59 - iter 259/373 - loss 0.06356117 - samples/sec: 18.20 - lr: 0.050000\n",
      "2022-05-06 22:22:43,670 epoch 59 - iter 296/373 - loss 0.06435170 - samples/sec: 16.86 - lr: 0.050000\n",
      "2022-05-06 22:22:51,975 epoch 59 - iter 333/373 - loss 0.06327344 - samples/sec: 18.95 - lr: 0.050000\n",
      "2022-05-06 22:23:00,560 epoch 59 - iter 370/373 - loss 0.06267080 - samples/sec: 18.31 - lr: 0.050000\n",
      "2022-05-06 22:23:01,745 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:23:01,746 EPOCH 59 done: loss 0.0629 - lr 0.050000\n",
      "2022-05-06 22:23:01,747 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 22:23:03,101 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:23:12,264 epoch 60 - iter 37/373 - loss 0.06300771 - samples/sec: 16.16 - lr: 0.050000\n",
      "2022-05-06 22:23:22,080 epoch 60 - iter 74/373 - loss 0.05679650 - samples/sec: 15.87 - lr: 0.050000\n",
      "2022-05-06 22:23:30,475 epoch 60 - iter 111/373 - loss 0.05865051 - samples/sec: 18.74 - lr: 0.050000\n",
      "2022-05-06 22:23:38,928 epoch 60 - iter 148/373 - loss 0.06067474 - samples/sec: 18.61 - lr: 0.050000\n",
      "2022-05-06 22:23:47,865 epoch 60 - iter 185/373 - loss 0.06053929 - samples/sec: 17.56 - lr: 0.050000\n",
      "2022-05-06 22:23:56,870 epoch 60 - iter 222/373 - loss 0.06225244 - samples/sec: 17.41 - lr: 0.050000\n",
      "2022-05-06 22:24:06,019 epoch 60 - iter 259/373 - loss 0.06171061 - samples/sec: 17.15 - lr: 0.050000\n",
      "2022-05-06 22:24:14,594 epoch 60 - iter 296/373 - loss 0.06338029 - samples/sec: 18.34 - lr: 0.050000\n",
      "2022-05-06 22:24:23,683 epoch 60 - iter 333/373 - loss 0.06314602 - samples/sec: 17.18 - lr: 0.050000\n",
      "2022-05-06 22:24:31,570 epoch 60 - iter 370/373 - loss 0.06360939 - samples/sec: 20.04 - lr: 0.050000\n",
      "2022-05-06 22:24:33,018 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:24:33,018 EPOCH 60 done: loss 0.0639 - lr 0.050000\n",
      "2022-05-06 22:24:33,019 BAD EPOCHS (no improvement): 1\n",
      "2022-05-06 22:24:34,416 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:24:43,416 epoch 61 - iter 37/373 - loss 0.06027379 - samples/sec: 16.45 - lr: 0.050000\n",
      "2022-05-06 22:24:51,317 epoch 61 - iter 74/373 - loss 0.06120184 - samples/sec: 20.01 - lr: 0.050000\n",
      "2022-05-06 22:24:59,996 epoch 61 - iter 111/373 - loss 0.06242197 - samples/sec: 18.10 - lr: 0.050000\n",
      "2022-05-06 22:25:09,596 epoch 61 - iter 148/373 - loss 0.06428142 - samples/sec: 16.31 - lr: 0.050000\n",
      "2022-05-06 22:25:18,086 epoch 61 - iter 185/373 - loss 0.06229332 - samples/sec: 18.53 - lr: 0.050000\n",
      "2022-05-06 22:25:27,004 epoch 61 - iter 222/373 - loss 0.06305043 - samples/sec: 17.58 - lr: 0.050000\n",
      "2022-05-06 22:25:37,470 epoch 61 - iter 259/373 - loss 0.06140289 - samples/sec: 14.85 - lr: 0.050000\n",
      "2022-05-06 22:25:46,655 epoch 61 - iter 296/373 - loss 0.06122506 - samples/sec: 17.02 - lr: 0.050000\n",
      "2022-05-06 22:25:54,810 epoch 61 - iter 333/373 - loss 0.06185501 - samples/sec: 19.35 - lr: 0.050000\n",
      "2022-05-06 22:26:03,992 epoch 61 - iter 370/373 - loss 0.06143511 - samples/sec: 17.00 - lr: 0.050000\n",
      "2022-05-06 22:26:05,188 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:26:05,189 EPOCH 61 done: loss 0.0613 - lr 0.050000\n",
      "2022-05-06 22:26:05,190 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 22:26:06,648 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:26:15,301 epoch 62 - iter 37/373 - loss 0.07929555 - samples/sec: 17.11 - lr: 0.050000\n",
      "2022-05-06 22:26:24,835 epoch 62 - iter 74/373 - loss 0.06223846 - samples/sec: 16.40 - lr: 0.050000\n",
      "2022-05-06 22:26:33,958 epoch 62 - iter 111/373 - loss 0.06331517 - samples/sec: 17.17 - lr: 0.050000\n",
      "2022-05-06 22:26:43,327 epoch 62 - iter 148/373 - loss 0.06264893 - samples/sec: 16.68 - lr: 0.050000\n",
      "2022-05-06 22:26:52,951 epoch 62 - iter 185/373 - loss 0.06502041 - samples/sec: 16.23 - lr: 0.050000\n",
      "2022-05-06 22:27:01,624 epoch 62 - iter 222/373 - loss 0.06440188 - samples/sec: 18.20 - lr: 0.050000\n",
      "2022-05-06 22:27:10,252 epoch 62 - iter 259/373 - loss 0.06424266 - samples/sec: 18.21 - lr: 0.050000\n",
      "2022-05-06 22:27:19,347 epoch 62 - iter 296/373 - loss 0.06345252 - samples/sec: 17.17 - lr: 0.050000\n",
      "2022-05-06 22:27:27,848 epoch 62 - iter 333/373 - loss 0.06314409 - samples/sec: 18.49 - lr: 0.050000\n",
      "2022-05-06 22:27:36,509 epoch 62 - iter 370/373 - loss 0.06315939 - samples/sec: 18.12 - lr: 0.050000\n",
      "2022-05-06 22:27:37,954 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:27:37,955 EPOCH 62 done: loss 0.0628 - lr 0.050000\n",
      "2022-05-06 22:27:37,956 BAD EPOCHS (no improvement): 1\n",
      "2022-05-06 22:27:39,356 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:27:47,111 epoch 63 - iter 37/373 - loss 0.06221216 - samples/sec: 19.09 - lr: 0.050000\n",
      "2022-05-06 22:27:56,051 epoch 63 - iter 74/373 - loss 0.06231231 - samples/sec: 17.54 - lr: 0.050000\n",
      "2022-05-06 22:28:06,394 epoch 63 - iter 111/373 - loss 0.06187439 - samples/sec: 15.06 - lr: 0.050000\n",
      "2022-05-06 22:28:16,182 epoch 63 - iter 148/373 - loss 0.06180840 - samples/sec: 15.93 - lr: 0.050000\n",
      "2022-05-06 22:28:25,833 epoch 63 - iter 185/373 - loss 0.06330862 - samples/sec: 16.17 - lr: 0.050000\n",
      "2022-05-06 22:28:34,785 epoch 63 - iter 222/373 - loss 0.06149630 - samples/sec: 17.54 - lr: 0.050000\n",
      "2022-05-06 22:28:42,596 epoch 63 - iter 259/373 - loss 0.06118696 - samples/sec: 20.20 - lr: 0.050000\n",
      "2022-05-06 22:28:50,675 epoch 63 - iter 296/373 - loss 0.06279951 - samples/sec: 19.52 - lr: 0.050000\n",
      "2022-05-06 22:28:59,224 epoch 63 - iter 333/373 - loss 0.06315507 - samples/sec: 18.35 - lr: 0.050000\n",
      "2022-05-06 22:29:08,481 epoch 63 - iter 370/373 - loss 0.06397887 - samples/sec: 16.91 - lr: 0.050000\n",
      "2022-05-06 22:29:09,835 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:29:09,836 EPOCH 63 done: loss 0.0639 - lr 0.050000\n",
      "2022-05-06 22:29:09,836 BAD EPOCHS (no improvement): 2\n",
      "2022-05-06 22:29:11,261 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:29:19,491 epoch 64 - iter 37/373 - loss 0.06004962 - samples/sec: 17.99 - lr: 0.050000\n",
      "2022-05-06 22:29:28,949 epoch 64 - iter 74/373 - loss 0.06776621 - samples/sec: 16.53 - lr: 0.050000\n",
      "2022-05-06 22:29:38,547 epoch 64 - iter 111/373 - loss 0.06664553 - samples/sec: 16.29 - lr: 0.050000\n",
      "2022-05-06 22:29:46,621 epoch 64 - iter 148/373 - loss 0.06542195 - samples/sec: 19.53 - lr: 0.050000\n",
      "2022-05-06 22:29:56,102 epoch 64 - iter 185/373 - loss 0.06292365 - samples/sec: 16.50 - lr: 0.050000\n",
      "2022-05-06 22:30:04,932 epoch 64 - iter 222/373 - loss 0.06304289 - samples/sec: 17.76 - lr: 0.050000\n",
      "2022-05-06 22:30:14,045 epoch 64 - iter 259/373 - loss 0.06218115 - samples/sec: 17.16 - lr: 0.050000\n",
      "2022-05-06 22:30:23,344 epoch 64 - iter 296/373 - loss 0.06167675 - samples/sec: 16.84 - lr: 0.050000\n",
      "2022-05-06 22:30:31,512 epoch 64 - iter 333/373 - loss 0.06219615 - samples/sec: 19.25 - lr: 0.050000\n",
      "2022-05-06 22:30:40,669 epoch 64 - iter 370/373 - loss 0.06203520 - samples/sec: 17.07 - lr: 0.050000\n",
      "2022-05-06 22:30:41,824 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:30:41,825 EPOCH 64 done: loss 0.0621 - lr 0.050000\n",
      "2022-05-06 22:30:41,825 BAD EPOCHS (no improvement): 3\n",
      "2022-05-06 22:30:43,231 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:30:50,930 epoch 65 - iter 37/373 - loss 0.06318154 - samples/sec: 19.23 - lr: 0.050000\n",
      "2022-05-06 22:30:59,753 epoch 65 - iter 74/373 - loss 0.06151093 - samples/sec: 17.79 - lr: 0.050000\n",
      "2022-05-06 22:31:08,990 epoch 65 - iter 111/373 - loss 0.05859019 - samples/sec: 17.01 - lr: 0.050000\n",
      "2022-05-06 22:31:18,348 epoch 65 - iter 148/373 - loss 0.05929303 - samples/sec: 16.75 - lr: 0.050000\n",
      "2022-05-06 22:31:27,534 epoch 65 - iter 185/373 - loss 0.05825241 - samples/sec: 17.00 - lr: 0.050000\n",
      "2022-05-06 22:31:36,052 epoch 65 - iter 222/373 - loss 0.05805930 - samples/sec: 18.42 - lr: 0.050000\n",
      "2022-05-06 22:31:45,273 epoch 65 - iter 259/373 - loss 0.06024186 - samples/sec: 16.96 - lr: 0.050000\n",
      "2022-05-06 22:31:54,269 epoch 65 - iter 296/373 - loss 0.06198041 - samples/sec: 17.42 - lr: 0.050000\n",
      "2022-05-06 22:32:03,657 epoch 65 - iter 333/373 - loss 0.06245676 - samples/sec: 16.69 - lr: 0.050000\n",
      "2022-05-06 22:32:12,096 epoch 65 - iter 370/373 - loss 0.06251349 - samples/sec: 18.64 - lr: 0.050000\n",
      "2022-05-06 22:32:13,428 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:32:13,429 EPOCH 65 done: loss 0.0625 - lr 0.050000\n",
      "2022-05-06 22:32:13,429 Epoch    65: reducing learning rate of group 0 to 2.5000e-02.\n",
      "2022-05-06 22:32:13,430 BAD EPOCHS (no improvement): 4\n",
      "2022-05-06 22:32:14,876 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:32:23,289 epoch 66 - iter 37/373 - loss 0.07224864 - samples/sec: 17.60 - lr: 0.025000\n",
      "2022-05-06 22:32:31,540 epoch 66 - iter 74/373 - loss 0.06480322 - samples/sec: 19.18 - lr: 0.025000\n",
      "2022-05-06 22:32:40,372 epoch 66 - iter 111/373 - loss 0.06313589 - samples/sec: 17.74 - lr: 0.025000\n",
      "2022-05-06 22:32:49,179 epoch 66 - iter 148/373 - loss 0.06312166 - samples/sec: 17.77 - lr: 0.025000\n",
      "2022-05-06 22:32:59,390 epoch 66 - iter 185/373 - loss 0.06111122 - samples/sec: 15.22 - lr: 0.025000\n",
      "2022-05-06 22:33:07,733 epoch 66 - iter 222/373 - loss 0.06184606 - samples/sec: 18.85 - lr: 0.025000\n",
      "2022-05-06 22:33:16,087 epoch 66 - iter 259/373 - loss 0.06135685 - samples/sec: 18.84 - lr: 0.025000\n",
      "2022-05-06 22:33:25,132 epoch 66 - iter 296/373 - loss 0.06035670 - samples/sec: 17.33 - lr: 0.025000\n",
      "2022-05-06 22:33:34,424 epoch 66 - iter 333/373 - loss 0.06021805 - samples/sec: 16.87 - lr: 0.025000\n",
      "2022-05-06 22:33:43,054 epoch 66 - iter 370/373 - loss 0.06046985 - samples/sec: 18.26 - lr: 0.025000\n",
      "2022-05-06 22:33:44,479 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:33:44,479 EPOCH 66 done: loss 0.0604 - lr 0.025000\n",
      "2022-05-06 22:33:44,480 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 22:33:45,918 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:33:54,830 epoch 67 - iter 37/373 - loss 0.05503137 - samples/sec: 16.61 - lr: 0.025000\n",
      "2022-05-06 22:34:04,508 epoch 67 - iter 74/373 - loss 0.05874816 - samples/sec: 16.08 - lr: 0.025000\n",
      "2022-05-06 22:34:13,817 epoch 67 - iter 111/373 - loss 0.06024774 - samples/sec: 16.83 - lr: 0.025000\n",
      "2022-05-06 22:34:23,315 epoch 67 - iter 148/373 - loss 0.06284681 - samples/sec: 16.51 - lr: 0.025000\n",
      "2022-05-06 22:34:32,476 epoch 67 - iter 185/373 - loss 0.06091074 - samples/sec: 17.08 - lr: 0.025000\n",
      "2022-05-06 22:34:41,597 epoch 67 - iter 222/373 - loss 0.05994705 - samples/sec: 17.20 - lr: 0.025000\n",
      "2022-05-06 22:34:50,235 epoch 67 - iter 259/373 - loss 0.05889049 - samples/sec: 18.19 - lr: 0.025000\n",
      "2022-05-06 22:34:58,456 epoch 67 - iter 296/373 - loss 0.05994600 - samples/sec: 19.16 - lr: 0.025000\n",
      "2022-05-06 22:35:07,023 epoch 67 - iter 333/373 - loss 0.06068877 - samples/sec: 18.32 - lr: 0.025000\n",
      "2022-05-06 22:35:16,061 epoch 67 - iter 370/373 - loss 0.05958284 - samples/sec: 17.35 - lr: 0.025000\n",
      "2022-05-06 22:35:17,420 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:35:17,421 EPOCH 67 done: loss 0.0595 - lr 0.025000\n",
      "2022-05-06 22:35:17,421 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 22:35:18,825 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:35:26,450 epoch 68 - iter 37/373 - loss 0.05845991 - samples/sec: 19.41 - lr: 0.025000\n",
      "2022-05-06 22:35:34,465 epoch 68 - iter 74/373 - loss 0.05906095 - samples/sec: 19.65 - lr: 0.025000\n",
      "2022-05-06 22:35:43,565 epoch 68 - iter 111/373 - loss 0.05945566 - samples/sec: 17.29 - lr: 0.025000\n",
      "2022-05-06 22:35:53,172 epoch 68 - iter 148/373 - loss 0.06172425 - samples/sec: 16.24 - lr: 0.025000\n",
      "2022-05-06 22:36:02,312 epoch 68 - iter 185/373 - loss 0.06220955 - samples/sec: 17.12 - lr: 0.025000\n",
      "2022-05-06 22:36:11,774 epoch 68 - iter 222/373 - loss 0.06100175 - samples/sec: 16.54 - lr: 0.025000\n",
      "2022-05-06 22:36:21,026 epoch 68 - iter 259/373 - loss 0.05991680 - samples/sec: 16.99 - lr: 0.025000\n",
      "2022-05-06 22:36:29,239 epoch 68 - iter 296/373 - loss 0.05970547 - samples/sec: 19.20 - lr: 0.025000\n",
      "2022-05-06 22:36:38,337 epoch 68 - iter 333/373 - loss 0.05900040 - samples/sec: 17.27 - lr: 0.025000\n",
      "2022-05-06 22:36:48,277 epoch 68 - iter 370/373 - loss 0.05853344 - samples/sec: 15.67 - lr: 0.025000\n",
      "2022-05-06 22:36:49,479 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:36:49,480 EPOCH 68 done: loss 0.0585 - lr 0.025000\n",
      "2022-05-06 22:36:49,481 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 22:36:50,875 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:36:59,079 epoch 69 - iter 37/373 - loss 0.05925350 - samples/sec: 18.05 - lr: 0.025000\n",
      "2022-05-06 22:37:07,871 epoch 69 - iter 74/373 - loss 0.06355792 - samples/sec: 17.85 - lr: 0.025000\n",
      "2022-05-06 22:37:17,053 epoch 69 - iter 111/373 - loss 0.06239171 - samples/sec: 17.04 - lr: 0.025000\n",
      "2022-05-06 22:37:26,512 epoch 69 - iter 148/373 - loss 0.06143893 - samples/sec: 16.52 - lr: 0.025000\n",
      "2022-05-06 22:37:34,315 epoch 69 - iter 185/373 - loss 0.06010238 - samples/sec: 20.25 - lr: 0.025000\n",
      "2022-05-06 22:37:43,292 epoch 69 - iter 222/373 - loss 0.05922646 - samples/sec: 17.48 - lr: 0.025000\n",
      "2022-05-06 22:37:52,877 epoch 69 - iter 259/373 - loss 0.05921520 - samples/sec: 16.31 - lr: 0.025000\n",
      "2022-05-06 22:38:02,699 epoch 69 - iter 296/373 - loss 0.05856704 - samples/sec: 15.84 - lr: 0.025000\n",
      "2022-05-06 22:38:11,277 epoch 69 - iter 333/373 - loss 0.05730295 - samples/sec: 18.28 - lr: 0.025000\n",
      "2022-05-06 22:38:18,662 epoch 69 - iter 370/373 - loss 0.05802100 - samples/sec: 21.43 - lr: 0.025000\n",
      "2022-05-06 22:38:19,931 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:38:19,932 EPOCH 69 done: loss 0.0578 - lr 0.025000\n",
      "2022-05-06 22:38:19,932 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 22:38:21,340 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:38:29,286 epoch 70 - iter 37/373 - loss 0.05594929 - samples/sec: 18.64 - lr: 0.025000\n",
      "2022-05-06 22:38:39,170 epoch 70 - iter 74/373 - loss 0.05736047 - samples/sec: 15.76 - lr: 0.025000\n",
      "2022-05-06 22:38:46,845 epoch 70 - iter 111/373 - loss 0.05750086 - samples/sec: 20.67 - lr: 0.025000\n",
      "2022-05-06 22:38:56,787 epoch 70 - iter 148/373 - loss 0.05585833 - samples/sec: 15.66 - lr: 0.025000\n",
      "2022-05-06 22:39:05,669 epoch 70 - iter 185/373 - loss 0.05931523 - samples/sec: 17.69 - lr: 0.025000\n",
      "2022-05-06 22:39:15,447 epoch 70 - iter 222/373 - loss 0.05921911 - samples/sec: 15.93 - lr: 0.025000\n",
      "2022-05-06 22:39:24,134 epoch 70 - iter 259/373 - loss 0.05855187 - samples/sec: 18.03 - lr: 0.025000\n",
      "2022-05-06 22:39:32,656 epoch 70 - iter 296/373 - loss 0.05792202 - samples/sec: 18.41 - lr: 0.025000\n",
      "2022-05-06 22:39:41,970 epoch 70 - iter 333/373 - loss 0.05775220 - samples/sec: 16.80 - lr: 0.025000\n",
      "2022-05-06 22:39:50,734 epoch 70 - iter 370/373 - loss 0.05747478 - samples/sec: 17.91 - lr: 0.025000\n",
      "2022-05-06 22:39:51,804 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:39:51,805 EPOCH 70 done: loss 0.0576 - lr 0.025000\n",
      "2022-05-06 22:39:51,805 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 22:39:53,238 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:40:02,038 epoch 71 - iter 37/373 - loss 0.05695043 - samples/sec: 16.83 - lr: 0.025000\n",
      "2022-05-06 22:40:10,998 epoch 71 - iter 74/373 - loss 0.05233959 - samples/sec: 17.49 - lr: 0.025000\n",
      "2022-05-06 22:40:19,862 epoch 71 - iter 111/373 - loss 0.05548306 - samples/sec: 17.78 - lr: 0.025000\n",
      "2022-05-06 22:40:29,456 epoch 71 - iter 148/373 - loss 0.05511515 - samples/sec: 16.26 - lr: 0.025000\n",
      "2022-05-06 22:40:38,541 epoch 71 - iter 185/373 - loss 0.05697492 - samples/sec: 17.25 - lr: 0.025000\n",
      "2022-05-06 22:40:48,022 epoch 71 - iter 222/373 - loss 0.05598975 - samples/sec: 16.50 - lr: 0.025000\n",
      "2022-05-06 22:40:56,883 epoch 71 - iter 259/373 - loss 0.05626906 - samples/sec: 17.67 - lr: 0.025000\n",
      "2022-05-06 22:41:05,466 epoch 71 - iter 296/373 - loss 0.05542283 - samples/sec: 18.28 - lr: 0.025000\n",
      "2022-05-06 22:41:15,123 epoch 71 - iter 333/373 - loss 0.05566995 - samples/sec: 16.19 - lr: 0.025000\n",
      "2022-05-06 22:41:25,395 epoch 71 - iter 370/373 - loss 0.05666702 - samples/sec: 15.23 - lr: 0.025000\n",
      "2022-05-06 22:41:26,523 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:41:26,524 EPOCH 71 done: loss 0.0565 - lr 0.025000\n",
      "2022-05-06 22:41:26,525 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 22:41:27,990 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:41:37,318 epoch 72 - iter 37/373 - loss 0.06259303 - samples/sec: 15.87 - lr: 0.025000\n",
      "2022-05-06 22:41:46,700 epoch 72 - iter 74/373 - loss 0.06246735 - samples/sec: 16.73 - lr: 0.025000\n",
      "2022-05-06 22:41:55,807 epoch 72 - iter 111/373 - loss 0.06109473 - samples/sec: 17.20 - lr: 0.025000\n",
      "2022-05-06 22:42:04,738 epoch 72 - iter 148/373 - loss 0.06003712 - samples/sec: 17.62 - lr: 0.025000\n",
      "2022-05-06 22:42:13,912 epoch 72 - iter 185/373 - loss 0.05828850 - samples/sec: 17.02 - lr: 0.025000\n",
      "2022-05-06 22:42:22,470 epoch 72 - iter 222/373 - loss 0.05835465 - samples/sec: 18.34 - lr: 0.025000\n",
      "2022-05-06 22:42:31,831 epoch 72 - iter 259/373 - loss 0.05827319 - samples/sec: 16.66 - lr: 0.025000\n",
      "2022-05-06 22:42:40,556 epoch 72 - iter 296/373 - loss 0.05825576 - samples/sec: 18.00 - lr: 0.025000\n",
      "2022-05-06 22:42:48,883 epoch 72 - iter 333/373 - loss 0.05812989 - samples/sec: 18.91 - lr: 0.025000\n",
      "2022-05-06 22:42:58,082 epoch 72 - iter 370/373 - loss 0.05810644 - samples/sec: 17.02 - lr: 0.025000\n",
      "2022-05-06 22:42:58,997 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:42:58,998 EPOCH 72 done: loss 0.0583 - lr 0.025000\n",
      "2022-05-06 22:42:58,999 BAD EPOCHS (no improvement): 1\n",
      "2022-05-06 22:43:00,429 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:43:08,403 epoch 73 - iter 37/373 - loss 0.05820996 - samples/sec: 18.57 - lr: 0.025000\n",
      "2022-05-06 22:43:17,612 epoch 73 - iter 74/373 - loss 0.05454593 - samples/sec: 16.99 - lr: 0.025000\n",
      "2022-05-06 22:43:27,071 epoch 73 - iter 111/373 - loss 0.05330794 - samples/sec: 16.50 - lr: 0.025000\n",
      "2022-05-06 22:43:36,155 epoch 73 - iter 148/373 - loss 0.05501880 - samples/sec: 17.23 - lr: 0.025000\n",
      "2022-05-06 22:43:44,770 epoch 73 - iter 185/373 - loss 0.05455526 - samples/sec: 18.21 - lr: 0.025000\n",
      "2022-05-06 22:43:53,353 epoch 73 - iter 222/373 - loss 0.05623064 - samples/sec: 18.34 - lr: 0.025000\n",
      "2022-05-06 22:44:02,743 epoch 73 - iter 259/373 - loss 0.05665649 - samples/sec: 16.66 - lr: 0.025000\n",
      "2022-05-06 22:44:11,471 epoch 73 - iter 296/373 - loss 0.05739265 - samples/sec: 18.03 - lr: 0.025000\n",
      "2022-05-06 22:44:19,875 epoch 73 - iter 333/373 - loss 0.05703971 - samples/sec: 18.70 - lr: 0.025000\n",
      "2022-05-06 22:44:29,964 epoch 73 - iter 370/373 - loss 0.05652796 - samples/sec: 15.47 - lr: 0.025000\n",
      "2022-05-06 22:44:31,082 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:44:31,083 EPOCH 73 done: loss 0.0565 - lr 0.025000\n",
      "2022-05-06 22:44:31,083 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 22:44:32,587 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:44:41,148 epoch 74 - iter 37/373 - loss 0.04941318 - samples/sec: 17.29 - lr: 0.025000\n",
      "2022-05-06 22:44:49,958 epoch 74 - iter 74/373 - loss 0.04983402 - samples/sec: 17.79 - lr: 0.025000\n",
      "2022-05-06 22:44:59,040 epoch 74 - iter 111/373 - loss 0.04980400 - samples/sec: 17.24 - lr: 0.025000\n",
      "2022-05-06 22:45:08,176 epoch 74 - iter 148/373 - loss 0.05144226 - samples/sec: 17.09 - lr: 0.025000\n",
      "2022-05-06 22:45:17,109 epoch 74 - iter 185/373 - loss 0.05477072 - samples/sec: 17.53 - lr: 0.025000\n",
      "2022-05-06 22:45:25,946 epoch 74 - iter 222/373 - loss 0.05493580 - samples/sec: 17.79 - lr: 0.025000\n",
      "2022-05-06 22:45:34,277 epoch 74 - iter 259/373 - loss 0.05560616 - samples/sec: 18.88 - lr: 0.025000\n",
      "2022-05-06 22:45:44,065 epoch 74 - iter 296/373 - loss 0.05465655 - samples/sec: 16.03 - lr: 0.025000\n",
      "2022-05-06 22:45:54,037 epoch 74 - iter 333/373 - loss 0.05666807 - samples/sec: 15.70 - lr: 0.025000\n",
      "2022-05-06 22:46:02,139 epoch 74 - iter 370/373 - loss 0.05709938 - samples/sec: 19.46 - lr: 0.025000\n",
      "2022-05-06 22:46:03,419 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:46:03,420 EPOCH 74 done: loss 0.0570 - lr 0.025000\n",
      "2022-05-06 22:46:03,421 BAD EPOCHS (no improvement): 1\n",
      "2022-05-06 22:46:04,841 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:46:12,674 epoch 75 - iter 37/373 - loss 0.06022321 - samples/sec: 18.90 - lr: 0.025000\n",
      "2022-05-06 22:46:20,751 epoch 75 - iter 74/373 - loss 0.05887672 - samples/sec: 19.55 - lr: 0.025000\n",
      "2022-05-06 22:46:29,310 epoch 75 - iter 111/373 - loss 0.05557950 - samples/sec: 18.37 - lr: 0.025000\n",
      "2022-05-06 22:46:39,391 epoch 75 - iter 148/373 - loss 0.05405748 - samples/sec: 15.51 - lr: 0.025000\n",
      "2022-05-06 22:46:48,154 epoch 75 - iter 185/373 - loss 0.05469188 - samples/sec: 17.86 - lr: 0.025000\n",
      "2022-05-06 22:46:57,479 epoch 75 - iter 222/373 - loss 0.05385327 - samples/sec: 16.82 - lr: 0.025000\n",
      "2022-05-06 22:47:06,103 epoch 75 - iter 259/373 - loss 0.05366030 - samples/sec: 18.19 - lr: 0.025000\n",
      "2022-05-06 22:47:15,131 epoch 75 - iter 296/373 - loss 0.05440441 - samples/sec: 17.36 - lr: 0.025000\n",
      "2022-05-06 22:47:25,243 epoch 75 - iter 333/373 - loss 0.05529003 - samples/sec: 15.42 - lr: 0.025000\n",
      "2022-05-06 22:47:34,655 epoch 75 - iter 370/373 - loss 0.05569716 - samples/sec: 16.65 - lr: 0.025000\n",
      "2022-05-06 22:47:35,682 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:47:35,683 EPOCH 75 done: loss 0.0557 - lr 0.025000\n",
      "2022-05-06 22:47:35,684 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 22:47:37,202 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:47:45,732 epoch 76 - iter 37/373 - loss 0.05766231 - samples/sec: 17.35 - lr: 0.025000\n",
      "2022-05-06 22:47:55,111 epoch 76 - iter 74/373 - loss 0.05533657 - samples/sec: 16.72 - lr: 0.025000\n",
      "2022-05-06 22:48:03,521 epoch 76 - iter 111/373 - loss 0.05570537 - samples/sec: 18.72 - lr: 0.025000\n",
      "2022-05-06 22:48:12,962 epoch 76 - iter 148/373 - loss 0.05492350 - samples/sec: 16.55 - lr: 0.025000\n",
      "2022-05-06 22:48:21,639 epoch 76 - iter 185/373 - loss 0.05637468 - samples/sec: 18.15 - lr: 0.025000\n",
      "2022-05-06 22:48:31,177 epoch 76 - iter 222/373 - loss 0.05651860 - samples/sec: 16.36 - lr: 0.025000\n",
      "2022-05-06 22:48:39,674 epoch 76 - iter 259/373 - loss 0.05645296 - samples/sec: 18.47 - lr: 0.025000\n",
      "2022-05-06 22:48:48,309 epoch 76 - iter 296/373 - loss 0.05683815 - samples/sec: 18.21 - lr: 0.025000\n",
      "2022-05-06 22:48:57,912 epoch 76 - iter 333/373 - loss 0.05576551 - samples/sec: 16.26 - lr: 0.025000\n",
      "2022-05-06 22:49:06,530 epoch 76 - iter 370/373 - loss 0.05579593 - samples/sec: 18.22 - lr: 0.025000\n",
      "2022-05-06 22:49:07,820 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:49:07,821 EPOCH 76 done: loss 0.0560 - lr 0.025000\n",
      "2022-05-06 22:49:07,821 BAD EPOCHS (no improvement): 1\n",
      "2022-05-06 22:49:09,208 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:49:17,099 epoch 77 - iter 37/373 - loss 0.05759335 - samples/sec: 18.76 - lr: 0.025000\n",
      "2022-05-06 22:49:26,450 epoch 77 - iter 74/373 - loss 0.05486185 - samples/sec: 16.72 - lr: 0.025000\n",
      "2022-05-06 22:49:35,133 epoch 77 - iter 111/373 - loss 0.05768436 - samples/sec: 18.08 - lr: 0.025000\n",
      "2022-05-06 22:49:44,251 epoch 77 - iter 148/373 - loss 0.05804297 - samples/sec: 17.15 - lr: 0.025000\n",
      "2022-05-06 22:49:53,240 epoch 77 - iter 185/373 - loss 0.05765153 - samples/sec: 17.42 - lr: 0.025000\n",
      "2022-05-06 22:50:02,768 epoch 77 - iter 222/373 - loss 0.05811763 - samples/sec: 16.40 - lr: 0.025000\n",
      "2022-05-06 22:50:10,867 epoch 77 - iter 259/373 - loss 0.05701803 - samples/sec: 19.48 - lr: 0.025000\n",
      "2022-05-06 22:50:19,047 epoch 77 - iter 296/373 - loss 0.05699017 - samples/sec: 19.30 - lr: 0.025000\n",
      "2022-05-06 22:50:27,903 epoch 77 - iter 333/373 - loss 0.05616260 - samples/sec: 17.75 - lr: 0.025000\n",
      "2022-05-06 22:50:37,919 epoch 77 - iter 370/373 - loss 0.05505308 - samples/sec: 15.54 - lr: 0.025000\n",
      "2022-05-06 22:50:38,816 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:50:38,817 EPOCH 77 done: loss 0.0550 - lr 0.025000\n",
      "2022-05-06 22:50:38,817 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 22:50:40,229 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:50:48,667 epoch 78 - iter 37/373 - loss 0.05269954 - samples/sec: 17.55 - lr: 0.025000\n",
      "2022-05-06 22:50:58,069 epoch 78 - iter 74/373 - loss 0.04964143 - samples/sec: 16.62 - lr: 0.025000\n",
      "2022-05-06 22:51:07,271 epoch 78 - iter 111/373 - loss 0.05029110 - samples/sec: 17.07 - lr: 0.025000\n",
      "2022-05-06 22:51:16,267 epoch 78 - iter 148/373 - loss 0.05135529 - samples/sec: 17.46 - lr: 0.025000\n",
      "2022-05-06 22:51:26,470 epoch 78 - iter 185/373 - loss 0.05081823 - samples/sec: 15.29 - lr: 0.025000\n",
      "2022-05-06 22:51:34,705 epoch 78 - iter 222/373 - loss 0.05145729 - samples/sec: 19.16 - lr: 0.025000\n",
      "2022-05-06 22:51:43,647 epoch 78 - iter 259/373 - loss 0.05334373 - samples/sec: 17.60 - lr: 0.025000\n",
      "2022-05-06 22:51:53,061 epoch 78 - iter 296/373 - loss 0.05397098 - samples/sec: 16.62 - lr: 0.025000\n",
      "2022-05-06 22:52:02,695 epoch 78 - iter 333/373 - loss 0.05524307 - samples/sec: 16.24 - lr: 0.025000\n",
      "2022-05-06 22:52:11,991 epoch 78 - iter 370/373 - loss 0.05602070 - samples/sec: 16.86 - lr: 0.025000\n",
      "2022-05-06 22:52:13,167 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:52:13,168 EPOCH 78 done: loss 0.0563 - lr 0.025000\n",
      "2022-05-06 22:52:13,169 BAD EPOCHS (no improvement): 1\n",
      "2022-05-06 22:52:14,528 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:52:22,562 epoch 79 - iter 37/373 - loss 0.06415816 - samples/sec: 18.44 - lr: 0.025000\n",
      "2022-05-06 22:52:31,285 epoch 79 - iter 74/373 - loss 0.06690437 - samples/sec: 17.98 - lr: 0.025000\n",
      "2022-05-06 22:52:41,363 epoch 79 - iter 111/373 - loss 0.06024073 - samples/sec: 15.43 - lr: 0.025000\n",
      "2022-05-06 22:52:49,864 epoch 79 - iter 148/373 - loss 0.05871039 - samples/sec: 18.44 - lr: 0.025000\n",
      "2022-05-06 22:52:59,305 epoch 79 - iter 185/373 - loss 0.05728170 - samples/sec: 16.54 - lr: 0.025000\n",
      "2022-05-06 22:53:08,495 epoch 79 - iter 222/373 - loss 0.05527953 - samples/sec: 17.04 - lr: 0.025000\n",
      "2022-05-06 22:53:16,397 epoch 79 - iter 259/373 - loss 0.05471476 - samples/sec: 19.98 - lr: 0.025000\n",
      "2022-05-06 22:53:25,119 epoch 79 - iter 296/373 - loss 0.05524475 - samples/sec: 17.95 - lr: 0.025000\n",
      "2022-05-06 22:53:33,918 epoch 79 - iter 333/373 - loss 0.05540546 - samples/sec: 17.79 - lr: 0.025000\n",
      "2022-05-06 22:53:42,200 epoch 79 - iter 370/373 - loss 0.05596744 - samples/sec: 18.97 - lr: 0.025000\n",
      "2022-05-06 22:53:43,487 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:53:43,488 EPOCH 79 done: loss 0.0561 - lr 0.025000\n",
      "2022-05-06 22:53:43,488 BAD EPOCHS (no improvement): 2\n",
      "2022-05-06 22:53:44,900 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:53:53,153 epoch 80 - iter 37/373 - loss 0.06030063 - samples/sec: 17.94 - lr: 0.025000\n",
      "2022-05-06 22:54:02,285 epoch 80 - iter 74/373 - loss 0.05902458 - samples/sec: 17.14 - lr: 0.025000\n",
      "2022-05-06 22:54:11,538 epoch 80 - iter 111/373 - loss 0.05601883 - samples/sec: 16.90 - lr: 0.025000\n",
      "2022-05-06 22:54:19,744 epoch 80 - iter 148/373 - loss 0.05513608 - samples/sec: 19.22 - lr: 0.025000\n",
      "2022-05-06 22:54:28,325 epoch 80 - iter 185/373 - loss 0.05695757 - samples/sec: 18.34 - lr: 0.025000\n",
      "2022-05-06 22:54:36,464 epoch 80 - iter 222/373 - loss 0.05644319 - samples/sec: 19.34 - lr: 0.025000\n",
      "2022-05-06 22:54:45,497 epoch 80 - iter 259/373 - loss 0.05594850 - samples/sec: 17.48 - lr: 0.025000\n",
      "2022-05-06 22:54:54,110 epoch 80 - iter 296/373 - loss 0.05563629 - samples/sec: 18.25 - lr: 0.025000\n",
      "2022-05-06 22:55:03,744 epoch 80 - iter 333/373 - loss 0.05596508 - samples/sec: 16.16 - lr: 0.025000\n",
      "2022-05-06 22:55:12,892 epoch 80 - iter 370/373 - loss 0.05537493 - samples/sec: 17.07 - lr: 0.025000\n",
      "2022-05-06 22:55:14,036 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:55:14,037 EPOCH 80 done: loss 0.0558 - lr 0.025000\n",
      "2022-05-06 22:55:14,038 BAD EPOCHS (no improvement): 3\n",
      "2022-05-06 22:55:15,456 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:55:23,529 epoch 81 - iter 37/373 - loss 0.04862086 - samples/sec: 18.34 - lr: 0.025000\n",
      "2022-05-06 22:55:32,052 epoch 81 - iter 74/373 - loss 0.04780956 - samples/sec: 18.44 - lr: 0.025000\n",
      "2022-05-06 22:55:40,672 epoch 81 - iter 111/373 - loss 0.04619071 - samples/sec: 18.21 - lr: 0.025000\n",
      "2022-05-06 22:55:49,401 epoch 81 - iter 148/373 - loss 0.04645092 - samples/sec: 17.96 - lr: 0.025000\n",
      "2022-05-06 22:55:59,581 epoch 81 - iter 185/373 - loss 0.04856676 - samples/sec: 15.32 - lr: 0.025000\n",
      "2022-05-06 22:56:08,996 epoch 81 - iter 222/373 - loss 0.05107734 - samples/sec: 16.60 - lr: 0.025000\n",
      "2022-05-06 22:56:18,227 epoch 81 - iter 259/373 - loss 0.05264069 - samples/sec: 17.07 - lr: 0.025000\n",
      "2022-05-06 22:56:27,205 epoch 81 - iter 296/373 - loss 0.05413347 - samples/sec: 17.41 - lr: 0.025000\n",
      "2022-05-06 22:56:36,583 epoch 81 - iter 333/373 - loss 0.05409065 - samples/sec: 16.65 - lr: 0.025000\n",
      "2022-05-06 22:56:45,173 epoch 81 - iter 370/373 - loss 0.05529307 - samples/sec: 18.28 - lr: 0.025000\n",
      "2022-05-06 22:56:46,282 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:56:46,283 EPOCH 81 done: loss 0.0552 - lr 0.025000\n",
      "2022-05-06 22:56:46,284 Epoch    81: reducing learning rate of group 0 to 1.2500e-02.\n",
      "2022-05-06 22:56:46,284 BAD EPOCHS (no improvement): 4\n",
      "2022-05-06 22:56:47,694 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:56:56,825 epoch 82 - iter 37/373 - loss 0.05982527 - samples/sec: 16.21 - lr: 0.012500\n",
      "2022-05-06 22:57:05,939 epoch 82 - iter 74/373 - loss 0.05493905 - samples/sec: 17.18 - lr: 0.012500\n",
      "2022-05-06 22:57:15,046 epoch 82 - iter 111/373 - loss 0.05379998 - samples/sec: 17.20 - lr: 0.012500\n",
      "2022-05-06 22:57:24,148 epoch 82 - iter 148/373 - loss 0.05515514 - samples/sec: 17.20 - lr: 0.012500\n",
      "2022-05-06 22:57:32,566 epoch 82 - iter 185/373 - loss 0.05436259 - samples/sec: 18.69 - lr: 0.012500\n",
      "2022-05-06 22:57:42,023 epoch 82 - iter 222/373 - loss 0.05371201 - samples/sec: 16.53 - lr: 0.012500\n",
      "2022-05-06 22:57:50,648 epoch 82 - iter 259/373 - loss 0.05315948 - samples/sec: 18.18 - lr: 0.012500\n",
      "2022-05-06 22:58:00,628 epoch 82 - iter 296/373 - loss 0.05418218 - samples/sec: 15.59 - lr: 0.012500\n",
      "2022-05-06 22:58:10,040 epoch 82 - iter 333/373 - loss 0.05449923 - samples/sec: 16.64 - lr: 0.012500\n",
      "2022-05-06 22:58:18,172 epoch 82 - iter 370/373 - loss 0.05514022 - samples/sec: 19.43 - lr: 0.012500\n",
      "2022-05-06 22:58:19,687 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:58:19,687 EPOCH 82 done: loss 0.0551 - lr 0.012500\n",
      "2022-05-06 22:58:19,688 BAD EPOCHS (no improvement): 1\n",
      "2022-05-06 22:58:21,131 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:58:29,136 epoch 83 - iter 37/373 - loss 0.06101653 - samples/sec: 18.50 - lr: 0.012500\n",
      "2022-05-06 22:58:37,636 epoch 83 - iter 74/373 - loss 0.05581550 - samples/sec: 18.51 - lr: 0.012500\n",
      "2022-05-06 22:58:47,200 epoch 83 - iter 111/373 - loss 0.05415423 - samples/sec: 16.36 - lr: 0.012500\n",
      "2022-05-06 22:58:56,589 epoch 83 - iter 148/373 - loss 0.05589780 - samples/sec: 16.67 - lr: 0.012500\n",
      "2022-05-06 22:59:05,446 epoch 83 - iter 185/373 - loss 0.05441573 - samples/sec: 17.74 - lr: 0.012500\n",
      "2022-05-06 22:59:13,804 epoch 83 - iter 222/373 - loss 0.05522266 - samples/sec: 18.82 - lr: 0.012500\n",
      "2022-05-06 22:59:22,661 epoch 83 - iter 259/373 - loss 0.05520482 - samples/sec: 17.70 - lr: 0.012500\n",
      "2022-05-06 22:59:32,491 epoch 83 - iter 296/373 - loss 0.05430211 - samples/sec: 15.87 - lr: 0.012500\n",
      "2022-05-06 22:59:42,227 epoch 83 - iter 333/373 - loss 0.05436590 - samples/sec: 16.04 - lr: 0.012500\n",
      "2022-05-06 22:59:51,996 epoch 83 - iter 370/373 - loss 0.05437421 - samples/sec: 16.00 - lr: 0.012500\n",
      "2022-05-06 22:59:52,842 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 22:59:52,843 EPOCH 83 done: loss 0.0546 - lr 0.012500\n",
      "2022-05-06 22:59:52,843 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 22:59:54,303 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:00:03,168 epoch 84 - iter 37/373 - loss 0.05790311 - samples/sec: 16.70 - lr: 0.012500\n",
      "2022-05-06 23:00:11,364 epoch 84 - iter 74/373 - loss 0.05266776 - samples/sec: 19.27 - lr: 0.012500\n",
      "2022-05-06 23:00:20,832 epoch 84 - iter 111/373 - loss 0.05548612 - samples/sec: 16.51 - lr: 0.012500\n",
      "2022-05-06 23:00:30,570 epoch 84 - iter 148/373 - loss 0.05766675 - samples/sec: 16.02 - lr: 0.012500\n",
      "2022-05-06 23:00:39,314 epoch 84 - iter 185/373 - loss 0.05522069 - samples/sec: 17.94 - lr: 0.012500\n",
      "2022-05-06 23:00:48,895 epoch 84 - iter 222/373 - loss 0.05646590 - samples/sec: 16.31 - lr: 0.012500\n",
      "2022-05-06 23:00:57,977 epoch 84 - iter 259/373 - loss 0.05558624 - samples/sec: 17.27 - lr: 0.012500\n",
      "2022-05-06 23:01:06,544 epoch 84 - iter 296/373 - loss 0.05600398 - samples/sec: 18.31 - lr: 0.012500\n",
      "2022-05-06 23:01:15,447 epoch 84 - iter 333/373 - loss 0.05584461 - samples/sec: 17.62 - lr: 0.012500\n",
      "2022-05-06 23:01:24,689 epoch 84 - iter 370/373 - loss 0.05553869 - samples/sec: 16.92 - lr: 0.012500\n",
      "2022-05-06 23:01:25,832 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:01:25,833 EPOCH 84 done: loss 0.0555 - lr 0.012500\n",
      "2022-05-06 23:01:25,834 BAD EPOCHS (no improvement): 1\n",
      "2022-05-06 23:01:27,260 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:01:35,578 epoch 85 - iter 37/373 - loss 0.04508938 - samples/sec: 17.80 - lr: 0.012500\n",
      "2022-05-06 23:01:45,003 epoch 85 - iter 74/373 - loss 0.04951962 - samples/sec: 16.57 - lr: 0.012500\n",
      "2022-05-06 23:01:53,956 epoch 85 - iter 111/373 - loss 0.04968209 - samples/sec: 17.51 - lr: 0.012500\n",
      "2022-05-06 23:02:02,836 epoch 85 - iter 148/373 - loss 0.05144517 - samples/sec: 17.65 - lr: 0.012500\n",
      "2022-05-06 23:02:12,251 epoch 85 - iter 185/373 - loss 0.05138636 - samples/sec: 16.57 - lr: 0.012500\n",
      "2022-05-06 23:02:21,524 epoch 85 - iter 222/373 - loss 0.05075335 - samples/sec: 16.90 - lr: 0.012500\n",
      "2022-05-06 23:02:31,004 epoch 85 - iter 259/373 - loss 0.05306957 - samples/sec: 16.48 - lr: 0.012500\n",
      "2022-05-06 23:02:39,469 epoch 85 - iter 296/373 - loss 0.05431625 - samples/sec: 18.54 - lr: 0.012500\n",
      "2022-05-06 23:02:47,917 epoch 85 - iter 333/373 - loss 0.05523808 - samples/sec: 18.65 - lr: 0.012500\n",
      "2022-05-06 23:02:56,657 epoch 85 - iter 370/373 - loss 0.05446900 - samples/sec: 17.95 - lr: 0.012500\n",
      "2022-05-06 23:02:57,877 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:02:57,877 EPOCH 85 done: loss 0.0548 - lr 0.012500\n",
      "2022-05-06 23:02:57,878 BAD EPOCHS (no improvement): 2\n",
      "2022-05-06 23:02:59,305 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:03:08,105 epoch 86 - iter 37/373 - loss 0.05287936 - samples/sec: 16.83 - lr: 0.012500\n",
      "2022-05-06 23:03:16,695 epoch 86 - iter 74/373 - loss 0.05271671 - samples/sec: 18.30 - lr: 0.012500\n",
      "2022-05-06 23:03:25,825 epoch 86 - iter 111/373 - loss 0.05581776 - samples/sec: 17.19 - lr: 0.012500\n",
      "2022-05-06 23:03:33,980 epoch 86 - iter 148/373 - loss 0.05538359 - samples/sec: 19.31 - lr: 0.012500\n",
      "2022-05-06 23:03:43,248 epoch 86 - iter 185/373 - loss 0.05608927 - samples/sec: 16.85 - lr: 0.012500\n",
      "2022-05-06 23:03:52,971 epoch 86 - iter 222/373 - loss 0.05454333 - samples/sec: 16.07 - lr: 0.012500\n",
      "2022-05-06 23:04:02,407 epoch 86 - iter 259/373 - loss 0.05408137 - samples/sec: 16.60 - lr: 0.012500\n",
      "2022-05-06 23:04:11,298 epoch 86 - iter 296/373 - loss 0.05382167 - samples/sec: 17.67 - lr: 0.012500\n",
      "2022-05-06 23:04:20,290 epoch 86 - iter 333/373 - loss 0.05422073 - samples/sec: 17.44 - lr: 0.012500\n",
      "2022-05-06 23:04:28,677 epoch 86 - iter 370/373 - loss 0.05373610 - samples/sec: 18.75 - lr: 0.012500\n",
      "2022-05-06 23:04:30,020 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:04:30,021 EPOCH 86 done: loss 0.0538 - lr 0.012500\n",
      "2022-05-06 23:04:30,022 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 23:04:31,449 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:04:40,771 epoch 87 - iter 37/373 - loss 0.05523940 - samples/sec: 15.88 - lr: 0.012500\n",
      "2022-05-06 23:04:50,593 epoch 87 - iter 74/373 - loss 0.05857583 - samples/sec: 15.85 - lr: 0.012500\n",
      "2022-05-06 23:04:59,020 epoch 87 - iter 111/373 - loss 0.05746281 - samples/sec: 18.60 - lr: 0.012500\n",
      "2022-05-06 23:05:07,836 epoch 87 - iter 148/373 - loss 0.05288256 - samples/sec: 17.76 - lr: 0.012500\n",
      "2022-05-06 23:05:16,045 epoch 87 - iter 185/373 - loss 0.05238391 - samples/sec: 19.18 - lr: 0.012500\n",
      "2022-05-06 23:05:25,883 epoch 87 - iter 222/373 - loss 0.05189206 - samples/sec: 15.85 - lr: 0.012500\n",
      "2022-05-06 23:05:35,149 epoch 87 - iter 259/373 - loss 0.05220798 - samples/sec: 16.87 - lr: 0.012500\n",
      "2022-05-06 23:05:42,779 epoch 87 - iter 296/373 - loss 0.05247190 - samples/sec: 20.76 - lr: 0.012500\n",
      "2022-05-06 23:05:51,558 epoch 87 - iter 333/373 - loss 0.05318867 - samples/sec: 17.87 - lr: 0.012500\n",
      "2022-05-06 23:06:00,094 epoch 87 - iter 370/373 - loss 0.05346715 - samples/sec: 18.39 - lr: 0.012500\n",
      "2022-05-06 23:06:01,340 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:06:01,340 EPOCH 87 done: loss 0.0535 - lr 0.012500\n",
      "2022-05-06 23:06:01,341 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 23:06:02,687 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:06:11,715 epoch 88 - iter 37/373 - loss 0.05794232 - samples/sec: 16.40 - lr: 0.012500\n",
      "2022-05-06 23:06:20,373 epoch 88 - iter 74/373 - loss 0.05948434 - samples/sec: 18.11 - lr: 0.012500\n",
      "2022-05-06 23:06:30,223 epoch 88 - iter 111/373 - loss 0.05753216 - samples/sec: 15.81 - lr: 0.012500\n",
      "2022-05-06 23:06:39,077 epoch 88 - iter 148/373 - loss 0.05752940 - samples/sec: 17.73 - lr: 0.012500\n",
      "2022-05-06 23:06:47,507 epoch 88 - iter 185/373 - loss 0.05655145 - samples/sec: 18.70 - lr: 0.012500\n",
      "2022-05-06 23:06:56,519 epoch 88 - iter 222/373 - loss 0.05486860 - samples/sec: 17.37 - lr: 0.012500\n",
      "2022-05-06 23:07:05,618 epoch 88 - iter 259/373 - loss 0.05390539 - samples/sec: 17.21 - lr: 0.012500\n",
      "2022-05-06 23:07:14,512 epoch 88 - iter 296/373 - loss 0.05252151 - samples/sec: 17.61 - lr: 0.012500\n",
      "2022-05-06 23:07:23,237 epoch 88 - iter 333/373 - loss 0.05304823 - samples/sec: 18.05 - lr: 0.012500\n",
      "2022-05-06 23:07:31,236 epoch 88 - iter 370/373 - loss 0.05297384 - samples/sec: 19.72 - lr: 0.012500\n",
      "2022-05-06 23:07:32,424 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:07:32,425 EPOCH 88 done: loss 0.0529 - lr 0.012500\n",
      "2022-05-06 23:07:32,426 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 23:07:33,803 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:07:42,799 epoch 89 - iter 37/373 - loss 0.04827160 - samples/sec: 16.46 - lr: 0.012500\n",
      "2022-05-06 23:07:52,384 epoch 89 - iter 74/373 - loss 0.04894977 - samples/sec: 16.26 - lr: 0.012500\n",
      "2022-05-06 23:08:01,116 epoch 89 - iter 111/373 - loss 0.05092797 - samples/sec: 17.94 - lr: 0.012500\n",
      "2022-05-06 23:08:09,434 epoch 89 - iter 148/373 - loss 0.05033103 - samples/sec: 18.92 - lr: 0.012500\n",
      "2022-05-06 23:08:16,969 epoch 89 - iter 185/373 - loss 0.04909748 - samples/sec: 21.00 - lr: 0.012500\n",
      "2022-05-06 23:08:26,756 epoch 89 - iter 222/373 - loss 0.04946487 - samples/sec: 15.92 - lr: 0.012500\n",
      "2022-05-06 23:08:35,470 epoch 89 - iter 259/373 - loss 0.05107605 - samples/sec: 18.06 - lr: 0.012500\n",
      "2022-05-06 23:08:46,056 epoch 89 - iter 296/373 - loss 0.05128249 - samples/sec: 14.73 - lr: 0.012500\n",
      "2022-05-06 23:08:54,686 epoch 89 - iter 333/373 - loss 0.05109141 - samples/sec: 18.19 - lr: 0.012500\n",
      "2022-05-06 23:09:03,521 epoch 89 - iter 370/373 - loss 0.05160403 - samples/sec: 17.74 - lr: 0.012500\n",
      "2022-05-06 23:09:04,544 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:09:04,545 EPOCH 89 done: loss 0.0517 - lr 0.012500\n",
      "2022-05-06 23:09:04,545 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 23:09:05,964 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:09:15,049 epoch 90 - iter 37/373 - loss 0.05088027 - samples/sec: 16.30 - lr: 0.012500\n",
      "2022-05-06 23:09:24,319 epoch 90 - iter 74/373 - loss 0.05055435 - samples/sec: 16.84 - lr: 0.012500\n",
      "2022-05-06 23:09:32,418 epoch 90 - iter 111/373 - loss 0.04944944 - samples/sec: 19.47 - lr: 0.012500\n",
      "2022-05-06 23:09:40,991 epoch 90 - iter 148/373 - loss 0.04931556 - samples/sec: 18.32 - lr: 0.012500\n",
      "2022-05-06 23:09:50,233 epoch 90 - iter 185/373 - loss 0.05052718 - samples/sec: 16.93 - lr: 0.012500\n",
      "2022-05-06 23:09:58,457 epoch 90 - iter 222/373 - loss 0.05315388 - samples/sec: 19.13 - lr: 0.012500\n",
      "2022-05-06 23:10:08,882 epoch 90 - iter 259/373 - loss 0.05364541 - samples/sec: 14.90 - lr: 0.012500\n",
      "2022-05-06 23:10:17,990 epoch 90 - iter 296/373 - loss 0.05418303 - samples/sec: 17.19 - lr: 0.012500\n",
      "2022-05-06 23:10:26,438 epoch 90 - iter 333/373 - loss 0.05310429 - samples/sec: 18.56 - lr: 0.012500\n",
      "2022-05-06 23:10:34,860 epoch 90 - iter 370/373 - loss 0.05324500 - samples/sec: 18.61 - lr: 0.012500\n",
      "2022-05-06 23:10:36,207 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:10:36,208 EPOCH 90 done: loss 0.0532 - lr 0.012500\n",
      "2022-05-06 23:10:36,208 BAD EPOCHS (no improvement): 1\n",
      "2022-05-06 23:10:37,625 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:10:45,979 epoch 91 - iter 37/373 - loss 0.05321756 - samples/sec: 17.72 - lr: 0.012500\n",
      "2022-05-06 23:10:54,202 epoch 91 - iter 74/373 - loss 0.05212115 - samples/sec: 19.20 - lr: 0.012500\n",
      "2022-05-06 23:11:03,396 epoch 91 - iter 111/373 - loss 0.04976644 - samples/sec: 17.01 - lr: 0.012500\n",
      "2022-05-06 23:11:12,504 epoch 91 - iter 148/373 - loss 0.05122374 - samples/sec: 17.17 - lr: 0.012500\n",
      "2022-05-06 23:11:21,987 epoch 91 - iter 185/373 - loss 0.04921247 - samples/sec: 16.47 - lr: 0.012500\n",
      "2022-05-06 23:11:31,247 epoch 91 - iter 222/373 - loss 0.05189958 - samples/sec: 16.90 - lr: 0.012500\n",
      "2022-05-06 23:11:40,884 epoch 91 - iter 259/373 - loss 0.05416186 - samples/sec: 16.22 - lr: 0.012500\n",
      "2022-05-06 23:11:49,324 epoch 91 - iter 296/373 - loss 0.05427929 - samples/sec: 18.57 - lr: 0.012500\n",
      "2022-05-06 23:11:58,126 epoch 91 - iter 333/373 - loss 0.05361958 - samples/sec: 17.83 - lr: 0.012500\n",
      "2022-05-06 23:12:06,427 epoch 91 - iter 370/373 - loss 0.05305455 - samples/sec: 19.04 - lr: 0.012500\n",
      "2022-05-06 23:12:07,958 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:12:07,959 EPOCH 91 done: loss 0.0529 - lr 0.012500\n",
      "2022-05-06 23:12:07,960 BAD EPOCHS (no improvement): 2\n",
      "2022-05-06 23:12:09,431 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:12:18,519 epoch 92 - iter 37/373 - loss 0.04968816 - samples/sec: 16.29 - lr: 0.012500\n",
      "2022-05-06 23:12:28,818 epoch 92 - iter 74/373 - loss 0.05349573 - samples/sec: 15.17 - lr: 0.012500\n",
      "2022-05-06 23:12:38,057 epoch 92 - iter 111/373 - loss 0.04969607 - samples/sec: 16.92 - lr: 0.012500\n",
      "2022-05-06 23:12:46,783 epoch 92 - iter 148/373 - loss 0.05166438 - samples/sec: 18.01 - lr: 0.012500\n",
      "2022-05-06 23:12:55,993 epoch 92 - iter 185/373 - loss 0.05116229 - samples/sec: 17.00 - lr: 0.012500\n",
      "2022-05-06 23:13:05,011 epoch 92 - iter 222/373 - loss 0.05058848 - samples/sec: 17.44 - lr: 0.012500\n",
      "2022-05-06 23:13:13,674 epoch 92 - iter 259/373 - loss 0.05081920 - samples/sec: 18.28 - lr: 0.012500\n",
      "2022-05-06 23:13:23,081 epoch 92 - iter 296/373 - loss 0.05096511 - samples/sec: 16.61 - lr: 0.012500\n",
      "2022-05-06 23:13:31,925 epoch 92 - iter 333/373 - loss 0.05128252 - samples/sec: 17.74 - lr: 0.012500\n",
      "2022-05-06 23:13:41,042 epoch 92 - iter 370/373 - loss 0.05110069 - samples/sec: 17.13 - lr: 0.012500\n",
      "2022-05-06 23:13:42,392 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:13:42,392 EPOCH 92 done: loss 0.0510 - lr 0.012500\n",
      "2022-05-06 23:13:42,393 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 23:13:43,833 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:13:52,231 epoch 93 - iter 37/373 - loss 0.04406102 - samples/sec: 17.63 - lr: 0.012500\n",
      "2022-05-06 23:14:00,878 epoch 93 - iter 74/373 - loss 0.04717984 - samples/sec: 18.17 - lr: 0.012500\n",
      "2022-05-06 23:14:09,943 epoch 93 - iter 111/373 - loss 0.04978292 - samples/sec: 17.29 - lr: 0.012500\n",
      "2022-05-06 23:14:18,818 epoch 93 - iter 148/373 - loss 0.04906739 - samples/sec: 17.69 - lr: 0.012500\n",
      "2022-05-06 23:14:28,121 epoch 93 - iter 185/373 - loss 0.05062828 - samples/sec: 16.79 - lr: 0.012500\n",
      "2022-05-06 23:14:37,719 epoch 93 - iter 222/373 - loss 0.05020728 - samples/sec: 16.33 - lr: 0.012500\n",
      "2022-05-06 23:14:47,305 epoch 93 - iter 259/373 - loss 0.04997284 - samples/sec: 16.26 - lr: 0.012500\n",
      "2022-05-06 23:14:56,496 epoch 93 - iter 296/373 - loss 0.05100745 - samples/sec: 17.01 - lr: 0.012500\n",
      "2022-05-06 23:15:05,945 epoch 93 - iter 333/373 - loss 0.05193443 - samples/sec: 16.53 - lr: 0.012500\n",
      "2022-05-06 23:15:13,255 epoch 93 - iter 370/373 - loss 0.05184376 - samples/sec: 21.71 - lr: 0.012500\n",
      "2022-05-06 23:15:14,410 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:15:14,410 EPOCH 93 done: loss 0.0517 - lr 0.012500\n",
      "2022-05-06 23:15:14,411 BAD EPOCHS (no improvement): 1\n",
      "2022-05-06 23:15:15,817 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:15:24,542 epoch 94 - iter 37/373 - loss 0.03931460 - samples/sec: 16.97 - lr: 0.012500\n",
      "2022-05-06 23:15:33,500 epoch 94 - iter 74/373 - loss 0.04591898 - samples/sec: 17.52 - lr: 0.012500\n",
      "2022-05-06 23:15:42,356 epoch 94 - iter 111/373 - loss 0.04869591 - samples/sec: 17.76 - lr: 0.012500\n",
      "2022-05-06 23:15:52,621 epoch 94 - iter 148/373 - loss 0.05083665 - samples/sec: 15.14 - lr: 0.012500\n",
      "2022-05-06 23:16:01,387 epoch 94 - iter 185/373 - loss 0.05246064 - samples/sec: 17.89 - lr: 0.012500\n",
      "2022-05-06 23:16:09,737 epoch 94 - iter 222/373 - loss 0.05182811 - samples/sec: 18.81 - lr: 0.012500\n",
      "2022-05-06 23:16:18,225 epoch 94 - iter 259/373 - loss 0.05020838 - samples/sec: 18.49 - lr: 0.012500\n",
      "2022-05-06 23:16:27,933 epoch 94 - iter 296/373 - loss 0.05121952 - samples/sec: 16.05 - lr: 0.012500\n",
      "2022-05-06 23:16:36,632 epoch 94 - iter 333/373 - loss 0.05144248 - samples/sec: 18.04 - lr: 0.012500\n",
      "2022-05-06 23:16:45,051 epoch 94 - iter 370/373 - loss 0.05167215 - samples/sec: 18.73 - lr: 0.012500\n",
      "2022-05-06 23:16:46,524 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:16:46,525 EPOCH 94 done: loss 0.0520 - lr 0.012500\n",
      "2022-05-06 23:16:46,525 BAD EPOCHS (no improvement): 2\n",
      "2022-05-06 23:16:47,998 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:16:56,737 epoch 95 - iter 37/373 - loss 0.04602251 - samples/sec: 16.94 - lr: 0.012500\n",
      "2022-05-06 23:17:05,709 epoch 95 - iter 74/373 - loss 0.05122978 - samples/sec: 17.42 - lr: 0.012500\n",
      "2022-05-06 23:17:14,962 epoch 95 - iter 111/373 - loss 0.04867242 - samples/sec: 16.90 - lr: 0.012500\n",
      "2022-05-06 23:17:24,983 epoch 95 - iter 148/373 - loss 0.05028554 - samples/sec: 15.56 - lr: 0.012500\n",
      "2022-05-06 23:17:33,925 epoch 95 - iter 185/373 - loss 0.04842256 - samples/sec: 17.53 - lr: 0.012500\n",
      "2022-05-06 23:17:43,343 epoch 95 - iter 222/373 - loss 0.05034834 - samples/sec: 16.59 - lr: 0.012500\n",
      "2022-05-06 23:17:52,902 epoch 95 - iter 259/373 - loss 0.04925951 - samples/sec: 16.32 - lr: 0.012500\n",
      "2022-05-06 23:18:02,402 epoch 95 - iter 296/373 - loss 0.05064995 - samples/sec: 16.46 - lr: 0.012500\n",
      "2022-05-06 23:18:10,997 epoch 95 - iter 333/373 - loss 0.04991913 - samples/sec: 18.29 - lr: 0.012500\n",
      "2022-05-06 23:18:18,364 epoch 95 - iter 370/373 - loss 0.05024127 - samples/sec: 21.53 - lr: 0.012500\n",
      "2022-05-06 23:18:19,384 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:18:19,385 EPOCH 95 done: loss 0.0501 - lr 0.012500\n",
      "2022-05-06 23:18:19,385 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 23:18:20,798 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:18:28,741 epoch 96 - iter 37/373 - loss 0.05137662 - samples/sec: 18.64 - lr: 0.012500\n",
      "2022-05-06 23:18:37,643 epoch 96 - iter 74/373 - loss 0.05090481 - samples/sec: 17.61 - lr: 0.012500\n",
      "2022-05-06 23:18:45,684 epoch 96 - iter 111/373 - loss 0.05321678 - samples/sec: 19.62 - lr: 0.012500\n",
      "2022-05-06 23:18:54,539 epoch 96 - iter 148/373 - loss 0.05395562 - samples/sec: 17.71 - lr: 0.012500\n",
      "2022-05-06 23:19:04,522 epoch 96 - iter 185/373 - loss 0.05397479 - samples/sec: 15.59 - lr: 0.012500\n",
      "2022-05-06 23:19:14,009 epoch 96 - iter 222/373 - loss 0.05249667 - samples/sec: 16.49 - lr: 0.012500\n",
      "2022-05-06 23:19:23,530 epoch 96 - iter 259/373 - loss 0.05222440 - samples/sec: 16.42 - lr: 0.012500\n",
      "2022-05-06 23:19:33,387 epoch 96 - iter 296/373 - loss 0.05127604 - samples/sec: 15.82 - lr: 0.012500\n",
      "2022-05-06 23:19:41,951 epoch 96 - iter 333/373 - loss 0.05234205 - samples/sec: 18.50 - lr: 0.012500\n",
      "2022-05-06 23:19:51,256 epoch 96 - iter 370/373 - loss 0.05175505 - samples/sec: 16.83 - lr: 0.012500\n",
      "2022-05-06 23:19:52,448 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:19:52,449 EPOCH 96 done: loss 0.0519 - lr 0.012500\n",
      "2022-05-06 23:19:52,449 BAD EPOCHS (no improvement): 1\n",
      "2022-05-06 23:19:53,905 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:20:02,276 epoch 97 - iter 37/373 - loss 0.05105349 - samples/sec: 17.69 - lr: 0.012500\n",
      "2022-05-06 23:20:11,051 epoch 97 - iter 74/373 - loss 0.04897384 - samples/sec: 17.92 - lr: 0.012500\n",
      "2022-05-06 23:20:21,182 epoch 97 - iter 111/373 - loss 0.04827129 - samples/sec: 15.39 - lr: 0.012500\n",
      "2022-05-06 23:20:31,008 epoch 97 - iter 148/373 - loss 0.04745414 - samples/sec: 15.91 - lr: 0.012500\n",
      "2022-05-06 23:20:38,699 epoch 97 - iter 185/373 - loss 0.04909476 - samples/sec: 20.61 - lr: 0.012500\n",
      "2022-05-06 23:20:47,589 epoch 97 - iter 222/373 - loss 0.04917387 - samples/sec: 17.66 - lr: 0.012500\n",
      "2022-05-06 23:20:57,024 epoch 97 - iter 259/373 - loss 0.04914858 - samples/sec: 16.64 - lr: 0.012500\n",
      "2022-05-06 23:21:07,471 epoch 97 - iter 296/373 - loss 0.05025742 - samples/sec: 14.91 - lr: 0.012500\n",
      "2022-05-06 23:21:17,170 epoch 97 - iter 333/373 - loss 0.04993899 - samples/sec: 16.11 - lr: 0.012500\n",
      "2022-05-06 23:21:26,008 epoch 97 - iter 370/373 - loss 0.05033001 - samples/sec: 17.78 - lr: 0.012500\n",
      "2022-05-06 23:21:27,382 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:21:27,383 EPOCH 97 done: loss 0.0504 - lr 0.012500\n",
      "2022-05-06 23:21:27,384 BAD EPOCHS (no improvement): 2\n",
      "2022-05-06 23:21:28,814 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:21:38,315 epoch 98 - iter 37/373 - loss 0.05297638 - samples/sec: 15.58 - lr: 0.012500\n",
      "2022-05-06 23:21:47,876 epoch 98 - iter 74/373 - loss 0.04908399 - samples/sec: 16.35 - lr: 0.012500\n",
      "2022-05-06 23:21:57,167 epoch 98 - iter 111/373 - loss 0.04823857 - samples/sec: 16.84 - lr: 0.012500\n",
      "2022-05-06 23:22:06,511 epoch 98 - iter 148/373 - loss 0.04645172 - samples/sec: 16.85 - lr: 0.012500\n",
      "2022-05-06 23:22:16,026 epoch 98 - iter 185/373 - loss 0.04792289 - samples/sec: 16.43 - lr: 0.012500\n",
      "2022-05-06 23:22:25,475 epoch 98 - iter 222/373 - loss 0.05052032 - samples/sec: 16.65 - lr: 0.012500\n",
      "2022-05-06 23:22:33,510 epoch 98 - iter 259/373 - loss 0.05033189 - samples/sec: 19.72 - lr: 0.012500\n",
      "2022-05-06 23:22:41,750 epoch 98 - iter 296/373 - loss 0.04970961 - samples/sec: 19.17 - lr: 0.012500\n",
      "2022-05-06 23:22:50,962 epoch 98 - iter 333/373 - loss 0.04898709 - samples/sec: 17.03 - lr: 0.012500\n",
      "2022-05-06 23:23:00,206 epoch 98 - iter 370/373 - loss 0.04952113 - samples/sec: 16.96 - lr: 0.012500\n",
      "2022-05-06 23:23:01,695 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:23:01,695 EPOCH 98 done: loss 0.0498 - lr 0.012500\n",
      "2022-05-06 23:23:01,696 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 23:23:03,137 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:23:11,554 epoch 99 - iter 37/373 - loss 0.04374215 - samples/sec: 17.59 - lr: 0.012500\n",
      "2022-05-06 23:23:20,820 epoch 99 - iter 74/373 - loss 0.03974684 - samples/sec: 16.91 - lr: 0.012500\n",
      "2022-05-06 23:23:29,948 epoch 99 - iter 111/373 - loss 0.04225408 - samples/sec: 17.17 - lr: 0.012500\n",
      "2022-05-06 23:23:39,030 epoch 99 - iter 148/373 - loss 0.04714433 - samples/sec: 17.26 - lr: 0.012500\n",
      "2022-05-06 23:23:48,301 epoch 99 - iter 185/373 - loss 0.04883787 - samples/sec: 16.90 - lr: 0.012500\n",
      "2022-05-06 23:23:58,981 epoch 99 - iter 222/373 - loss 0.04762949 - samples/sec: 14.57 - lr: 0.012500\n",
      "2022-05-06 23:24:08,406 epoch 99 - iter 259/373 - loss 0.04861989 - samples/sec: 16.61 - lr: 0.012500\n",
      "2022-05-06 23:24:17,414 epoch 99 - iter 296/373 - loss 0.04926011 - samples/sec: 17.44 - lr: 0.012500\n",
      "2022-05-06 23:24:26,604 epoch 99 - iter 333/373 - loss 0.04940672 - samples/sec: 17.09 - lr: 0.012500\n",
      "2022-05-06 23:24:35,900 epoch 99 - iter 370/373 - loss 0.04957268 - samples/sec: 16.86 - lr: 0.012500\n",
      "2022-05-06 23:24:37,277 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:24:37,278 EPOCH 99 done: loss 0.0493 - lr 0.012500\n",
      "2022-05-06 23:24:37,278 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 23:24:38,743 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:24:47,818 epoch 100 - iter 37/373 - loss 0.04144144 - samples/sec: 16.31 - lr: 0.012500\n",
      "2022-05-06 23:24:57,742 epoch 100 - iter 74/373 - loss 0.04501380 - samples/sec: 15.73 - lr: 0.012500\n",
      "2022-05-06 23:25:07,058 epoch 100 - iter 111/373 - loss 0.04706750 - samples/sec: 16.79 - lr: 0.012500\n",
      "2022-05-06 23:25:15,898 epoch 100 - iter 148/373 - loss 0.04629561 - samples/sec: 17.77 - lr: 0.012500\n",
      "2022-05-06 23:25:25,785 epoch 100 - iter 185/373 - loss 0.04651407 - samples/sec: 15.75 - lr: 0.012500\n",
      "2022-05-06 23:25:34,513 epoch 100 - iter 222/373 - loss 0.04792331 - samples/sec: 18.00 - lr: 0.012500\n",
      "2022-05-06 23:25:43,163 epoch 100 - iter 259/373 - loss 0.04964895 - samples/sec: 18.20 - lr: 0.012500\n",
      "2022-05-06 23:25:51,544 epoch 100 - iter 296/373 - loss 0.04989816 - samples/sec: 18.76 - lr: 0.012500\n",
      "2022-05-06 23:25:59,569 epoch 100 - iter 333/373 - loss 0.04901271 - samples/sec: 19.68 - lr: 0.012500\n",
      "2022-05-06 23:26:08,395 epoch 100 - iter 370/373 - loss 0.04931160 - samples/sec: 17.76 - lr: 0.012500\n",
      "2022-05-06 23:26:09,229 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:26:09,230 EPOCH 100 done: loss 0.0492 - lr 0.012500\n",
      "2022-05-06 23:26:09,231 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 23:26:10,637 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:26:18,471 epoch 101 - iter 37/373 - loss 0.05477998 - samples/sec: 18.91 - lr: 0.012500\n",
      "2022-05-06 23:26:25,946 epoch 101 - iter 74/373 - loss 0.05430925 - samples/sec: 21.22 - lr: 0.012500\n",
      "2022-05-06 23:26:34,710 epoch 101 - iter 111/373 - loss 0.05244272 - samples/sec: 17.88 - lr: 0.012500\n",
      "2022-05-06 23:26:43,826 epoch 101 - iter 148/373 - loss 0.05103467 - samples/sec: 17.18 - lr: 0.012500\n",
      "2022-05-06 23:26:53,693 epoch 101 - iter 185/373 - loss 0.05097117 - samples/sec: 15.83 - lr: 0.012500\n",
      "2022-05-06 23:27:03,349 epoch 101 - iter 222/373 - loss 0.04961712 - samples/sec: 16.21 - lr: 0.012500\n",
      "2022-05-06 23:27:13,151 epoch 101 - iter 259/373 - loss 0.04997400 - samples/sec: 15.93 - lr: 0.012500\n",
      "2022-05-06 23:27:23,541 epoch 101 - iter 296/373 - loss 0.05052700 - samples/sec: 14.96 - lr: 0.012500\n",
      "2022-05-06 23:27:33,312 epoch 101 - iter 333/373 - loss 0.04960264 - samples/sec: 16.03 - lr: 0.012500\n",
      "2022-05-06 23:27:41,573 epoch 101 - iter 370/373 - loss 0.04864791 - samples/sec: 19.05 - lr: 0.012500\n",
      "2022-05-06 23:27:42,895 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:27:42,896 EPOCH 101 done: loss 0.0489 - lr 0.012500\n",
      "2022-05-06 23:27:42,897 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 23:27:44,296 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:27:53,383 epoch 102 - iter 37/373 - loss 0.03809884 - samples/sec: 16.29 - lr: 0.012500\n",
      "2022-05-06 23:28:02,358 epoch 102 - iter 74/373 - loss 0.04445007 - samples/sec: 17.45 - lr: 0.012500\n",
      "2022-05-06 23:28:10,774 epoch 102 - iter 111/373 - loss 0.04650934 - samples/sec: 18.72 - lr: 0.012500\n",
      "2022-05-06 23:28:18,674 epoch 102 - iter 148/373 - loss 0.04597958 - samples/sec: 20.00 - lr: 0.012500\n",
      "2022-05-06 23:28:28,795 epoch 102 - iter 185/373 - loss 0.04748732 - samples/sec: 15.42 - lr: 0.012500\n",
      "2022-05-06 23:28:37,852 epoch 102 - iter 222/373 - loss 0.04784680 - samples/sec: 17.34 - lr: 0.012500\n",
      "2022-05-06 23:28:47,294 epoch 102 - iter 259/373 - loss 0.04861806 - samples/sec: 16.53 - lr: 0.012500\n",
      "2022-05-06 23:28:56,813 epoch 102 - iter 296/373 - loss 0.04850764 - samples/sec: 16.40 - lr: 0.012500\n",
      "2022-05-06 23:29:05,266 epoch 102 - iter 333/373 - loss 0.04810544 - samples/sec: 18.60 - lr: 0.012500\n",
      "2022-05-06 23:29:14,139 epoch 102 - iter 370/373 - loss 0.04798534 - samples/sec: 17.69 - lr: 0.012500\n",
      "2022-05-06 23:29:15,581 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:29:15,582 EPOCH 102 done: loss 0.0479 - lr 0.012500\n",
      "2022-05-06 23:29:15,582 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 23:29:16,943 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:29:26,025 epoch 103 - iter 37/373 - loss 0.05277000 - samples/sec: 16.30 - lr: 0.012500\n",
      "2022-05-06 23:29:34,877 epoch 103 - iter 74/373 - loss 0.05278498 - samples/sec: 17.70 - lr: 0.012500\n",
      "2022-05-06 23:29:44,440 epoch 103 - iter 111/373 - loss 0.05103376 - samples/sec: 16.33 - lr: 0.012500\n",
      "2022-05-06 23:29:53,984 epoch 103 - iter 148/373 - loss 0.05059537 - samples/sec: 16.36 - lr: 0.012500\n",
      "2022-05-06 23:30:03,153 epoch 103 - iter 185/373 - loss 0.04919724 - samples/sec: 17.09 - lr: 0.012500\n",
      "2022-05-06 23:30:11,600 epoch 103 - iter 222/373 - loss 0.05056231 - samples/sec: 18.63 - lr: 0.012500\n",
      "2022-05-06 23:30:20,769 epoch 103 - iter 259/373 - loss 0.04958683 - samples/sec: 17.04 - lr: 0.012500\n",
      "2022-05-06 23:30:28,675 epoch 103 - iter 296/373 - loss 0.04906058 - samples/sec: 20.01 - lr: 0.012500\n",
      "2022-05-06 23:30:37,296 epoch 103 - iter 333/373 - loss 0.04781617 - samples/sec: 18.22 - lr: 0.012500\n",
      "2022-05-06 23:30:46,567 epoch 103 - iter 370/373 - loss 0.04791085 - samples/sec: 16.82 - lr: 0.012500\n",
      "2022-05-06 23:30:47,596 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:30:47,597 EPOCH 103 done: loss 0.0481 - lr 0.012500\n",
      "2022-05-06 23:30:47,598 BAD EPOCHS (no improvement): 1\n",
      "2022-05-06 23:30:49,011 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:30:58,544 epoch 104 - iter 37/373 - loss 0.04136903 - samples/sec: 15.53 - lr: 0.012500\n",
      "2022-05-06 23:31:08,133 epoch 104 - iter 74/373 - loss 0.04813467 - samples/sec: 16.26 - lr: 0.012500\n",
      "2022-05-06 23:31:18,133 epoch 104 - iter 111/373 - loss 0.04665508 - samples/sec: 15.61 - lr: 0.012500\n",
      "2022-05-06 23:31:27,467 epoch 104 - iter 148/373 - loss 0.04797256 - samples/sec: 16.77 - lr: 0.012500\n",
      "2022-05-06 23:31:36,019 epoch 104 - iter 185/373 - loss 0.04719272 - samples/sec: 18.40 - lr: 0.012500\n",
      "2022-05-06 23:31:44,883 epoch 104 - iter 222/373 - loss 0.04638285 - samples/sec: 17.72 - lr: 0.012500\n",
      "2022-05-06 23:31:53,442 epoch 104 - iter 259/373 - loss 0.04767904 - samples/sec: 18.38 - lr: 0.012500\n",
      "2022-05-06 23:32:02,476 epoch 104 - iter 296/373 - loss 0.04832936 - samples/sec: 17.36 - lr: 0.012500\n",
      "2022-05-06 23:32:11,720 epoch 104 - iter 333/373 - loss 0.04757383 - samples/sec: 16.97 - lr: 0.012500\n",
      "2022-05-06 23:32:20,237 epoch 104 - iter 370/373 - loss 0.04729020 - samples/sec: 18.42 - lr: 0.012500\n",
      "2022-05-06 23:32:21,392 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:32:21,393 EPOCH 104 done: loss 0.0471 - lr 0.012500\n",
      "2022-05-06 23:32:21,393 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 23:32:22,746 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:32:30,950 epoch 105 - iter 37/373 - loss 0.04460646 - samples/sec: 18.05 - lr: 0.012500\n",
      "2022-05-06 23:32:40,042 epoch 105 - iter 74/373 - loss 0.04715608 - samples/sec: 17.21 - lr: 0.012500\n",
      "2022-05-06 23:32:48,815 epoch 105 - iter 111/373 - loss 0.04448564 - samples/sec: 17.89 - lr: 0.012500\n",
      "2022-05-06 23:32:58,195 epoch 105 - iter 148/373 - loss 0.04588799 - samples/sec: 16.69 - lr: 0.012500\n",
      "2022-05-06 23:33:07,512 epoch 105 - iter 185/373 - loss 0.04509224 - samples/sec: 16.79 - lr: 0.012500\n",
      "2022-05-06 23:33:15,888 epoch 105 - iter 222/373 - loss 0.04539833 - samples/sec: 18.84 - lr: 0.012500\n",
      "2022-05-06 23:33:25,055 epoch 105 - iter 259/373 - loss 0.04671633 - samples/sec: 17.06 - lr: 0.012500\n",
      "2022-05-06 23:33:33,477 epoch 105 - iter 296/373 - loss 0.04640993 - samples/sec: 18.74 - lr: 0.012500\n",
      "2022-05-06 23:33:42,345 epoch 105 - iter 333/373 - loss 0.04633519 - samples/sec: 17.68 - lr: 0.012500\n",
      "2022-05-06 23:33:51,081 epoch 105 - iter 370/373 - loss 0.04602539 - samples/sec: 17.94 - lr: 0.012500\n",
      "2022-05-06 23:33:52,190 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:33:52,190 EPOCH 105 done: loss 0.0461 - lr 0.012500\n",
      "2022-05-06 23:33:52,191 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 23:33:53,571 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:34:01,545 epoch 106 - iter 37/373 - loss 0.05780615 - samples/sec: 18.57 - lr: 0.012500\n",
      "2022-05-06 23:34:09,814 epoch 106 - iter 74/373 - loss 0.05278434 - samples/sec: 19.06 - lr: 0.012500\n",
      "2022-05-06 23:34:18,260 epoch 106 - iter 111/373 - loss 0.04924118 - samples/sec: 18.61 - lr: 0.012500\n",
      "2022-05-06 23:34:26,766 epoch 106 - iter 148/373 - loss 0.05064080 - samples/sec: 18.48 - lr: 0.012500\n",
      "2022-05-06 23:34:35,594 epoch 106 - iter 185/373 - loss 0.04941807 - samples/sec: 17.78 - lr: 0.012500\n",
      "2022-05-06 23:34:45,203 epoch 106 - iter 222/373 - loss 0.04828775 - samples/sec: 16.26 - lr: 0.012500\n",
      "2022-05-06 23:34:53,489 epoch 106 - iter 259/373 - loss 0.04822404 - samples/sec: 19.00 - lr: 0.012500\n",
      "2022-05-06 23:35:02,155 epoch 106 - iter 296/373 - loss 0.04690128 - samples/sec: 18.13 - lr: 0.012500\n",
      "2022-05-06 23:35:11,407 epoch 106 - iter 333/373 - loss 0.04522542 - samples/sec: 16.89 - lr: 0.012500\n",
      "2022-05-06 23:35:21,881 epoch 106 - iter 370/373 - loss 0.04652105 - samples/sec: 14.81 - lr: 0.012500\n",
      "2022-05-06 23:35:23,207 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:35:23,207 EPOCH 106 done: loss 0.0464 - lr 0.012500\n",
      "2022-05-06 23:35:23,208 BAD EPOCHS (no improvement): 1\n",
      "2022-05-06 23:35:24,647 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:35:33,961 epoch 107 - iter 37/373 - loss 0.03613908 - samples/sec: 15.90 - lr: 0.012500\n",
      "2022-05-06 23:35:42,584 epoch 107 - iter 74/373 - loss 0.04077254 - samples/sec: 18.25 - lr: 0.012500\n",
      "2022-05-06 23:35:51,891 epoch 107 - iter 111/373 - loss 0.04238687 - samples/sec: 16.78 - lr: 0.012500\n",
      "2022-05-06 23:36:01,150 epoch 107 - iter 148/373 - loss 0.04466561 - samples/sec: 16.89 - lr: 0.012500\n",
      "2022-05-06 23:36:10,751 epoch 107 - iter 185/373 - loss 0.04510634 - samples/sec: 16.27 - lr: 0.012500\n",
      "2022-05-06 23:36:19,579 epoch 107 - iter 222/373 - loss 0.04512842 - samples/sec: 17.75 - lr: 0.012500\n",
      "2022-05-06 23:36:29,194 epoch 107 - iter 259/373 - loss 0.04466423 - samples/sec: 16.27 - lr: 0.012500\n",
      "2022-05-06 23:36:36,674 epoch 107 - iter 296/373 - loss 0.04512625 - samples/sec: 21.18 - lr: 0.012500\n",
      "2022-05-06 23:36:44,687 epoch 107 - iter 333/373 - loss 0.04446091 - samples/sec: 19.68 - lr: 0.012500\n",
      "2022-05-06 23:36:53,072 epoch 107 - iter 370/373 - loss 0.04358010 - samples/sec: 18.72 - lr: 0.012500\n",
      "2022-05-06 23:36:54,356 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:36:54,357 EPOCH 107 done: loss 0.0438 - lr 0.012500\n",
      "2022-05-06 23:36:54,357 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 23:36:55,739 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:37:04,636 epoch 108 - iter 37/373 - loss 0.04536663 - samples/sec: 16.64 - lr: 0.012500\n",
      "2022-05-06 23:37:14,516 epoch 108 - iter 74/373 - loss 0.04472798 - samples/sec: 15.79 - lr: 0.012500\n",
      "2022-05-06 23:37:23,694 epoch 108 - iter 111/373 - loss 0.04532915 - samples/sec: 17.03 - lr: 0.012500\n",
      "2022-05-06 23:37:32,556 epoch 108 - iter 148/373 - loss 0.04573747 - samples/sec: 17.69 - lr: 0.012500\n",
      "2022-05-06 23:37:41,438 epoch 108 - iter 185/373 - loss 0.04648235 - samples/sec: 17.67 - lr: 0.012500\n",
      "2022-05-06 23:37:51,210 epoch 108 - iter 222/373 - loss 0.04436795 - samples/sec: 15.99 - lr: 0.012500\n",
      "2022-05-06 23:37:59,553 epoch 108 - iter 259/373 - loss 0.04374856 - samples/sec: 18.86 - lr: 0.012500\n",
      "2022-05-06 23:38:08,518 epoch 108 - iter 296/373 - loss 0.04455109 - samples/sec: 17.50 - lr: 0.012500\n",
      "2022-05-06 23:38:17,406 epoch 108 - iter 333/373 - loss 0.04488308 - samples/sec: 17.61 - lr: 0.012500\n",
      "2022-05-06 23:38:26,173 epoch 108 - iter 370/373 - loss 0.04471855 - samples/sec: 17.85 - lr: 0.012500\n",
      "2022-05-06 23:38:27,089 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:38:27,090 EPOCH 108 done: loss 0.0447 - lr 0.012500\n",
      "2022-05-06 23:38:27,091 BAD EPOCHS (no improvement): 1\n",
      "2022-05-06 23:38:28,509 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:38:37,343 epoch 109 - iter 37/373 - loss 0.04956387 - samples/sec: 16.76 - lr: 0.012500\n",
      "2022-05-06 23:38:45,320 epoch 109 - iter 74/373 - loss 0.04741958 - samples/sec: 19.79 - lr: 0.012500\n",
      "2022-05-06 23:38:54,391 epoch 109 - iter 111/373 - loss 0.04533016 - samples/sec: 17.26 - lr: 0.012500\n",
      "2022-05-06 23:39:03,005 epoch 109 - iter 148/373 - loss 0.04430700 - samples/sec: 18.25 - lr: 0.012500\n",
      "2022-05-06 23:39:12,363 epoch 109 - iter 185/373 - loss 0.04265753 - samples/sec: 16.71 - lr: 0.012500\n",
      "2022-05-06 23:39:21,460 epoch 109 - iter 222/373 - loss 0.04253126 - samples/sec: 17.24 - lr: 0.012500\n",
      "2022-05-06 23:39:30,731 epoch 109 - iter 259/373 - loss 0.04350106 - samples/sec: 16.87 - lr: 0.012500\n",
      "2022-05-06 23:39:40,009 epoch 109 - iter 296/373 - loss 0.04474468 - samples/sec: 16.84 - lr: 0.012500\n",
      "2022-05-06 23:39:48,644 epoch 109 - iter 333/373 - loss 0.04425923 - samples/sec: 18.15 - lr: 0.012500\n",
      "2022-05-06 23:39:57,319 epoch 109 - iter 370/373 - loss 0.04436112 - samples/sec: 18.07 - lr: 0.012500\n",
      "2022-05-06 23:39:58,540 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:39:58,541 EPOCH 109 done: loss 0.0443 - lr 0.012500\n",
      "2022-05-06 23:39:58,541 BAD EPOCHS (no improvement): 2\n",
      "2022-05-06 23:39:59,945 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:40:07,689 epoch 110 - iter 37/373 - loss 0.03398959 - samples/sec: 19.12 - lr: 0.012500\n",
      "2022-05-06 23:40:16,355 epoch 110 - iter 74/373 - loss 0.04114094 - samples/sec: 18.14 - lr: 0.012500\n",
      "2022-05-06 23:40:25,453 epoch 110 - iter 111/373 - loss 0.04142816 - samples/sec: 17.21 - lr: 0.012500\n",
      "2022-05-06 23:40:33,985 epoch 110 - iter 148/373 - loss 0.04065703 - samples/sec: 18.43 - lr: 0.012500\n",
      "2022-05-06 23:40:42,606 epoch 110 - iter 185/373 - loss 0.04332039 - samples/sec: 18.22 - lr: 0.012500\n",
      "2022-05-06 23:40:52,386 epoch 110 - iter 222/373 - loss 0.04222743 - samples/sec: 16.01 - lr: 0.012500\n",
      "2022-05-06 23:41:01,149 epoch 110 - iter 259/373 - loss 0.04270353 - samples/sec: 17.91 - lr: 0.012500\n",
      "2022-05-06 23:41:10,179 epoch 110 - iter 296/373 - loss 0.04325135 - samples/sec: 17.32 - lr: 0.012500\n",
      "2022-05-06 23:41:18,812 epoch 110 - iter 333/373 - loss 0.04298627 - samples/sec: 18.19 - lr: 0.012500\n",
      "2022-05-06 23:41:28,618 epoch 110 - iter 370/373 - loss 0.04318350 - samples/sec: 15.92 - lr: 0.012500\n",
      "2022-05-06 23:41:29,644 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:41:29,645 EPOCH 110 done: loss 0.0432 - lr 0.012500\n",
      "2022-05-06 23:41:29,645 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 23:41:31,081 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:41:39,110 epoch 111 - iter 37/373 - loss 0.03162675 - samples/sec: 18.44 - lr: 0.012500\n",
      "2022-05-06 23:41:48,460 epoch 111 - iter 74/373 - loss 0.03800759 - samples/sec: 16.74 - lr: 0.012500\n",
      "2022-05-06 23:41:56,873 epoch 111 - iter 111/373 - loss 0.03961453 - samples/sec: 18.72 - lr: 0.012500\n",
      "2022-05-06 23:42:05,439 epoch 111 - iter 148/373 - loss 0.03850717 - samples/sec: 18.34 - lr: 0.012500\n",
      "2022-05-06 23:42:14,297 epoch 111 - iter 185/373 - loss 0.04099234 - samples/sec: 17.80 - lr: 0.012500\n",
      "2022-05-06 23:42:22,941 epoch 111 - iter 222/373 - loss 0.04287593 - samples/sec: 18.20 - lr: 0.012500\n",
      "2022-05-06 23:42:32,584 epoch 111 - iter 259/373 - loss 0.04296868 - samples/sec: 16.19 - lr: 0.012500\n",
      "2022-05-06 23:42:42,327 epoch 111 - iter 296/373 - loss 0.04292823 - samples/sec: 15.97 - lr: 0.012500\n",
      "2022-05-06 23:42:51,314 epoch 111 - iter 333/373 - loss 0.04312599 - samples/sec: 17.44 - lr: 0.012500\n",
      "2022-05-06 23:43:00,611 epoch 111 - iter 370/373 - loss 0.04279476 - samples/sec: 16.86 - lr: 0.012500\n",
      "2022-05-06 23:43:02,032 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:43:02,033 EPOCH 111 done: loss 0.0427 - lr 0.012500\n",
      "2022-05-06 23:43:02,034 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 23:43:03,456 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:43:11,271 epoch 112 - iter 37/373 - loss 0.04402386 - samples/sec: 18.95 - lr: 0.012500\n",
      "2022-05-06 23:43:21,153 epoch 112 - iter 74/373 - loss 0.04344975 - samples/sec: 15.78 - lr: 0.012500\n",
      "2022-05-06 23:43:30,335 epoch 112 - iter 111/373 - loss 0.04208720 - samples/sec: 17.12 - lr: 0.012500\n",
      "2022-05-06 23:43:39,166 epoch 112 - iter 148/373 - loss 0.04135564 - samples/sec: 17.82 - lr: 0.012500\n",
      "2022-05-06 23:43:47,750 epoch 112 - iter 185/373 - loss 0.04178493 - samples/sec: 18.36 - lr: 0.012500\n",
      "2022-05-06 23:43:56,812 epoch 112 - iter 222/373 - loss 0.04146102 - samples/sec: 17.31 - lr: 0.012500\n",
      "2022-05-06 23:44:06,138 epoch 112 - iter 259/373 - loss 0.04187780 - samples/sec: 16.76 - lr: 0.012500\n",
      "2022-05-06 23:44:15,027 epoch 112 - iter 296/373 - loss 0.04118342 - samples/sec: 17.61 - lr: 0.012500\n",
      "2022-05-06 23:44:24,447 epoch 112 - iter 333/373 - loss 0.04099285 - samples/sec: 16.56 - lr: 0.012500\n",
      "2022-05-06 23:44:34,009 epoch 112 - iter 370/373 - loss 0.04090997 - samples/sec: 16.35 - lr: 0.012500\n",
      "2022-05-06 23:44:35,181 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:44:35,182 EPOCH 112 done: loss 0.0410 - lr 0.012500\n",
      "2022-05-06 23:44:35,182 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 23:44:36,549 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:44:45,352 epoch 113 - iter 37/373 - loss 0.03179417 - samples/sec: 16.82 - lr: 0.012500\n",
      "2022-05-06 23:44:55,786 epoch 113 - iter 74/373 - loss 0.03607842 - samples/sec: 14.91 - lr: 0.012500\n",
      "2022-05-06 23:45:04,996 epoch 113 - iter 111/373 - loss 0.03854457 - samples/sec: 16.97 - lr: 0.012500\n",
      "2022-05-06 23:45:14,344 epoch 113 - iter 148/373 - loss 0.04065694 - samples/sec: 16.73 - lr: 0.012500\n",
      "2022-05-06 23:45:23,044 epoch 113 - iter 185/373 - loss 0.03970288 - samples/sec: 18.02 - lr: 0.012500\n",
      "2022-05-06 23:45:32,110 epoch 113 - iter 222/373 - loss 0.03930865 - samples/sec: 17.25 - lr: 0.012500\n",
      "2022-05-06 23:45:41,012 epoch 113 - iter 259/373 - loss 0.04024401 - samples/sec: 17.60 - lr: 0.012500\n",
      "2022-05-06 23:45:50,258 epoch 113 - iter 296/373 - loss 0.03970708 - samples/sec: 16.92 - lr: 0.012500\n",
      "2022-05-06 23:45:58,808 epoch 113 - iter 333/373 - loss 0.03951442 - samples/sec: 18.40 - lr: 0.012500\n",
      "2022-05-06 23:46:06,907 epoch 113 - iter 370/373 - loss 0.04026660 - samples/sec: 19.46 - lr: 0.012500\n",
      "2022-05-06 23:46:08,075 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:46:08,075 EPOCH 113 done: loss 0.0406 - lr 0.012500\n",
      "2022-05-06 23:46:08,076 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 23:46:09,491 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:46:18,367 epoch 114 - iter 37/373 - loss 0.03795863 - samples/sec: 16.68 - lr: 0.012500\n",
      "2022-05-06 23:46:27,001 epoch 114 - iter 74/373 - loss 0.04183245 - samples/sec: 18.22 - lr: 0.012500\n",
      "2022-05-06 23:46:35,570 epoch 114 - iter 111/373 - loss 0.04012132 - samples/sec: 18.34 - lr: 0.012500\n",
      "2022-05-06 23:46:45,244 epoch 114 - iter 148/373 - loss 0.04145395 - samples/sec: 16.17 - lr: 0.012500\n",
      "2022-05-06 23:46:53,924 epoch 114 - iter 185/373 - loss 0.04052981 - samples/sec: 18.08 - lr: 0.012500\n",
      "2022-05-06 23:47:03,503 epoch 114 - iter 222/373 - loss 0.04091255 - samples/sec: 16.28 - lr: 0.012500\n",
      "2022-05-06 23:47:12,372 epoch 114 - iter 259/373 - loss 0.04110473 - samples/sec: 17.68 - lr: 0.012500\n",
      "2022-05-06 23:47:20,782 epoch 114 - iter 296/373 - loss 0.04034015 - samples/sec: 18.74 - lr: 0.012500\n",
      "2022-05-06 23:47:30,080 epoch 114 - iter 333/373 - loss 0.03998992 - samples/sec: 16.85 - lr: 0.012500\n",
      "2022-05-06 23:47:39,279 epoch 114 - iter 370/373 - loss 0.04049759 - samples/sec: 17.06 - lr: 0.012500\n",
      "2022-05-06 23:47:40,760 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:47:40,760 EPOCH 114 done: loss 0.0404 - lr 0.012500\n",
      "2022-05-06 23:47:40,761 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 23:47:42,214 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:47:52,252 epoch 115 - iter 37/373 - loss 0.04036140 - samples/sec: 14.75 - lr: 0.012500\n",
      "2022-05-06 23:48:00,850 epoch 115 - iter 74/373 - loss 0.03788559 - samples/sec: 18.31 - lr: 0.012500\n",
      "2022-05-06 23:48:09,789 epoch 115 - iter 111/373 - loss 0.03625541 - samples/sec: 17.51 - lr: 0.012500\n",
      "2022-05-06 23:48:19,112 epoch 115 - iter 148/373 - loss 0.03948739 - samples/sec: 16.76 - lr: 0.012500\n",
      "2022-05-06 23:48:27,965 epoch 115 - iter 185/373 - loss 0.03883046 - samples/sec: 17.78 - lr: 0.012500\n",
      "2022-05-06 23:48:36,606 epoch 115 - iter 222/373 - loss 0.03878499 - samples/sec: 18.16 - lr: 0.012500\n",
      "2022-05-06 23:48:44,585 epoch 115 - iter 259/373 - loss 0.03906367 - samples/sec: 19.81 - lr: 0.012500\n",
      "2022-05-06 23:48:54,974 epoch 115 - iter 296/373 - loss 0.03993120 - samples/sec: 14.98 - lr: 0.012500\n",
      "2022-05-06 23:49:04,284 epoch 115 - iter 333/373 - loss 0.03999112 - samples/sec: 16.84 - lr: 0.012500\n",
      "2022-05-06 23:49:13,300 epoch 115 - iter 370/373 - loss 0.03939714 - samples/sec: 17.39 - lr: 0.012500\n",
      "2022-05-06 23:49:14,402 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:49:14,402 EPOCH 115 done: loss 0.0392 - lr 0.012500\n",
      "2022-05-06 23:49:14,403 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 23:49:15,839 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:49:24,688 epoch 116 - iter 37/373 - loss 0.04327795 - samples/sec: 16.73 - lr: 0.012500\n",
      "2022-05-06 23:49:33,714 epoch 116 - iter 74/373 - loss 0.03790935 - samples/sec: 17.40 - lr: 0.012500\n",
      "2022-05-06 23:49:43,199 epoch 116 - iter 111/373 - loss 0.03994279 - samples/sec: 16.52 - lr: 0.012500\n",
      "2022-05-06 23:49:51,989 epoch 116 - iter 148/373 - loss 0.04048200 - samples/sec: 17.81 - lr: 0.012500\n",
      "2022-05-06 23:50:00,890 epoch 116 - iter 185/373 - loss 0.03928623 - samples/sec: 17.58 - lr: 0.012500\n",
      "2022-05-06 23:50:09,219 epoch 116 - iter 222/373 - loss 0.03934929 - samples/sec: 18.87 - lr: 0.012500\n",
      "2022-05-06 23:50:18,332 epoch 116 - iter 259/373 - loss 0.03881332 - samples/sec: 17.23 - lr: 0.012500\n",
      "2022-05-06 23:50:26,323 epoch 116 - iter 296/373 - loss 0.03785127 - samples/sec: 19.71 - lr: 0.012500\n",
      "2022-05-06 23:50:36,058 epoch 116 - iter 333/373 - loss 0.03751921 - samples/sec: 16.03 - lr: 0.012500\n",
      "2022-05-06 23:50:45,072 epoch 116 - iter 370/373 - loss 0.03720094 - samples/sec: 17.39 - lr: 0.012500\n",
      "2022-05-06 23:50:46,303 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:50:46,304 EPOCH 116 done: loss 0.0371 - lr 0.012500\n",
      "2022-05-06 23:50:46,304 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 23:50:47,744 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:50:55,442 epoch 117 - iter 37/373 - loss 0.03574241 - samples/sec: 19.23 - lr: 0.012500\n",
      "2022-05-06 23:51:05,440 epoch 117 - iter 74/373 - loss 0.04144468 - samples/sec: 15.58 - lr: 0.012500\n",
      "2022-05-06 23:51:14,586 epoch 117 - iter 111/373 - loss 0.04150078 - samples/sec: 17.12 - lr: 0.012500\n",
      "2022-05-06 23:51:24,254 epoch 117 - iter 148/373 - loss 0.03962954 - samples/sec: 16.12 - lr: 0.012500\n",
      "2022-05-06 23:51:32,617 epoch 117 - iter 185/373 - loss 0.03797642 - samples/sec: 18.77 - lr: 0.012500\n",
      "2022-05-06 23:51:41,964 epoch 117 - iter 222/373 - loss 0.03755776 - samples/sec: 16.69 - lr: 0.012500\n",
      "2022-05-06 23:51:49,579 epoch 117 - iter 259/373 - loss 0.03799351 - samples/sec: 20.83 - lr: 0.012500\n",
      "2022-05-06 23:51:58,002 epoch 117 - iter 296/373 - loss 0.03833740 - samples/sec: 18.71 - lr: 0.012500\n",
      "2022-05-06 23:52:07,587 epoch 117 - iter 333/373 - loss 0.03794268 - samples/sec: 16.29 - lr: 0.012500\n",
      "2022-05-06 23:52:16,319 epoch 117 - iter 370/373 - loss 0.03721802 - samples/sec: 17.98 - lr: 0.012500\n",
      "2022-05-06 23:52:17,476 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:52:17,477 EPOCH 117 done: loss 0.0372 - lr 0.012500\n",
      "2022-05-06 23:52:17,477 BAD EPOCHS (no improvement): 1\n",
      "2022-05-06 23:52:18,845 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:52:27,800 epoch 118 - iter 37/373 - loss 0.02659851 - samples/sec: 16.53 - lr: 0.012500\n",
      "2022-05-06 23:52:35,830 epoch 118 - iter 74/373 - loss 0.03291129 - samples/sec: 19.62 - lr: 0.012500\n",
      "2022-05-06 23:52:44,393 epoch 118 - iter 111/373 - loss 0.03431771 - samples/sec: 18.35 - lr: 0.012500\n",
      "2022-05-06 23:52:53,075 epoch 118 - iter 148/373 - loss 0.03502930 - samples/sec: 18.08 - lr: 0.012500\n",
      "2022-05-06 23:53:01,360 epoch 118 - iter 185/373 - loss 0.03559280 - samples/sec: 18.96 - lr: 0.012500\n",
      "2022-05-06 23:53:10,308 epoch 118 - iter 222/373 - loss 0.03651371 - samples/sec: 17.47 - lr: 0.012500\n",
      "2022-05-06 23:53:19,250 epoch 118 - iter 259/373 - loss 0.03638277 - samples/sec: 17.56 - lr: 0.012500\n",
      "2022-05-06 23:53:28,125 epoch 118 - iter 296/373 - loss 0.03614232 - samples/sec: 17.66 - lr: 0.012500\n",
      "2022-05-06 23:53:37,669 epoch 118 - iter 333/373 - loss 0.03610331 - samples/sec: 16.35 - lr: 0.012500\n",
      "2022-05-06 23:53:46,729 epoch 118 - iter 370/373 - loss 0.03608416 - samples/sec: 17.29 - lr: 0.012500\n",
      "2022-05-06 23:53:48,006 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:53:48,006 EPOCH 118 done: loss 0.0359 - lr 0.012500\n",
      "2022-05-06 23:53:48,007 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 23:53:49,397 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:53:56,586 epoch 119 - iter 37/373 - loss 0.03238031 - samples/sec: 20.60 - lr: 0.012500\n",
      "2022-05-06 23:54:05,918 epoch 119 - iter 74/373 - loss 0.03465545 - samples/sec: 16.76 - lr: 0.012500\n",
      "2022-05-06 23:54:15,017 epoch 119 - iter 111/373 - loss 0.03543246 - samples/sec: 17.24 - lr: 0.012500\n",
      "2022-05-06 23:54:25,161 epoch 119 - iter 148/373 - loss 0.03456073 - samples/sec: 15.32 - lr: 0.012500\n",
      "2022-05-06 23:54:34,759 epoch 119 - iter 185/373 - loss 0.03531482 - samples/sec: 16.23 - lr: 0.012500\n",
      "2022-05-06 23:54:43,416 epoch 119 - iter 222/373 - loss 0.03547633 - samples/sec: 18.12 - lr: 0.012500\n",
      "2022-05-06 23:54:52,509 epoch 119 - iter 259/373 - loss 0.03661360 - samples/sec: 17.24 - lr: 0.012500\n",
      "2022-05-06 23:55:02,052 epoch 119 - iter 296/373 - loss 0.03629006 - samples/sec: 16.42 - lr: 0.012500\n",
      "2022-05-06 23:55:10,431 epoch 119 - iter 333/373 - loss 0.03610403 - samples/sec: 18.79 - lr: 0.012500\n",
      "2022-05-06 23:55:19,516 epoch 119 - iter 370/373 - loss 0.03548620 - samples/sec: 17.36 - lr: 0.012500\n",
      "2022-05-06 23:55:20,471 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:55:20,472 EPOCH 119 done: loss 0.0355 - lr 0.012500\n",
      "2022-05-06 23:55:20,472 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 23:55:21,870 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:55:30,075 epoch 120 - iter 37/373 - loss 0.03355775 - samples/sec: 18.05 - lr: 0.012500\n",
      "2022-05-06 23:55:39,101 epoch 120 - iter 74/373 - loss 0.03703690 - samples/sec: 17.34 - lr: 0.012500\n",
      "2022-05-06 23:55:48,643 epoch 120 - iter 111/373 - loss 0.03505052 - samples/sec: 16.34 - lr: 0.012500\n",
      "2022-05-06 23:55:57,600 epoch 120 - iter 148/373 - loss 0.03309299 - samples/sec: 17.50 - lr: 0.012500\n",
      "2022-05-06 23:56:05,581 epoch 120 - iter 185/373 - loss 0.03241882 - samples/sec: 19.75 - lr: 0.012500\n",
      "2022-05-06 23:56:15,969 epoch 120 - iter 222/373 - loss 0.03177576 - samples/sec: 14.98 - lr: 0.012500\n",
      "2022-05-06 23:56:25,939 epoch 120 - iter 259/373 - loss 0.03266223 - samples/sec: 15.62 - lr: 0.012500\n",
      "2022-05-06 23:56:34,453 epoch 120 - iter 296/373 - loss 0.03344402 - samples/sec: 18.61 - lr: 0.012500\n",
      "2022-05-06 23:56:43,616 epoch 120 - iter 333/373 - loss 0.03434215 - samples/sec: 17.11 - lr: 0.012500\n",
      "2022-05-06 23:56:52,376 epoch 120 - iter 370/373 - loss 0.03436243 - samples/sec: 17.94 - lr: 0.012500\n",
      "2022-05-06 23:56:53,714 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:56:53,716 EPOCH 120 done: loss 0.0343 - lr 0.012500\n",
      "2022-05-06 23:56:53,716 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 23:56:55,166 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:57:03,657 epoch 121 - iter 37/373 - loss 0.04432746 - samples/sec: 17.44 - lr: 0.012500\n",
      "2022-05-06 23:57:12,888 epoch 121 - iter 74/373 - loss 0.03819275 - samples/sec: 17.02 - lr: 0.012500\n",
      "2022-05-06 23:57:21,754 epoch 121 - iter 111/373 - loss 0.03746600 - samples/sec: 17.67 - lr: 0.012500\n",
      "2022-05-06 23:57:30,696 epoch 121 - iter 148/373 - loss 0.03515412 - samples/sec: 17.47 - lr: 0.012500\n",
      "2022-05-06 23:57:39,456 epoch 121 - iter 185/373 - loss 0.03510281 - samples/sec: 17.88 - lr: 0.012500\n",
      "2022-05-06 23:57:48,112 epoch 121 - iter 222/373 - loss 0.03401134 - samples/sec: 18.18 - lr: 0.012500\n",
      "2022-05-06 23:57:57,254 epoch 121 - iter 259/373 - loss 0.03372700 - samples/sec: 17.11 - lr: 0.012500\n",
      "2022-05-06 23:58:06,353 epoch 121 - iter 296/373 - loss 0.03397970 - samples/sec: 17.19 - lr: 0.012500\n",
      "2022-05-06 23:58:15,715 epoch 121 - iter 333/373 - loss 0.03382997 - samples/sec: 16.71 - lr: 0.012500\n",
      "2022-05-06 23:58:24,878 epoch 121 - iter 370/373 - loss 0.03310506 - samples/sec: 17.11 - lr: 0.012500\n",
      "2022-05-06 23:58:25,828 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:58:25,829 EPOCH 121 done: loss 0.0332 - lr 0.012500\n",
      "2022-05-06 23:58:25,830 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 23:58:27,225 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:58:34,890 epoch 122 - iter 37/373 - loss 0.03694891 - samples/sec: 19.32 - lr: 0.012500\n",
      "2022-05-06 23:58:43,290 epoch 122 - iter 74/373 - loss 0.03217472 - samples/sec: 18.70 - lr: 0.012500\n",
      "2022-05-06 23:58:52,603 epoch 122 - iter 111/373 - loss 0.03116158 - samples/sec: 16.79 - lr: 0.012500\n",
      "2022-05-06 23:59:01,336 epoch 122 - iter 148/373 - loss 0.03257110 - samples/sec: 17.94 - lr: 0.012500\n",
      "2022-05-06 23:59:09,655 epoch 122 - iter 185/373 - loss 0.03255579 - samples/sec: 18.87 - lr: 0.012500\n",
      "2022-05-06 23:59:18,573 epoch 122 - iter 222/373 - loss 0.03291994 - samples/sec: 17.59 - lr: 0.012500\n",
      "2022-05-06 23:59:28,339 epoch 122 - iter 259/373 - loss 0.03324438 - samples/sec: 15.95 - lr: 0.012500\n",
      "2022-05-06 23:59:37,344 epoch 122 - iter 296/373 - loss 0.03275571 - samples/sec: 17.37 - lr: 0.012500\n",
      "2022-05-06 23:59:45,536 epoch 122 - iter 333/373 - loss 0.03177828 - samples/sec: 19.25 - lr: 0.012500\n",
      "2022-05-06 23:59:55,231 epoch 122 - iter 370/373 - loss 0.03216721 - samples/sec: 16.08 - lr: 0.012500\n",
      "2022-05-06 23:59:56,455 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-06 23:59:56,456 EPOCH 122 done: loss 0.0321 - lr 0.012500\n",
      "2022-05-06 23:59:56,456 BAD EPOCHS (no improvement): 0\n",
      "2022-05-06 23:59:57,875 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:00:05,913 epoch 123 - iter 37/373 - loss 0.02440369 - samples/sec: 18.42 - lr: 0.012500\n",
      "2022-05-07 00:00:14,260 epoch 123 - iter 74/373 - loss 0.02756345 - samples/sec: 18.81 - lr: 0.012500\n",
      "2022-05-07 00:00:23,624 epoch 123 - iter 111/373 - loss 0.03045933 - samples/sec: 16.66 - lr: 0.012500\n",
      "2022-05-07 00:00:31,370 epoch 123 - iter 148/373 - loss 0.03038485 - samples/sec: 20.39 - lr: 0.012500\n",
      "2022-05-07 00:00:40,574 epoch 123 - iter 185/373 - loss 0.02942155 - samples/sec: 16.97 - lr: 0.012500\n",
      "2022-05-07 00:00:49,190 epoch 123 - iter 222/373 - loss 0.03128541 - samples/sec: 18.20 - lr: 0.012500\n",
      "2022-05-07 00:00:58,299 epoch 123 - iter 259/373 - loss 0.03121282 - samples/sec: 17.20 - lr: 0.012500\n",
      "2022-05-07 00:01:06,829 epoch 123 - iter 296/373 - loss 0.03079725 - samples/sec: 18.42 - lr: 0.012500\n",
      "2022-05-07 00:01:15,203 epoch 123 - iter 333/373 - loss 0.03050038 - samples/sec: 18.79 - lr: 0.012500\n",
      "2022-05-07 00:01:25,147 epoch 123 - iter 370/373 - loss 0.03105703 - samples/sec: 15.65 - lr: 0.012500\n",
      "2022-05-07 00:01:26,533 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:01:26,534 EPOCH 123 done: loss 0.0308 - lr 0.012500\n",
      "2022-05-07 00:01:26,535 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 00:01:27,911 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:01:35,594 epoch 124 - iter 37/373 - loss 0.02946644 - samples/sec: 19.27 - lr: 0.012500\n",
      "2022-05-07 00:01:44,080 epoch 124 - iter 74/373 - loss 0.02924253 - samples/sec: 18.48 - lr: 0.012500\n",
      "2022-05-07 00:01:52,690 epoch 124 - iter 111/373 - loss 0.03030992 - samples/sec: 18.22 - lr: 0.012500\n",
      "2022-05-07 00:02:01,331 epoch 124 - iter 148/373 - loss 0.02934238 - samples/sec: 18.14 - lr: 0.012500\n",
      "2022-05-07 00:02:10,219 epoch 124 - iter 185/373 - loss 0.02925932 - samples/sec: 17.64 - lr: 0.012500\n",
      "2022-05-07 00:02:19,805 epoch 124 - iter 222/373 - loss 0.03120712 - samples/sec: 16.28 - lr: 0.012500\n",
      "2022-05-07 00:02:28,689 epoch 124 - iter 259/373 - loss 0.03145727 - samples/sec: 17.67 - lr: 0.012500\n",
      "2022-05-07 00:02:38,223 epoch 124 - iter 296/373 - loss 0.03099313 - samples/sec: 16.37 - lr: 0.012500\n",
      "2022-05-07 00:02:46,731 epoch 124 - iter 333/373 - loss 0.03064887 - samples/sec: 18.44 - lr: 0.012500\n",
      "2022-05-07 00:02:56,563 epoch 124 - iter 370/373 - loss 0.03033794 - samples/sec: 15.86 - lr: 0.012500\n",
      "2022-05-07 00:02:57,800 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:02:57,801 EPOCH 124 done: loss 0.0303 - lr 0.012500\n",
      "2022-05-07 00:02:57,801 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 00:02:59,196 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:03:08,266 epoch 125 - iter 37/373 - loss 0.02957433 - samples/sec: 16.32 - lr: 0.012500\n",
      "2022-05-07 00:03:16,841 epoch 125 - iter 74/373 - loss 0.03036338 - samples/sec: 18.31 - lr: 0.012500\n",
      "2022-05-07 00:03:26,237 epoch 125 - iter 111/373 - loss 0.02902140 - samples/sec: 16.60 - lr: 0.012500\n",
      "2022-05-07 00:03:34,731 epoch 125 - iter 148/373 - loss 0.02922066 - samples/sec: 18.48 - lr: 0.012500\n",
      "2022-05-07 00:03:43,420 epoch 125 - iter 185/373 - loss 0.02868086 - samples/sec: 18.07 - lr: 0.012500\n",
      "2022-05-07 00:03:51,436 epoch 125 - iter 222/373 - loss 0.02857624 - samples/sec: 19.67 - lr: 0.012500\n",
      "2022-05-07 00:04:00,398 epoch 125 - iter 259/373 - loss 0.02876559 - samples/sec: 17.53 - lr: 0.012500\n",
      "2022-05-07 00:04:09,239 epoch 125 - iter 296/373 - loss 0.02907640 - samples/sec: 17.78 - lr: 0.012500\n",
      "2022-05-07 00:04:18,042 epoch 125 - iter 333/373 - loss 0.02921675 - samples/sec: 17.83 - lr: 0.012500\n",
      "2022-05-07 00:04:27,167 epoch 125 - iter 370/373 - loss 0.02981902 - samples/sec: 17.14 - lr: 0.012500\n",
      "2022-05-07 00:04:28,378 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:04:28,379 EPOCH 125 done: loss 0.0297 - lr 0.012500\n",
      "2022-05-07 00:04:28,380 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 00:04:29,787 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:04:39,098 epoch 126 - iter 37/373 - loss 0.02627787 - samples/sec: 15.90 - lr: 0.012500\n",
      "2022-05-07 00:04:47,343 epoch 126 - iter 74/373 - loss 0.02890221 - samples/sec: 19.12 - lr: 0.012500\n",
      "2022-05-07 00:04:55,668 epoch 126 - iter 111/373 - loss 0.02896956 - samples/sec: 18.86 - lr: 0.012500\n",
      "2022-05-07 00:05:04,394 epoch 126 - iter 148/373 - loss 0.02760272 - samples/sec: 17.94 - lr: 0.012500\n",
      "2022-05-07 00:05:12,943 epoch 126 - iter 185/373 - loss 0.02806347 - samples/sec: 18.44 - lr: 0.012500\n",
      "2022-05-07 00:05:22,344 epoch 126 - iter 222/373 - loss 0.02667629 - samples/sec: 16.62 - lr: 0.012500\n",
      "2022-05-07 00:05:31,222 epoch 126 - iter 259/373 - loss 0.02807902 - samples/sec: 17.65 - lr: 0.012500\n",
      "2022-05-07 00:05:39,974 epoch 126 - iter 296/373 - loss 0.02730289 - samples/sec: 17.91 - lr: 0.012500\n",
      "2022-05-07 00:05:49,941 epoch 126 - iter 333/373 - loss 0.02698617 - samples/sec: 15.64 - lr: 0.012500\n",
      "2022-05-07 00:06:00,115 epoch 126 - iter 370/373 - loss 0.02762080 - samples/sec: 16.23 - lr: 0.012500\n",
      "2022-05-07 00:06:01,338 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:06:01,339 EPOCH 126 done: loss 0.0275 - lr 0.012500\n",
      "2022-05-07 00:06:01,339 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 00:06:02,742 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:06:12,382 epoch 127 - iter 37/373 - loss 0.02184788 - samples/sec: 15.36 - lr: 0.012500\n",
      "2022-05-07 00:06:20,682 epoch 127 - iter 74/373 - loss 0.02548909 - samples/sec: 18.91 - lr: 0.012500\n",
      "2022-05-07 00:06:28,719 epoch 127 - iter 111/373 - loss 0.02268291 - samples/sec: 19.67 - lr: 0.012500\n",
      "2022-05-07 00:06:37,809 epoch 127 - iter 148/373 - loss 0.02474814 - samples/sec: 17.18 - lr: 0.012500\n",
      "2022-05-07 00:06:46,031 epoch 127 - iter 185/373 - loss 0.02560215 - samples/sec: 19.18 - lr: 0.012500\n",
      "2022-05-07 00:06:55,591 epoch 127 - iter 222/373 - loss 0.02584175 - samples/sec: 16.35 - lr: 0.012500\n",
      "2022-05-07 00:07:05,067 epoch 127 - iter 259/373 - loss 0.02699787 - samples/sec: 16.49 - lr: 0.012500\n",
      "2022-05-07 00:07:13,969 epoch 127 - iter 296/373 - loss 0.02717640 - samples/sec: 17.59 - lr: 0.012500\n",
      "2022-05-07 00:07:22,124 epoch 127 - iter 333/373 - loss 0.02737996 - samples/sec: 19.29 - lr: 0.012500\n",
      "2022-05-07 00:07:31,433 epoch 127 - iter 370/373 - loss 0.02656960 - samples/sec: 16.80 - lr: 0.012500\n",
      "2022-05-07 00:07:32,678 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:07:32,679 EPOCH 127 done: loss 0.0266 - lr 0.012500\n",
      "2022-05-07 00:07:32,679 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 00:07:34,049 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:07:41,954 epoch 128 - iter 37/373 - loss 0.02506341 - samples/sec: 18.73 - lr: 0.012500\n",
      "2022-05-07 00:07:50,918 epoch 128 - iter 74/373 - loss 0.02540914 - samples/sec: 17.44 - lr: 0.012500\n",
      "2022-05-07 00:08:00,490 epoch 128 - iter 111/373 - loss 0.02342301 - samples/sec: 16.27 - lr: 0.012500\n",
      "2022-05-07 00:08:08,649 epoch 128 - iter 148/373 - loss 0.02536579 - samples/sec: 19.35 - lr: 0.012500\n",
      "2022-05-07 00:08:17,301 epoch 128 - iter 185/373 - loss 0.02630771 - samples/sec: 18.18 - lr: 0.012500\n",
      "2022-05-07 00:08:25,972 epoch 128 - iter 222/373 - loss 0.02562081 - samples/sec: 18.12 - lr: 0.012500\n",
      "2022-05-07 00:08:34,549 epoch 128 - iter 259/373 - loss 0.02622845 - samples/sec: 18.33 - lr: 0.012500\n",
      "2022-05-07 00:08:43,890 epoch 128 - iter 296/373 - loss 0.02670271 - samples/sec: 16.75 - lr: 0.012500\n",
      "2022-05-07 00:08:53,031 epoch 128 - iter 333/373 - loss 0.02667892 - samples/sec: 17.11 - lr: 0.012500\n",
      "2022-05-07 00:09:02,809 epoch 128 - iter 370/373 - loss 0.02691031 - samples/sec: 15.95 - lr: 0.012500\n",
      "2022-05-07 00:09:04,119 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:09:04,120 EPOCH 128 done: loss 0.0270 - lr 0.012500\n",
      "2022-05-07 00:09:04,121 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 00:09:05,504 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:09:13,315 epoch 129 - iter 37/373 - loss 0.01695579 - samples/sec: 18.95 - lr: 0.012500\n",
      "2022-05-07 00:09:22,239 epoch 129 - iter 74/373 - loss 0.02250646 - samples/sec: 17.55 - lr: 0.012500\n",
      "2022-05-07 00:09:31,422 epoch 129 - iter 111/373 - loss 0.02241729 - samples/sec: 17.00 - lr: 0.012500\n",
      "2022-05-07 00:09:40,095 epoch 129 - iter 148/373 - loss 0.02364059 - samples/sec: 18.09 - lr: 0.012500\n",
      "2022-05-07 00:09:48,496 epoch 129 - iter 185/373 - loss 0.02397993 - samples/sec: 18.75 - lr: 0.012500\n",
      "2022-05-07 00:09:57,272 epoch 129 - iter 222/373 - loss 0.02406536 - samples/sec: 17.88 - lr: 0.012500\n",
      "2022-05-07 00:10:06,863 epoch 129 - iter 259/373 - loss 0.02369421 - samples/sec: 16.31 - lr: 0.012500\n",
      "2022-05-07 00:10:15,814 epoch 129 - iter 296/373 - loss 0.02342972 - samples/sec: 17.51 - lr: 0.012500\n",
      "2022-05-07 00:10:25,059 epoch 129 - iter 333/373 - loss 0.02299553 - samples/sec: 16.91 - lr: 0.012500\n",
      "2022-05-07 00:10:34,226 epoch 129 - iter 370/373 - loss 0.02379125 - samples/sec: 17.07 - lr: 0.012500\n",
      "2022-05-07 00:10:35,426 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:10:35,426 EPOCH 129 done: loss 0.0238 - lr 0.012500\n",
      "2022-05-07 00:10:35,427 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 00:10:36,827 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:10:44,322 epoch 130 - iter 37/373 - loss 0.02310157 - samples/sec: 19.75 - lr: 0.012500\n",
      "2022-05-07 00:10:52,832 epoch 130 - iter 74/373 - loss 0.02255241 - samples/sec: 18.43 - lr: 0.012500\n",
      "2022-05-07 00:11:01,396 epoch 130 - iter 111/373 - loss 0.02321574 - samples/sec: 18.32 - lr: 0.012500\n",
      "2022-05-07 00:11:10,710 epoch 130 - iter 148/373 - loss 0.02350478 - samples/sec: 16.79 - lr: 0.012500\n",
      "2022-05-07 00:11:19,840 epoch 130 - iter 185/373 - loss 0.02330899 - samples/sec: 17.17 - lr: 0.012500\n",
      "2022-05-07 00:11:29,213 epoch 130 - iter 222/373 - loss 0.02279005 - samples/sec: 16.67 - lr: 0.012500\n",
      "2022-05-07 00:11:38,874 epoch 130 - iter 259/373 - loss 0.02325316 - samples/sec: 16.18 - lr: 0.012500\n",
      "2022-05-07 00:11:46,679 epoch 130 - iter 296/373 - loss 0.02319366 - samples/sec: 20.24 - lr: 0.012500\n",
      "2022-05-07 00:11:56,005 epoch 130 - iter 333/373 - loss 0.02337716 - samples/sec: 16.77 - lr: 0.012500\n",
      "2022-05-07 00:12:05,957 epoch 130 - iter 370/373 - loss 0.02390871 - samples/sec: 15.66 - lr: 0.012500\n",
      "2022-05-07 00:12:06,970 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:12:06,971 EPOCH 130 done: loss 0.0238 - lr 0.012500\n",
      "2022-05-07 00:12:06,971 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 00:12:08,383 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:12:16,361 epoch 131 - iter 37/373 - loss 0.02482792 - samples/sec: 18.56 - lr: 0.012500\n",
      "2022-05-07 00:12:25,354 epoch 131 - iter 74/373 - loss 0.02645535 - samples/sec: 17.40 - lr: 0.012500\n",
      "2022-05-07 00:12:34,431 epoch 131 - iter 111/373 - loss 0.02433856 - samples/sec: 17.21 - lr: 0.012500\n",
      "2022-05-07 00:12:43,508 epoch 131 - iter 148/373 - loss 0.02447803 - samples/sec: 17.27 - lr: 0.012500\n",
      "2022-05-07 00:12:52,788 epoch 131 - iter 185/373 - loss 0.02586209 - samples/sec: 16.86 - lr: 0.012500\n",
      "2022-05-07 00:13:01,789 epoch 131 - iter 222/373 - loss 0.02474184 - samples/sec: 17.44 - lr: 0.012500\n",
      "2022-05-07 00:13:10,402 epoch 131 - iter 259/373 - loss 0.02462026 - samples/sec: 18.28 - lr: 0.012500\n",
      "2022-05-07 00:13:18,852 epoch 131 - iter 296/373 - loss 0.02460062 - samples/sec: 18.59 - lr: 0.012500\n",
      "2022-05-07 00:13:28,413 epoch 131 - iter 333/373 - loss 0.02445254 - samples/sec: 16.33 - lr: 0.012500\n",
      "2022-05-07 00:13:37,348 epoch 131 - iter 370/373 - loss 0.02476297 - samples/sec: 17.55 - lr: 0.012500\n",
      "2022-05-07 00:13:38,409 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:13:38,409 EPOCH 131 done: loss 0.0248 - lr 0.012500\n",
      "2022-05-07 00:13:38,410 BAD EPOCHS (no improvement): 2\n",
      "2022-05-07 00:13:39,769 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:13:47,901 epoch 132 - iter 37/373 - loss 0.02257787 - samples/sec: 18.21 - lr: 0.012500\n",
      "2022-05-07 00:13:56,825 epoch 132 - iter 74/373 - loss 0.02405145 - samples/sec: 17.52 - lr: 0.012500\n",
      "2022-05-07 00:14:05,544 epoch 132 - iter 111/373 - loss 0.02319995 - samples/sec: 18.03 - lr: 0.012500\n",
      "2022-05-07 00:14:14,702 epoch 132 - iter 148/373 - loss 0.02283219 - samples/sec: 17.11 - lr: 0.012500\n",
      "2022-05-07 00:14:22,918 epoch 132 - iter 185/373 - loss 0.02183747 - samples/sec: 19.22 - lr: 0.012500\n",
      "2022-05-07 00:14:32,430 epoch 132 - iter 222/373 - loss 0.02214884 - samples/sec: 16.41 - lr: 0.012500\n",
      "2022-05-07 00:14:41,994 epoch 132 - iter 259/373 - loss 0.02217208 - samples/sec: 16.33 - lr: 0.012500\n",
      "2022-05-07 00:14:51,193 epoch 132 - iter 296/373 - loss 0.02337572 - samples/sec: 17.01 - lr: 0.012500\n",
      "2022-05-07 00:15:00,133 epoch 132 - iter 333/373 - loss 0.02326268 - samples/sec: 17.54 - lr: 0.012500\n",
      "2022-05-07 00:15:08,902 epoch 132 - iter 370/373 - loss 0.02337405 - samples/sec: 17.95 - lr: 0.012500\n",
      "2022-05-07 00:15:09,759 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:15:09,760 EPOCH 132 done: loss 0.0233 - lr 0.012500\n",
      "2022-05-07 00:15:09,761 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 00:15:11,153 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:15:19,893 epoch 133 - iter 37/373 - loss 0.02280831 - samples/sec: 16.94 - lr: 0.012500\n",
      "2022-05-07 00:15:28,357 epoch 133 - iter 74/373 - loss 0.02292898 - samples/sec: 18.54 - lr: 0.012500\n",
      "2022-05-07 00:15:37,259 epoch 133 - iter 111/373 - loss 0.02108901 - samples/sec: 17.66 - lr: 0.012500\n",
      "2022-05-07 00:15:45,642 epoch 133 - iter 148/373 - loss 0.02142550 - samples/sec: 18.80 - lr: 0.012500\n",
      "2022-05-07 00:15:54,950 epoch 133 - iter 185/373 - loss 0.02143953 - samples/sec: 16.78 - lr: 0.012500\n",
      "2022-05-07 00:16:03,859 epoch 133 - iter 222/373 - loss 0.02160108 - samples/sec: 17.59 - lr: 0.012500\n",
      "2022-05-07 00:16:12,650 epoch 133 - iter 259/373 - loss 0.02167563 - samples/sec: 17.87 - lr: 0.012500\n",
      "2022-05-07 00:16:21,717 epoch 133 - iter 296/373 - loss 0.02198029 - samples/sec: 17.32 - lr: 0.012500\n",
      "2022-05-07 00:16:30,363 epoch 133 - iter 333/373 - loss 0.02152897 - samples/sec: 18.15 - lr: 0.012500\n",
      "2022-05-07 00:16:39,826 epoch 133 - iter 370/373 - loss 0.02151024 - samples/sec: 16.51 - lr: 0.012500\n",
      "2022-05-07 00:16:40,907 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:16:40,908 EPOCH 133 done: loss 0.0216 - lr 0.012500\n",
      "2022-05-07 00:16:40,908 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 00:16:42,279 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:16:50,008 epoch 134 - iter 37/373 - loss 0.02330058 - samples/sec: 19.16 - lr: 0.012500\n",
      "2022-05-07 00:16:59,082 epoch 134 - iter 74/373 - loss 0.02309337 - samples/sec: 17.24 - lr: 0.012500\n",
      "2022-05-07 00:17:07,771 epoch 134 - iter 111/373 - loss 0.02240292 - samples/sec: 18.05 - lr: 0.012500\n",
      "2022-05-07 00:17:17,289 epoch 134 - iter 148/373 - loss 0.02175946 - samples/sec: 16.40 - lr: 0.012500\n",
      "2022-05-07 00:17:27,225 epoch 134 - iter 185/373 - loss 0.02181753 - samples/sec: 15.68 - lr: 0.012500\n",
      "2022-05-07 00:17:36,473 epoch 134 - iter 222/373 - loss 0.02279168 - samples/sec: 16.89 - lr: 0.012500\n",
      "2022-05-07 00:17:45,209 epoch 134 - iter 259/373 - loss 0.02229693 - samples/sec: 17.96 - lr: 0.012500\n",
      "2022-05-07 00:17:54,632 epoch 134 - iter 296/373 - loss 0.02219379 - samples/sec: 16.62 - lr: 0.012500\n",
      "2022-05-07 00:18:03,860 epoch 134 - iter 333/373 - loss 0.02166952 - samples/sec: 16.98 - lr: 0.012500\n",
      "2022-05-07 00:18:13,350 epoch 134 - iter 370/373 - loss 0.02145106 - samples/sec: 16.43 - lr: 0.012500\n",
      "2022-05-07 00:18:14,304 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:18:14,304 EPOCH 134 done: loss 0.0214 - lr 0.012500\n",
      "2022-05-07 00:18:14,305 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 00:18:15,708 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:18:24,477 epoch 135 - iter 37/373 - loss 0.01527648 - samples/sec: 16.88 - lr: 0.012500\n",
      "2022-05-07 00:18:32,444 epoch 135 - iter 74/373 - loss 0.01555607 - samples/sec: 19.84 - lr: 0.012500\n",
      "2022-05-07 00:18:42,519 epoch 135 - iter 111/373 - loss 0.01905000 - samples/sec: 15.48 - lr: 0.012500\n",
      "2022-05-07 00:18:51,240 epoch 135 - iter 148/373 - loss 0.02059433 - samples/sec: 18.10 - lr: 0.012500\n",
      "2022-05-07 00:19:00,838 epoch 135 - iter 185/373 - loss 0.02104214 - samples/sec: 16.34 - lr: 0.012500\n",
      "2022-05-07 00:19:09,494 epoch 135 - iter 222/373 - loss 0.02074297 - samples/sec: 18.17 - lr: 0.012500\n",
      "2022-05-07 00:19:18,577 epoch 135 - iter 259/373 - loss 0.02107718 - samples/sec: 17.28 - lr: 0.012500\n",
      "2022-05-07 00:19:27,383 epoch 135 - iter 296/373 - loss 0.02104141 - samples/sec: 17.84 - lr: 0.012500\n",
      "2022-05-07 00:19:36,922 epoch 135 - iter 333/373 - loss 0.02173727 - samples/sec: 16.44 - lr: 0.012500\n",
      "2022-05-07 00:19:46,420 epoch 135 - iter 370/373 - loss 0.02203163 - samples/sec: 16.49 - lr: 0.012500\n",
      "2022-05-07 00:19:47,700 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:19:47,701 EPOCH 135 done: loss 0.0220 - lr 0.012500\n",
      "2022-05-07 00:19:47,702 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 00:19:49,094 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:19:58,214 epoch 136 - iter 37/373 - loss 0.02354608 - samples/sec: 16.23 - lr: 0.012500\n",
      "2022-05-07 00:20:07,117 epoch 136 - iter 74/373 - loss 0.02285560 - samples/sec: 17.60 - lr: 0.012500\n",
      "2022-05-07 00:20:15,729 epoch 136 - iter 111/373 - loss 0.02326573 - samples/sec: 18.26 - lr: 0.012500\n",
      "2022-05-07 00:20:24,835 epoch 136 - iter 148/373 - loss 0.02220668 - samples/sec: 17.23 - lr: 0.012500\n",
      "2022-05-07 00:20:33,779 epoch 136 - iter 185/373 - loss 0.02149904 - samples/sec: 17.54 - lr: 0.012500\n",
      "2022-05-07 00:20:43,309 epoch 136 - iter 222/373 - loss 0.02119114 - samples/sec: 16.40 - lr: 0.012500\n",
      "2022-05-07 00:20:53,710 epoch 136 - iter 259/373 - loss 0.02116707 - samples/sec: 14.97 - lr: 0.012500\n",
      "2022-05-07 00:21:02,589 epoch 136 - iter 296/373 - loss 0.02080628 - samples/sec: 17.69 - lr: 0.012500\n",
      "2022-05-07 00:21:12,533 epoch 136 - iter 333/373 - loss 0.02115058 - samples/sec: 15.71 - lr: 0.012500\n",
      "2022-05-07 00:21:21,134 epoch 136 - iter 370/373 - loss 0.02082207 - samples/sec: 18.28 - lr: 0.012500\n",
      "2022-05-07 00:21:22,027 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:21:22,027 EPOCH 136 done: loss 0.0208 - lr 0.012500\n",
      "2022-05-07 00:21:22,028 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 00:21:23,363 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:21:30,453 epoch 137 - iter 37/373 - loss 0.02279281 - samples/sec: 20.89 - lr: 0.012500\n",
      "2022-05-07 00:21:40,758 epoch 137 - iter 74/373 - loss 0.01812513 - samples/sec: 15.08 - lr: 0.012500\n",
      "2022-05-07 00:21:49,639 epoch 137 - iter 111/373 - loss 0.01785545 - samples/sec: 17.66 - lr: 0.012500\n",
      "2022-05-07 00:21:58,942 epoch 137 - iter 148/373 - loss 0.01876662 - samples/sec: 16.81 - lr: 0.012500\n",
      "2022-05-07 00:22:07,867 epoch 137 - iter 185/373 - loss 0.01872836 - samples/sec: 17.58 - lr: 0.012500\n",
      "2022-05-07 00:22:16,965 epoch 137 - iter 222/373 - loss 0.01936280 - samples/sec: 17.22 - lr: 0.012500\n",
      "2022-05-07 00:22:26,328 epoch 137 - iter 259/373 - loss 0.01915662 - samples/sec: 16.70 - lr: 0.012500\n",
      "2022-05-07 00:22:34,973 epoch 137 - iter 296/373 - loss 0.01917113 - samples/sec: 18.24 - lr: 0.012500\n",
      "2022-05-07 00:22:44,462 epoch 137 - iter 333/373 - loss 0.01995056 - samples/sec: 16.46 - lr: 0.012500\n",
      "2022-05-07 00:22:53,897 epoch 137 - iter 370/373 - loss 0.02068849 - samples/sec: 16.60 - lr: 0.012500\n",
      "2022-05-07 00:22:55,098 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:22:55,099 EPOCH 137 done: loss 0.0207 - lr 0.012500\n",
      "2022-05-07 00:22:55,100 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 00:22:56,467 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:23:05,536 epoch 138 - iter 37/373 - loss 0.01938122 - samples/sec: 16.33 - lr: 0.012500\n",
      "2022-05-07 00:23:14,122 epoch 138 - iter 74/373 - loss 0.01953977 - samples/sec: 18.29 - lr: 0.012500\n",
      "2022-05-07 00:23:23,096 epoch 138 - iter 111/373 - loss 0.01898433 - samples/sec: 17.48 - lr: 0.012500\n",
      "2022-05-07 00:23:33,301 epoch 138 - iter 148/373 - loss 0.01888120 - samples/sec: 15.26 - lr: 0.012500\n",
      "2022-05-07 00:23:42,565 epoch 138 - iter 185/373 - loss 0.02071809 - samples/sec: 16.89 - lr: 0.012500\n",
      "2022-05-07 00:23:52,713 epoch 138 - iter 222/373 - loss 0.02084165 - samples/sec: 15.37 - lr: 0.012500\n",
      "2022-05-07 00:24:01,855 epoch 138 - iter 259/373 - loss 0.02004331 - samples/sec: 17.24 - lr: 0.012500\n",
      "2022-05-07 00:24:11,195 epoch 138 - iter 296/373 - loss 0.01998885 - samples/sec: 16.78 - lr: 0.012500\n",
      "2022-05-07 00:24:21,204 epoch 138 - iter 333/373 - loss 0.02029576 - samples/sec: 15.59 - lr: 0.012500\n",
      "2022-05-07 00:24:28,904 epoch 138 - iter 370/373 - loss 0.02025493 - samples/sec: 20.53 - lr: 0.012500\n",
      "2022-05-07 00:24:29,899 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:24:29,900 EPOCH 138 done: loss 0.0202 - lr 0.012500\n",
      "2022-05-07 00:24:29,900 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 00:24:31,225 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:24:39,521 epoch 139 - iter 37/373 - loss 0.01945952 - samples/sec: 17.85 - lr: 0.012500\n",
      "2022-05-07 00:24:49,222 epoch 139 - iter 74/373 - loss 0.01862456 - samples/sec: 16.05 - lr: 0.012500\n",
      "2022-05-07 00:24:57,219 epoch 139 - iter 111/373 - loss 0.01980857 - samples/sec: 19.75 - lr: 0.012500\n",
      "2022-05-07 00:25:06,919 epoch 139 - iter 148/373 - loss 0.02097376 - samples/sec: 16.07 - lr: 0.012500\n",
      "2022-05-07 00:25:16,228 epoch 139 - iter 185/373 - loss 0.02016811 - samples/sec: 16.80 - lr: 0.012500\n",
      "2022-05-07 00:25:26,060 epoch 139 - iter 222/373 - loss 0.01925435 - samples/sec: 15.90 - lr: 0.012500\n",
      "2022-05-07 00:25:34,735 epoch 139 - iter 259/373 - loss 0.01861269 - samples/sec: 18.14 - lr: 0.012500\n",
      "2022-05-07 00:25:43,758 epoch 139 - iter 296/373 - loss 0.01856844 - samples/sec: 17.35 - lr: 0.012500\n",
      "2022-05-07 00:25:52,877 epoch 139 - iter 333/373 - loss 0.01864257 - samples/sec: 17.24 - lr: 0.012500\n",
      "2022-05-07 00:26:01,618 epoch 139 - iter 370/373 - loss 0.01851387 - samples/sec: 17.96 - lr: 0.012500\n",
      "2022-05-07 00:26:02,952 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:26:02,953 EPOCH 139 done: loss 0.0184 - lr 0.012500\n",
      "2022-05-07 00:26:02,953 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 00:26:04,334 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:26:13,742 epoch 140 - iter 37/373 - loss 0.01655991 - samples/sec: 15.74 - lr: 0.012500\n",
      "2022-05-07 00:26:23,710 epoch 140 - iter 74/373 - loss 0.01635418 - samples/sec: 15.66 - lr: 0.012500\n",
      "2022-05-07 00:26:33,764 epoch 140 - iter 111/373 - loss 0.01779170 - samples/sec: 15.55 - lr: 0.012500\n",
      "2022-05-07 00:26:41,921 epoch 140 - iter 148/373 - loss 0.01715896 - samples/sec: 19.37 - lr: 0.012500\n",
      "2022-05-07 00:26:50,744 epoch 140 - iter 185/373 - loss 0.01813830 - samples/sec: 17.80 - lr: 0.012500\n",
      "2022-05-07 00:26:59,501 epoch 140 - iter 222/373 - loss 0.01843579 - samples/sec: 18.01 - lr: 0.012500\n",
      "2022-05-07 00:27:08,836 epoch 140 - iter 259/373 - loss 0.01822786 - samples/sec: 16.76 - lr: 0.012500\n",
      "2022-05-07 00:27:18,271 epoch 140 - iter 296/373 - loss 0.01829376 - samples/sec: 16.61 - lr: 0.012500\n",
      "2022-05-07 00:27:26,633 epoch 140 - iter 333/373 - loss 0.01827141 - samples/sec: 18.84 - lr: 0.012500\n",
      "2022-05-07 00:27:35,763 epoch 140 - iter 370/373 - loss 0.01803437 - samples/sec: 17.15 - lr: 0.012500\n",
      "2022-05-07 00:27:36,698 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:27:36,698 EPOCH 140 done: loss 0.0180 - lr 0.012500\n",
      "2022-05-07 00:27:36,699 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 00:27:38,077 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:27:46,430 epoch 141 - iter 37/373 - loss 0.01804982 - samples/sec: 17.73 - lr: 0.012500\n",
      "2022-05-07 00:27:54,980 epoch 141 - iter 74/373 - loss 0.01941180 - samples/sec: 18.37 - lr: 0.012500\n",
      "2022-05-07 00:28:04,665 epoch 141 - iter 111/373 - loss 0.01859058 - samples/sec: 16.13 - lr: 0.012500\n",
      "2022-05-07 00:28:13,117 epoch 141 - iter 148/373 - loss 0.01957173 - samples/sec: 18.69 - lr: 0.012500\n",
      "2022-05-07 00:28:22,017 epoch 141 - iter 185/373 - loss 0.01903839 - samples/sec: 17.65 - lr: 0.012500\n",
      "2022-05-07 00:28:31,092 epoch 141 - iter 222/373 - loss 0.01887628 - samples/sec: 17.30 - lr: 0.012500\n",
      "2022-05-07 00:28:39,582 epoch 141 - iter 259/373 - loss 0.01852153 - samples/sec: 18.58 - lr: 0.012500\n",
      "2022-05-07 00:28:49,870 epoch 141 - iter 296/373 - loss 0.01890497 - samples/sec: 15.15 - lr: 0.012500\n",
      "2022-05-07 00:28:59,055 epoch 141 - iter 333/373 - loss 0.01874209 - samples/sec: 17.09 - lr: 0.012500\n",
      "2022-05-07 00:29:08,994 epoch 141 - iter 370/373 - loss 0.01879230 - samples/sec: 15.68 - lr: 0.012500\n",
      "2022-05-07 00:29:10,393 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:29:10,394 EPOCH 141 done: loss 0.0186 - lr 0.012500\n",
      "2022-05-07 00:29:10,395 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 00:29:11,756 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:29:20,990 epoch 142 - iter 37/373 - loss 0.01971810 - samples/sec: 16.03 - lr: 0.012500\n",
      "2022-05-07 00:29:28,876 epoch 142 - iter 74/373 - loss 0.01897638 - samples/sec: 19.99 - lr: 0.012500\n",
      "2022-05-07 00:29:37,141 epoch 142 - iter 111/373 - loss 0.02144000 - samples/sec: 19.06 - lr: 0.012500\n",
      "2022-05-07 00:29:46,508 epoch 142 - iter 148/373 - loss 0.02022073 - samples/sec: 16.66 - lr: 0.012500\n",
      "2022-05-07 00:29:54,741 epoch 142 - iter 185/373 - loss 0.02062569 - samples/sec: 19.17 - lr: 0.012500\n",
      "2022-05-07 00:30:03,840 epoch 142 - iter 222/373 - loss 0.02033225 - samples/sec: 17.21 - lr: 0.012500\n",
      "2022-05-07 00:30:12,660 epoch 142 - iter 259/373 - loss 0.01962954 - samples/sec: 17.85 - lr: 0.012500\n",
      "2022-05-07 00:30:22,285 epoch 142 - iter 296/373 - loss 0.01940287 - samples/sec: 16.25 - lr: 0.012500\n",
      "2022-05-07 00:30:31,983 epoch 142 - iter 333/373 - loss 0.01907086 - samples/sec: 16.14 - lr: 0.012500\n",
      "2022-05-07 00:30:41,828 epoch 142 - iter 370/373 - loss 0.01886271 - samples/sec: 15.84 - lr: 0.012500\n",
      "2022-05-07 00:30:43,063 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:30:43,064 EPOCH 142 done: loss 0.0189 - lr 0.012500\n",
      "2022-05-07 00:30:43,064 BAD EPOCHS (no improvement): 2\n",
      "2022-05-07 00:30:44,452 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:30:52,870 epoch 143 - iter 37/373 - loss 0.01462267 - samples/sec: 17.59 - lr: 0.012500\n",
      "2022-05-07 00:31:02,460 epoch 143 - iter 74/373 - loss 0.01503899 - samples/sec: 16.26 - lr: 0.012500\n",
      "2022-05-07 00:31:11,043 epoch 143 - iter 111/373 - loss 0.01701701 - samples/sec: 18.31 - lr: 0.012500\n",
      "2022-05-07 00:31:20,394 epoch 143 - iter 148/373 - loss 0.01561201 - samples/sec: 16.74 - lr: 0.012500\n",
      "2022-05-07 00:31:28,905 epoch 143 - iter 185/373 - loss 0.01503298 - samples/sec: 18.49 - lr: 0.012500\n",
      "2022-05-07 00:31:37,701 epoch 143 - iter 222/373 - loss 0.01670867 - samples/sec: 17.85 - lr: 0.012500\n",
      "2022-05-07 00:31:46,855 epoch 143 - iter 259/373 - loss 0.01679691 - samples/sec: 17.12 - lr: 0.012500\n",
      "2022-05-07 00:31:55,569 epoch 143 - iter 296/373 - loss 0.01705894 - samples/sec: 17.99 - lr: 0.012500\n",
      "2022-05-07 00:32:04,528 epoch 143 - iter 333/373 - loss 0.01688754 - samples/sec: 17.50 - lr: 0.012500\n",
      "2022-05-07 00:32:13,471 epoch 143 - iter 370/373 - loss 0.01685126 - samples/sec: 17.57 - lr: 0.012500\n",
      "2022-05-07 00:32:14,464 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:32:14,464 EPOCH 143 done: loss 0.0169 - lr 0.012500\n",
      "2022-05-07 00:32:14,465 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 00:32:15,852 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:32:24,425 epoch 144 - iter 37/373 - loss 0.01314393 - samples/sec: 17.27 - lr: 0.012500\n",
      "2022-05-07 00:32:33,188 epoch 144 - iter 74/373 - loss 0.01354397 - samples/sec: 17.86 - lr: 0.012500\n",
      "2022-05-07 00:32:42,569 epoch 144 - iter 111/373 - loss 0.01605535 - samples/sec: 16.67 - lr: 0.012500\n",
      "2022-05-07 00:32:51,555 epoch 144 - iter 148/373 - loss 0.01761734 - samples/sec: 17.46 - lr: 0.012500\n",
      "2022-05-07 00:33:01,260 epoch 144 - iter 185/373 - loss 0.01650329 - samples/sec: 16.11 - lr: 0.012500\n",
      "2022-05-07 00:33:10,182 epoch 144 - iter 222/373 - loss 0.01609230 - samples/sec: 17.58 - lr: 0.012500\n",
      "2022-05-07 00:33:19,830 epoch 144 - iter 259/373 - loss 0.01579217 - samples/sec: 16.22 - lr: 0.012500\n",
      "2022-05-07 00:33:29,036 epoch 144 - iter 296/373 - loss 0.01642903 - samples/sec: 17.06 - lr: 0.012500\n",
      "2022-05-07 00:33:37,798 epoch 144 - iter 333/373 - loss 0.01711810 - samples/sec: 17.93 - lr: 0.012500\n",
      "2022-05-07 00:33:47,278 epoch 144 - iter 370/373 - loss 0.01682673 - samples/sec: 16.53 - lr: 0.012500\n",
      "2022-05-07 00:33:48,309 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:33:48,310 EPOCH 144 done: loss 0.0168 - lr 0.012500\n",
      "2022-05-07 00:33:48,310 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 00:33:49,725 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:33:58,376 epoch 145 - iter 37/373 - loss 0.01544064 - samples/sec: 17.12 - lr: 0.012500\n",
      "2022-05-07 00:34:07,011 epoch 145 - iter 74/373 - loss 0.01860444 - samples/sec: 18.19 - lr: 0.012500\n",
      "2022-05-07 00:34:15,635 epoch 145 - iter 111/373 - loss 0.02026298 - samples/sec: 18.23 - lr: 0.012500\n",
      "2022-05-07 00:34:24,995 epoch 145 - iter 148/373 - loss 0.01835780 - samples/sec: 16.74 - lr: 0.012500\n",
      "2022-05-07 00:34:34,410 epoch 145 - iter 185/373 - loss 0.01756131 - samples/sec: 16.66 - lr: 0.012500\n",
      "2022-05-07 00:34:43,676 epoch 145 - iter 222/373 - loss 0.01752191 - samples/sec: 16.85 - lr: 0.012500\n",
      "2022-05-07 00:34:53,418 epoch 145 - iter 259/373 - loss 0.01671370 - samples/sec: 16.06 - lr: 0.012500\n",
      "2022-05-07 00:35:03,351 epoch 145 - iter 296/373 - loss 0.01629290 - samples/sec: 15.70 - lr: 0.012500\n",
      "2022-05-07 00:35:12,881 epoch 145 - iter 333/373 - loss 0.01642075 - samples/sec: 16.36 - lr: 0.012500\n",
      "2022-05-07 00:35:21,085 epoch 145 - iter 370/373 - loss 0.01636429 - samples/sec: 19.22 - lr: 0.012500\n",
      "2022-05-07 00:35:22,293 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:35:22,293 EPOCH 145 done: loss 0.0164 - lr 0.012500\n",
      "2022-05-07 00:35:22,294 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 00:35:23,676 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:35:32,619 epoch 146 - iter 37/373 - loss 0.01294325 - samples/sec: 16.55 - lr: 0.012500\n",
      "2022-05-07 00:35:41,049 epoch 146 - iter 74/373 - loss 0.01487787 - samples/sec: 18.65 - lr: 0.012500\n",
      "2022-05-07 00:35:49,996 epoch 146 - iter 111/373 - loss 0.01485368 - samples/sec: 17.46 - lr: 0.012500\n",
      "2022-05-07 00:35:58,453 epoch 146 - iter 148/373 - loss 0.01651981 - samples/sec: 18.59 - lr: 0.012500\n",
      "2022-05-07 00:36:08,997 epoch 146 - iter 185/373 - loss 0.01604909 - samples/sec: 14.75 - lr: 0.012500\n",
      "2022-05-07 00:36:18,007 epoch 146 - iter 222/373 - loss 0.01709306 - samples/sec: 17.37 - lr: 0.012500\n",
      "2022-05-07 00:36:26,439 epoch 146 - iter 259/373 - loss 0.01694873 - samples/sec: 18.69 - lr: 0.012500\n",
      "2022-05-07 00:36:35,905 epoch 146 - iter 296/373 - loss 0.01650999 - samples/sec: 16.50 - lr: 0.012500\n",
      "2022-05-07 00:36:44,201 epoch 146 - iter 333/373 - loss 0.01661904 - samples/sec: 18.99 - lr: 0.012500\n",
      "2022-05-07 00:36:52,527 epoch 146 - iter 370/373 - loss 0.01683491 - samples/sec: 18.98 - lr: 0.012500\n",
      "2022-05-07 00:36:53,576 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:36:53,577 EPOCH 146 done: loss 0.0168 - lr 0.012500\n",
      "2022-05-07 00:36:53,577 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 00:36:54,968 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:37:02,435 epoch 147 - iter 37/373 - loss 0.01770917 - samples/sec: 19.83 - lr: 0.012500\n",
      "2022-05-07 00:37:10,632 epoch 147 - iter 74/373 - loss 0.01691322 - samples/sec: 19.20 - lr: 0.012500\n",
      "2022-05-07 00:37:19,636 epoch 147 - iter 111/373 - loss 0.01758781 - samples/sec: 17.36 - lr: 0.012500\n",
      "2022-05-07 00:37:29,448 epoch 147 - iter 148/373 - loss 0.01679854 - samples/sec: 15.87 - lr: 0.012500\n",
      "2022-05-07 00:37:39,210 epoch 147 - iter 185/373 - loss 0.01712560 - samples/sec: 15.96 - lr: 0.012500\n",
      "2022-05-07 00:37:47,987 epoch 147 - iter 222/373 - loss 0.01791855 - samples/sec: 17.85 - lr: 0.012500\n",
      "2022-05-07 00:37:56,941 epoch 147 - iter 259/373 - loss 0.01687500 - samples/sec: 17.48 - lr: 0.012500\n",
      "2022-05-07 00:38:06,517 epoch 147 - iter 296/373 - loss 0.01642009 - samples/sec: 16.27 - lr: 0.012500\n",
      "2022-05-07 00:38:15,788 epoch 147 - iter 333/373 - loss 0.01645770 - samples/sec: 16.88 - lr: 0.012500\n",
      "2022-05-07 00:38:24,787 epoch 147 - iter 370/373 - loss 0.01728978 - samples/sec: 17.43 - lr: 0.012500\n",
      "2022-05-07 00:38:25,850 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:38:25,851 EPOCH 147 done: loss 0.0172 - lr 0.012500\n",
      "2022-05-07 00:38:25,851 BAD EPOCHS (no improvement): 2\n",
      "2022-05-07 00:38:27,242 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:38:35,052 epoch 148 - iter 37/373 - loss 0.01654475 - samples/sec: 18.96 - lr: 0.012500\n",
      "2022-05-07 00:38:43,024 epoch 148 - iter 74/373 - loss 0.01720598 - samples/sec: 19.80 - lr: 0.012500\n",
      "2022-05-07 00:38:52,356 epoch 148 - iter 111/373 - loss 0.01806251 - samples/sec: 16.73 - lr: 0.012500\n",
      "2022-05-07 00:39:02,332 epoch 148 - iter 148/373 - loss 0.01728473 - samples/sec: 15.62 - lr: 0.012500\n",
      "2022-05-07 00:39:10,870 epoch 148 - iter 185/373 - loss 0.01666173 - samples/sec: 18.37 - lr: 0.012500\n",
      "2022-05-07 00:39:19,024 epoch 148 - iter 222/373 - loss 0.01634765 - samples/sec: 19.37 - lr: 0.012500\n",
      "2022-05-07 00:39:28,532 epoch 148 - iter 259/373 - loss 0.01574394 - samples/sec: 16.41 - lr: 0.012500\n",
      "2022-05-07 00:39:37,896 epoch 148 - iter 296/373 - loss 0.01585390 - samples/sec: 16.69 - lr: 0.012500\n",
      "2022-05-07 00:39:46,842 epoch 148 - iter 333/373 - loss 0.01539302 - samples/sec: 17.51 - lr: 0.012500\n",
      "2022-05-07 00:39:56,246 epoch 148 - iter 370/373 - loss 0.01593804 - samples/sec: 16.60 - lr: 0.012500\n",
      "2022-05-07 00:39:57,294 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:39:57,295 EPOCH 148 done: loss 0.0159 - lr 0.012500\n",
      "2022-05-07 00:39:57,295 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 00:39:58,674 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:40:07,069 epoch 149 - iter 37/373 - loss 0.01723009 - samples/sec: 17.64 - lr: 0.012500\n",
      "2022-05-07 00:40:15,859 epoch 149 - iter 74/373 - loss 0.01600811 - samples/sec: 17.78 - lr: 0.012500\n",
      "2022-05-07 00:40:25,117 epoch 149 - iter 111/373 - loss 0.01659097 - samples/sec: 16.88 - lr: 0.012500\n",
      "2022-05-07 00:40:34,164 epoch 149 - iter 148/373 - loss 0.01696030 - samples/sec: 17.27 - lr: 0.012500\n",
      "2022-05-07 00:40:42,923 epoch 149 - iter 185/373 - loss 0.01598462 - samples/sec: 17.89 - lr: 0.012500\n",
      "2022-05-07 00:40:51,495 epoch 149 - iter 222/373 - loss 0.01621937 - samples/sec: 18.29 - lr: 0.012500\n",
      "2022-05-07 00:41:00,486 epoch 149 - iter 259/373 - loss 0.01579125 - samples/sec: 17.41 - lr: 0.012500\n",
      "2022-05-07 00:41:10,124 epoch 149 - iter 296/373 - loss 0.01512660 - samples/sec: 16.20 - lr: 0.012500\n",
      "2022-05-07 00:41:18,802 epoch 149 - iter 333/373 - loss 0.01531131 - samples/sec: 18.06 - lr: 0.012500\n",
      "2022-05-07 00:41:27,571 epoch 149 - iter 370/373 - loss 0.01507955 - samples/sec: 17.88 - lr: 0.012500\n",
      "2022-05-07 00:41:28,493 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:41:28,493 EPOCH 149 done: loss 0.0150 - lr 0.012500\n",
      "2022-05-07 00:41:28,494 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 00:41:29,867 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:41:38,672 epoch 150 - iter 37/373 - loss 0.01757558 - samples/sec: 16.82 - lr: 0.012500\n",
      "2022-05-07 00:41:48,845 epoch 150 - iter 74/373 - loss 0.01491760 - samples/sec: 15.25 - lr: 0.012500\n",
      "2022-05-07 00:41:57,805 epoch 150 - iter 111/373 - loss 0.01381862 - samples/sec: 17.42 - lr: 0.012500\n",
      "2022-05-07 00:42:06,424 epoch 150 - iter 148/373 - loss 0.01477457 - samples/sec: 18.20 - lr: 0.012500\n",
      "2022-05-07 00:42:15,016 epoch 150 - iter 185/373 - loss 0.01433342 - samples/sec: 18.26 - lr: 0.012500\n",
      "2022-05-07 00:42:23,599 epoch 150 - iter 222/373 - loss 0.01429835 - samples/sec: 18.30 - lr: 0.012500\n",
      "2022-05-07 00:42:32,154 epoch 150 - iter 259/373 - loss 0.01527644 - samples/sec: 18.36 - lr: 0.012500\n",
      "2022-05-07 00:42:41,616 epoch 150 - iter 296/373 - loss 0.01513318 - samples/sec: 16.49 - lr: 0.012500\n",
      "2022-05-07 00:42:51,157 epoch 150 - iter 333/373 - loss 0.01502896 - samples/sec: 16.59 - lr: 0.012500\n",
      "2022-05-07 00:42:59,843 epoch 150 - iter 370/373 - loss 0.01571117 - samples/sec: 18.08 - lr: 0.012500\n",
      "2022-05-07 00:43:01,039 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:43:01,040 EPOCH 150 done: loss 0.0160 - lr 0.012500\n",
      "2022-05-07 00:43:01,041 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 00:43:02,445 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:43:11,084 epoch 151 - iter 37/373 - loss 0.01369912 - samples/sec: 17.14 - lr: 0.012500\n",
      "2022-05-07 00:43:20,485 epoch 151 - iter 74/373 - loss 0.01513985 - samples/sec: 16.59 - lr: 0.012500\n",
      "2022-05-07 00:43:28,881 epoch 151 - iter 111/373 - loss 0.01670716 - samples/sec: 18.70 - lr: 0.012500\n",
      "2022-05-07 00:43:37,441 epoch 151 - iter 148/373 - loss 0.01656994 - samples/sec: 18.33 - lr: 0.012500\n",
      "2022-05-07 00:43:46,897 epoch 151 - iter 185/373 - loss 0.01594510 - samples/sec: 16.50 - lr: 0.012500\n",
      "2022-05-07 00:43:55,200 epoch 151 - iter 222/373 - loss 0.01603735 - samples/sec: 18.97 - lr: 0.012500\n",
      "2022-05-07 00:44:04,031 epoch 151 - iter 259/373 - loss 0.01607144 - samples/sec: 17.76 - lr: 0.012500\n",
      "2022-05-07 00:44:12,751 epoch 151 - iter 296/373 - loss 0.01641433 - samples/sec: 18.02 - lr: 0.012500\n",
      "2022-05-07 00:44:22,496 epoch 151 - iter 333/373 - loss 0.01682546 - samples/sec: 16.02 - lr: 0.012500\n",
      "2022-05-07 00:44:30,870 epoch 151 - iter 370/373 - loss 0.01602325 - samples/sec: 18.77 - lr: 0.012500\n",
      "2022-05-07 00:44:31,930 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:44:31,931 EPOCH 151 done: loss 0.0161 - lr 0.012500\n",
      "2022-05-07 00:44:31,932 BAD EPOCHS (no improvement): 2\n",
      "2022-05-07 00:44:33,319 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:44:42,362 epoch 152 - iter 37/373 - loss 0.01276254 - samples/sec: 16.37 - lr: 0.012500\n",
      "2022-05-07 00:44:50,664 epoch 152 - iter 74/373 - loss 0.01178860 - samples/sec: 18.94 - lr: 0.012500\n",
      "2022-05-07 00:44:59,518 epoch 152 - iter 111/373 - loss 0.01297402 - samples/sec: 17.73 - lr: 0.012500\n",
      "2022-05-07 00:45:08,694 epoch 152 - iter 148/373 - loss 0.01285774 - samples/sec: 17.06 - lr: 0.012500\n",
      "2022-05-07 00:45:17,789 epoch 152 - iter 185/373 - loss 0.01268824 - samples/sec: 17.31 - lr: 0.012500\n",
      "2022-05-07 00:45:27,269 epoch 152 - iter 222/373 - loss 0.01339275 - samples/sec: 16.47 - lr: 0.012500\n",
      "2022-05-07 00:45:37,525 epoch 152 - iter 259/373 - loss 0.01349513 - samples/sec: 15.19 - lr: 0.012500\n",
      "2022-05-07 00:45:47,217 epoch 152 - iter 296/373 - loss 0.01433630 - samples/sec: 16.09 - lr: 0.012500\n",
      "2022-05-07 00:45:55,558 epoch 152 - iter 333/373 - loss 0.01457260 - samples/sec: 18.86 - lr: 0.012500\n",
      "2022-05-07 00:46:04,418 epoch 152 - iter 370/373 - loss 0.01465520 - samples/sec: 17.75 - lr: 0.012500\n",
      "2022-05-07 00:46:05,852 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:46:05,852 EPOCH 152 done: loss 0.0146 - lr 0.012500\n",
      "2022-05-07 00:46:05,853 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 00:46:07,321 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:46:16,499 epoch 153 - iter 37/373 - loss 0.00968752 - samples/sec: 16.13 - lr: 0.012500\n",
      "2022-05-07 00:46:25,394 epoch 153 - iter 74/373 - loss 0.01379125 - samples/sec: 17.62 - lr: 0.012500\n",
      "2022-05-07 00:46:33,361 epoch 153 - iter 111/373 - loss 0.01198461 - samples/sec: 19.83 - lr: 0.012500\n",
      "2022-05-07 00:46:42,087 epoch 153 - iter 148/373 - loss 0.01319016 - samples/sec: 18.08 - lr: 0.012500\n",
      "2022-05-07 00:46:51,276 epoch 153 - iter 185/373 - loss 0.01496629 - samples/sec: 17.08 - lr: 0.012500\n",
      "2022-05-07 00:47:00,133 epoch 153 - iter 222/373 - loss 0.01484143 - samples/sec: 17.72 - lr: 0.012500\n",
      "2022-05-07 00:47:10,620 epoch 153 - iter 259/373 - loss 0.01463074 - samples/sec: 14.87 - lr: 0.012500\n",
      "2022-05-07 00:47:19,019 epoch 153 - iter 296/373 - loss 0.01469467 - samples/sec: 18.77 - lr: 0.012500\n",
      "2022-05-07 00:47:28,906 epoch 153 - iter 333/373 - loss 0.01530546 - samples/sec: 15.81 - lr: 0.012500\n",
      "2022-05-07 00:47:38,900 epoch 153 - iter 370/373 - loss 0.01482290 - samples/sec: 15.61 - lr: 0.012500\n",
      "2022-05-07 00:47:40,044 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:47:40,045 EPOCH 153 done: loss 0.0151 - lr 0.012500\n",
      "2022-05-07 00:47:40,046 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 00:47:41,486 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:47:50,698 epoch 154 - iter 37/373 - loss 0.01230884 - samples/sec: 16.07 - lr: 0.012500\n",
      "2022-05-07 00:48:00,948 epoch 154 - iter 74/373 - loss 0.01484633 - samples/sec: 15.19 - lr: 0.012500\n",
      "2022-05-07 00:48:09,891 epoch 154 - iter 111/373 - loss 0.01469731 - samples/sec: 17.47 - lr: 0.012500\n",
      "2022-05-07 00:48:18,179 epoch 154 - iter 148/373 - loss 0.01561696 - samples/sec: 19.02 - lr: 0.012500\n",
      "2022-05-07 00:48:27,439 epoch 154 - iter 185/373 - loss 0.01508074 - samples/sec: 16.89 - lr: 0.012500\n",
      "2022-05-07 00:48:35,864 epoch 154 - iter 222/373 - loss 0.01460072 - samples/sec: 18.73 - lr: 0.012500\n",
      "2022-05-07 00:48:44,585 epoch 154 - iter 259/373 - loss 0.01397143 - samples/sec: 18.01 - lr: 0.012500\n",
      "2022-05-07 00:48:53,251 epoch 154 - iter 296/373 - loss 0.01420473 - samples/sec: 18.16 - lr: 0.012500\n",
      "2022-05-07 00:49:02,883 epoch 154 - iter 333/373 - loss 0.01434193 - samples/sec: 16.19 - lr: 0.012500\n",
      "2022-05-07 00:49:10,933 epoch 154 - iter 370/373 - loss 0.01450964 - samples/sec: 19.62 - lr: 0.012500\n",
      "2022-05-07 00:49:12,170 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:49:12,170 EPOCH 154 done: loss 0.0144 - lr 0.012500\n",
      "2022-05-07 00:49:12,171 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 00:49:13,611 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:49:21,912 epoch 155 - iter 37/373 - loss 0.02192965 - samples/sec: 17.84 - lr: 0.012500\n",
      "2022-05-07 00:49:31,261 epoch 155 - iter 74/373 - loss 0.01788550 - samples/sec: 16.71 - lr: 0.012500\n",
      "2022-05-07 00:49:39,970 epoch 155 - iter 111/373 - loss 0.01557430 - samples/sec: 17.99 - lr: 0.012500\n",
      "2022-05-07 00:49:48,989 epoch 155 - iter 148/373 - loss 0.01535738 - samples/sec: 17.32 - lr: 0.012500\n",
      "2022-05-07 00:49:56,879 epoch 155 - iter 185/373 - loss 0.01485853 - samples/sec: 20.01 - lr: 0.012500\n",
      "2022-05-07 00:50:06,287 epoch 155 - iter 222/373 - loss 0.01473972 - samples/sec: 16.62 - lr: 0.012500\n",
      "2022-05-07 00:50:15,733 epoch 155 - iter 259/373 - loss 0.01445633 - samples/sec: 16.52 - lr: 0.012500\n",
      "2022-05-07 00:50:24,717 epoch 155 - iter 296/373 - loss 0.01417927 - samples/sec: 17.43 - lr: 0.012500\n",
      "2022-05-07 00:50:33,558 epoch 155 - iter 333/373 - loss 0.01451641 - samples/sec: 17.78 - lr: 0.012500\n",
      "2022-05-07 00:50:42,777 epoch 155 - iter 370/373 - loss 0.01395239 - samples/sec: 16.98 - lr: 0.012500\n",
      "2022-05-07 00:50:44,039 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:50:44,040 EPOCH 155 done: loss 0.0139 - lr 0.012500\n",
      "2022-05-07 00:50:44,041 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 00:50:45,481 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:50:53,910 epoch 156 - iter 37/373 - loss 0.01238260 - samples/sec: 17.57 - lr: 0.012500\n",
      "2022-05-07 00:51:03,256 epoch 156 - iter 74/373 - loss 0.01594597 - samples/sec: 16.71 - lr: 0.012500\n",
      "2022-05-07 00:51:12,382 epoch 156 - iter 111/373 - loss 0.01622598 - samples/sec: 17.11 - lr: 0.012500\n",
      "2022-05-07 00:51:20,636 epoch 156 - iter 148/373 - loss 0.01468448 - samples/sec: 19.02 - lr: 0.012500\n",
      "2022-05-07 00:51:30,627 epoch 156 - iter 185/373 - loss 0.01378392 - samples/sec: 15.61 - lr: 0.012500\n",
      "2022-05-07 00:51:39,578 epoch 156 - iter 222/373 - loss 0.01456709 - samples/sec: 17.52 - lr: 0.012500\n",
      "2022-05-07 00:51:48,842 epoch 156 - iter 259/373 - loss 0.01393424 - samples/sec: 16.86 - lr: 0.012500\n",
      "2022-05-07 00:51:56,313 epoch 156 - iter 296/373 - loss 0.01387262 - samples/sec: 21.25 - lr: 0.012500\n",
      "2022-05-07 00:52:04,458 epoch 156 - iter 333/373 - loss 0.01412974 - samples/sec: 19.37 - lr: 0.012500\n",
      "2022-05-07 00:52:14,816 epoch 156 - iter 370/373 - loss 0.01435391 - samples/sec: 15.02 - lr: 0.012500\n",
      "2022-05-07 00:52:16,209 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:52:16,210 EPOCH 156 done: loss 0.0144 - lr 0.012500\n",
      "2022-05-07 00:52:16,211 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 00:52:17,647 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:52:27,858 epoch 157 - iter 37/373 - loss 0.01186778 - samples/sec: 14.50 - lr: 0.012500\n",
      "2022-05-07 00:52:35,565 epoch 157 - iter 74/373 - loss 0.01181263 - samples/sec: 20.47 - lr: 0.012500\n",
      "2022-05-07 00:52:44,229 epoch 157 - iter 111/373 - loss 0.01327906 - samples/sec: 18.10 - lr: 0.012500\n",
      "2022-05-07 00:52:52,992 epoch 157 - iter 148/373 - loss 0.01239265 - samples/sec: 17.87 - lr: 0.012500\n",
      "2022-05-07 00:53:02,226 epoch 157 - iter 185/373 - loss 0.01225778 - samples/sec: 16.94 - lr: 0.012500\n",
      "2022-05-07 00:53:11,262 epoch 157 - iter 222/373 - loss 0.01286271 - samples/sec: 17.30 - lr: 0.012500\n",
      "2022-05-07 00:53:20,072 epoch 157 - iter 259/373 - loss 0.01288861 - samples/sec: 17.91 - lr: 0.012500\n",
      "2022-05-07 00:53:29,534 epoch 157 - iter 296/373 - loss 0.01279200 - samples/sec: 16.51 - lr: 0.012500\n",
      "2022-05-07 00:53:37,865 epoch 157 - iter 333/373 - loss 0.01329598 - samples/sec: 18.91 - lr: 0.012500\n",
      "2022-05-07 00:53:47,410 epoch 157 - iter 370/373 - loss 0.01331882 - samples/sec: 16.39 - lr: 0.012500\n",
      "2022-05-07 00:53:48,314 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:53:48,315 EPOCH 157 done: loss 0.0133 - lr 0.012500\n",
      "2022-05-07 00:53:48,315 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 00:53:49,741 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:53:58,172 epoch 158 - iter 37/373 - loss 0.01264999 - samples/sec: 17.56 - lr: 0.012500\n",
      "2022-05-07 00:54:07,086 epoch 158 - iter 74/373 - loss 0.01207300 - samples/sec: 17.64 - lr: 0.012500\n",
      "2022-05-07 00:54:15,763 epoch 158 - iter 111/373 - loss 0.01069772 - samples/sec: 18.08 - lr: 0.012500\n",
      "2022-05-07 00:54:24,817 epoch 158 - iter 148/373 - loss 0.01186254 - samples/sec: 17.28 - lr: 0.012500\n",
      "2022-05-07 00:54:34,078 epoch 158 - iter 185/373 - loss 0.01221577 - samples/sec: 16.84 - lr: 0.012500\n",
      "2022-05-07 00:54:43,053 epoch 158 - iter 222/373 - loss 0.01298798 - samples/sec: 17.46 - lr: 0.012500\n",
      "2022-05-07 00:54:53,001 epoch 158 - iter 259/373 - loss 0.01321616 - samples/sec: 15.66 - lr: 0.012500\n",
      "2022-05-07 00:55:01,434 epoch 158 - iter 296/373 - loss 0.01336031 - samples/sec: 18.65 - lr: 0.012500\n",
      "2022-05-07 00:55:09,488 epoch 158 - iter 333/373 - loss 0.01386741 - samples/sec: 19.59 - lr: 0.012500\n",
      "2022-05-07 00:55:17,842 epoch 158 - iter 370/373 - loss 0.01376880 - samples/sec: 18.84 - lr: 0.012500\n",
      "2022-05-07 00:55:19,022 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:55:19,023 EPOCH 158 done: loss 0.0137 - lr 0.012500\n",
      "2022-05-07 00:55:19,023 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 00:55:20,525 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:55:30,480 epoch 159 - iter 37/373 - loss 0.00948077 - samples/sec: 14.87 - lr: 0.012500\n",
      "2022-05-07 00:55:38,722 epoch 159 - iter 74/373 - loss 0.01138608 - samples/sec: 19.19 - lr: 0.012500\n",
      "2022-05-07 00:55:47,740 epoch 159 - iter 111/373 - loss 0.01107298 - samples/sec: 17.37 - lr: 0.012500\n",
      "2022-05-07 00:55:55,871 epoch 159 - iter 148/373 - loss 0.01139821 - samples/sec: 19.38 - lr: 0.012500\n",
      "2022-05-07 00:56:05,274 epoch 159 - iter 185/373 - loss 0.01137331 - samples/sec: 16.69 - lr: 0.012500\n",
      "2022-05-07 00:56:14,823 epoch 159 - iter 222/373 - loss 0.01132922 - samples/sec: 16.37 - lr: 0.012500\n",
      "2022-05-07 00:56:24,231 epoch 159 - iter 259/373 - loss 0.01211215 - samples/sec: 16.65 - lr: 0.012500\n",
      "2022-05-07 00:56:34,461 epoch 159 - iter 296/373 - loss 0.01188050 - samples/sec: 15.24 - lr: 0.012500\n",
      "2022-05-07 00:56:43,524 epoch 159 - iter 333/373 - loss 0.01210887 - samples/sec: 17.34 - lr: 0.012500\n",
      "2022-05-07 00:56:52,667 epoch 159 - iter 370/373 - loss 0.01181081 - samples/sec: 17.15 - lr: 0.012500\n",
      "2022-05-07 00:56:53,618 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:56:53,620 EPOCH 159 done: loss 0.0120 - lr 0.012500\n",
      "2022-05-07 00:56:53,620 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 00:56:55,134 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:57:03,396 epoch 160 - iter 37/373 - loss 0.01295121 - samples/sec: 17.92 - lr: 0.012500\n",
      "2022-05-07 00:57:12,482 epoch 160 - iter 74/373 - loss 0.01190745 - samples/sec: 17.28 - lr: 0.012500\n",
      "2022-05-07 00:57:21,266 epoch 160 - iter 111/373 - loss 0.01255504 - samples/sec: 17.86 - lr: 0.012500\n",
      "2022-05-07 00:57:29,367 epoch 160 - iter 148/373 - loss 0.01333347 - samples/sec: 19.45 - lr: 0.012500\n",
      "2022-05-07 00:57:38,652 epoch 160 - iter 185/373 - loss 0.01303496 - samples/sec: 16.85 - lr: 0.012500\n",
      "2022-05-07 00:57:47,957 epoch 160 - iter 222/373 - loss 0.01291292 - samples/sec: 16.87 - lr: 0.012500\n",
      "2022-05-07 00:57:57,515 epoch 160 - iter 259/373 - loss 0.01263642 - samples/sec: 16.40 - lr: 0.012500\n",
      "2022-05-07 00:58:06,745 epoch 160 - iter 296/373 - loss 0.01284510 - samples/sec: 17.01 - lr: 0.012500\n",
      "2022-05-07 00:58:15,916 epoch 160 - iter 333/373 - loss 0.01287289 - samples/sec: 17.13 - lr: 0.012500\n",
      "2022-05-07 00:58:26,240 epoch 160 - iter 370/373 - loss 0.01313784 - samples/sec: 15.11 - lr: 0.012500\n",
      "2022-05-07 00:58:27,152 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:58:27,154 EPOCH 160 done: loss 0.0131 - lr 0.012500\n",
      "2022-05-07 00:58:27,154 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 00:58:28,573 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 00:58:37,217 epoch 161 - iter 37/373 - loss 0.01722438 - samples/sec: 17.12 - lr: 0.012500\n",
      "2022-05-07 00:58:45,705 epoch 161 - iter 74/373 - loss 0.01265846 - samples/sec: 18.55 - lr: 0.012500\n",
      "2022-05-07 00:58:55,437 epoch 161 - iter 111/373 - loss 0.01310081 - samples/sec: 16.12 - lr: 0.012500\n",
      "2022-05-07 00:59:04,094 epoch 161 - iter 148/373 - loss 0.01192705 - samples/sec: 18.16 - lr: 0.012500\n",
      "2022-05-07 00:59:12,291 epoch 161 - iter 185/373 - loss 0.01140292 - samples/sec: 19.23 - lr: 0.012500\n",
      "2022-05-07 00:59:22,157 epoch 161 - iter 222/373 - loss 0.01239890 - samples/sec: 15.88 - lr: 0.012500\n",
      "2022-05-07 00:59:31,772 epoch 161 - iter 259/373 - loss 0.01221211 - samples/sec: 16.27 - lr: 0.012500\n",
      "2022-05-07 00:59:40,489 epoch 161 - iter 296/373 - loss 0.01185186 - samples/sec: 18.03 - lr: 0.012500\n",
      "2022-05-07 00:59:49,387 epoch 161 - iter 333/373 - loss 0.01194839 - samples/sec: 17.66 - lr: 0.012500\n",
      "2022-05-07 00:59:58,776 epoch 161 - iter 370/373 - loss 0.01172478 - samples/sec: 16.68 - lr: 0.012500\n",
      "2022-05-07 01:00:00,216 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:00:00,217 EPOCH 161 done: loss 0.0116 - lr 0.012500\n",
      "2022-05-07 01:00:00,217 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 01:00:01,692 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:00:09,761 epoch 162 - iter 37/373 - loss 0.01231523 - samples/sec: 18.35 - lr: 0.012500\n",
      "2022-05-07 01:00:19,311 epoch 162 - iter 74/373 - loss 0.01512346 - samples/sec: 16.38 - lr: 0.012500\n",
      "2022-05-07 01:00:28,846 epoch 162 - iter 111/373 - loss 0.01270819 - samples/sec: 16.45 - lr: 0.012500\n",
      "2022-05-07 01:00:37,743 epoch 162 - iter 148/373 - loss 0.01376778 - samples/sec: 17.67 - lr: 0.012500\n",
      "2022-05-07 01:00:45,613 epoch 162 - iter 185/373 - loss 0.01440108 - samples/sec: 20.07 - lr: 0.012500\n",
      "2022-05-07 01:00:54,315 epoch 162 - iter 222/373 - loss 0.01456952 - samples/sec: 18.03 - lr: 0.012500\n",
      "2022-05-07 01:01:03,988 epoch 162 - iter 259/373 - loss 0.01421456 - samples/sec: 16.15 - lr: 0.012500\n",
      "2022-05-07 01:01:13,998 epoch 162 - iter 296/373 - loss 0.01416642 - samples/sec: 15.60 - lr: 0.012500\n",
      "2022-05-07 01:01:22,658 epoch 162 - iter 333/373 - loss 0.01403852 - samples/sec: 18.19 - lr: 0.012500\n",
      "2022-05-07 01:01:32,202 epoch 162 - iter 370/373 - loss 0.01393374 - samples/sec: 16.40 - lr: 0.012500\n",
      "2022-05-07 01:01:33,592 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:01:33,593 EPOCH 162 done: loss 0.0140 - lr 0.012500\n",
      "2022-05-07 01:01:33,594 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 01:01:35,004 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:01:43,859 epoch 163 - iter 37/373 - loss 0.00948259 - samples/sec: 16.72 - lr: 0.012500\n",
      "2022-05-07 01:01:54,420 epoch 163 - iter 74/373 - loss 0.01125004 - samples/sec: 14.74 - lr: 0.012500\n",
      "2022-05-07 01:02:03,033 epoch 163 - iter 111/373 - loss 0.01071382 - samples/sec: 18.30 - lr: 0.012500\n",
      "2022-05-07 01:02:12,925 epoch 163 - iter 148/373 - loss 0.01114532 - samples/sec: 15.80 - lr: 0.012500\n",
      "2022-05-07 01:02:20,966 epoch 163 - iter 185/373 - loss 0.01171325 - samples/sec: 19.64 - lr: 0.012500\n",
      "2022-05-07 01:02:30,024 epoch 163 - iter 222/373 - loss 0.01172859 - samples/sec: 17.32 - lr: 0.012500\n",
      "2022-05-07 01:02:40,263 epoch 163 - iter 259/373 - loss 0.01204702 - samples/sec: 15.23 - lr: 0.012500\n",
      "2022-05-07 01:02:50,222 epoch 163 - iter 296/373 - loss 0.01258639 - samples/sec: 15.68 - lr: 0.012500\n",
      "2022-05-07 01:02:59,008 epoch 163 - iter 333/373 - loss 0.01272290 - samples/sec: 17.92 - lr: 0.012500\n",
      "2022-05-07 01:03:07,751 epoch 163 - iter 370/373 - loss 0.01231583 - samples/sec: 17.96 - lr: 0.012500\n",
      "2022-05-07 01:03:08,627 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:03:08,628 EPOCH 163 done: loss 0.0123 - lr 0.012500\n",
      "2022-05-07 01:03:08,628 BAD EPOCHS (no improvement): 2\n",
      "2022-05-07 01:03:10,111 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:03:19,164 epoch 164 - iter 37/373 - loss 0.01152174 - samples/sec: 16.35 - lr: 0.012500\n",
      "2022-05-07 01:03:27,712 epoch 164 - iter 74/373 - loss 0.01537312 - samples/sec: 18.46 - lr: 0.012500\n",
      "2022-05-07 01:03:37,808 epoch 164 - iter 111/373 - loss 0.01526397 - samples/sec: 15.45 - lr: 0.012500\n",
      "2022-05-07 01:03:46,441 epoch 164 - iter 148/373 - loss 0.01419614 - samples/sec: 18.24 - lr: 0.012500\n",
      "2022-05-07 01:03:55,439 epoch 164 - iter 185/373 - loss 0.01414493 - samples/sec: 17.47 - lr: 0.012500\n",
      "2022-05-07 01:04:05,422 epoch 164 - iter 222/373 - loss 0.01345297 - samples/sec: 15.64 - lr: 0.012500\n",
      "2022-05-07 01:04:15,365 epoch 164 - iter 259/373 - loss 0.01301960 - samples/sec: 15.75 - lr: 0.012500\n",
      "2022-05-07 01:04:24,390 epoch 164 - iter 296/373 - loss 0.01279483 - samples/sec: 17.41 - lr: 0.012500\n",
      "2022-05-07 01:04:32,761 epoch 164 - iter 333/373 - loss 0.01241516 - samples/sec: 18.83 - lr: 0.012500\n",
      "2022-05-07 01:04:41,991 epoch 164 - iter 370/373 - loss 0.01270065 - samples/sec: 16.98 - lr: 0.012500\n",
      "2022-05-07 01:04:43,145 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:04:43,146 EPOCH 164 done: loss 0.0129 - lr 0.012500\n",
      "2022-05-07 01:04:43,147 BAD EPOCHS (no improvement): 3\n",
      "2022-05-07 01:04:44,549 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:04:52,840 epoch 165 - iter 37/373 - loss 0.01533632 - samples/sec: 17.86 - lr: 0.012500\n",
      "2022-05-07 01:05:01,509 epoch 165 - iter 74/373 - loss 0.01237617 - samples/sec: 18.15 - lr: 0.012500\n",
      "2022-05-07 01:05:10,600 epoch 165 - iter 111/373 - loss 0.01143242 - samples/sec: 17.21 - lr: 0.012500\n",
      "2022-05-07 01:05:20,398 epoch 165 - iter 148/373 - loss 0.01141753 - samples/sec: 15.92 - lr: 0.012500\n",
      "2022-05-07 01:05:29,647 epoch 165 - iter 185/373 - loss 0.01091622 - samples/sec: 16.88 - lr: 0.012500\n",
      "2022-05-07 01:05:38,308 epoch 165 - iter 222/373 - loss 0.01057780 - samples/sec: 18.11 - lr: 0.012500\n",
      "2022-05-07 01:05:46,873 epoch 165 - iter 259/373 - loss 0.01076683 - samples/sec: 18.35 - lr: 0.012500\n",
      "2022-05-07 01:05:55,626 epoch 165 - iter 296/373 - loss 0.01092165 - samples/sec: 17.96 - lr: 0.012500\n",
      "2022-05-07 01:06:04,794 epoch 165 - iter 333/373 - loss 0.01153522 - samples/sec: 17.03 - lr: 0.012500\n",
      "2022-05-07 01:06:14,910 epoch 165 - iter 370/373 - loss 0.01170944 - samples/sec: 15.39 - lr: 0.012500\n",
      "2022-05-07 01:06:16,110 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:06:16,111 EPOCH 165 done: loss 0.0117 - lr 0.012500\n",
      "2022-05-07 01:06:16,111 Epoch   165: reducing learning rate of group 0 to 6.2500e-03.\n",
      "2022-05-07 01:06:16,111 BAD EPOCHS (no improvement): 4\n",
      "2022-05-07 01:06:17,666 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:06:26,284 epoch 166 - iter 37/373 - loss 0.00914604 - samples/sec: 17.18 - lr: 0.006250\n",
      "2022-05-07 01:06:34,818 epoch 166 - iter 74/373 - loss 0.01213787 - samples/sec: 18.48 - lr: 0.006250\n",
      "2022-05-07 01:06:44,484 epoch 166 - iter 111/373 - loss 0.01004185 - samples/sec: 16.14 - lr: 0.006250\n",
      "2022-05-07 01:06:52,910 epoch 166 - iter 148/373 - loss 0.01040486 - samples/sec: 18.70 - lr: 0.006250\n",
      "2022-05-07 01:07:02,250 epoch 166 - iter 185/373 - loss 0.01005376 - samples/sec: 16.80 - lr: 0.006250\n",
      "2022-05-07 01:07:11,935 epoch 166 - iter 222/373 - loss 0.00979619 - samples/sec: 16.12 - lr: 0.006250\n",
      "2022-05-07 01:07:20,635 epoch 166 - iter 259/373 - loss 0.00961519 - samples/sec: 18.04 - lr: 0.006250\n",
      "2022-05-07 01:07:30,148 epoch 166 - iter 296/373 - loss 0.00995286 - samples/sec: 16.49 - lr: 0.006250\n",
      "2022-05-07 01:07:40,270 epoch 166 - iter 333/373 - loss 0.01046065 - samples/sec: 15.40 - lr: 0.006250\n",
      "2022-05-07 01:07:49,563 epoch 166 - iter 370/373 - loss 0.01023127 - samples/sec: 16.94 - lr: 0.006250\n",
      "2022-05-07 01:07:50,717 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:07:50,718 EPOCH 166 done: loss 0.0102 - lr 0.006250\n",
      "2022-05-07 01:07:50,719 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 01:07:52,193 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:08:01,093 epoch 167 - iter 37/373 - loss 0.00882360 - samples/sec: 16.64 - lr: 0.006250\n",
      "2022-05-07 01:08:10,554 epoch 167 - iter 74/373 - loss 0.00730829 - samples/sec: 16.58 - lr: 0.006250\n",
      "2022-05-07 01:08:19,861 epoch 167 - iter 111/373 - loss 0.00761447 - samples/sec: 16.85 - lr: 0.006250\n",
      "2022-05-07 01:08:29,012 epoch 167 - iter 148/373 - loss 0.00799390 - samples/sec: 17.09 - lr: 0.006250\n",
      "2022-05-07 01:08:37,368 epoch 167 - iter 185/373 - loss 0.00873906 - samples/sec: 18.82 - lr: 0.006250\n",
      "2022-05-07 01:08:45,984 epoch 167 - iter 222/373 - loss 0.00893905 - samples/sec: 18.22 - lr: 0.006250\n",
      "2022-05-07 01:08:54,841 epoch 167 - iter 259/373 - loss 0.00884143 - samples/sec: 17.67 - lr: 0.006250\n",
      "2022-05-07 01:09:03,096 epoch 167 - iter 296/373 - loss 0.00861720 - samples/sec: 19.06 - lr: 0.006250\n",
      "2022-05-07 01:09:13,328 epoch 167 - iter 333/373 - loss 0.00879035 - samples/sec: 15.20 - lr: 0.006250\n",
      "2022-05-07 01:09:21,797 epoch 167 - iter 370/373 - loss 0.00920871 - samples/sec: 18.56 - lr: 0.006250\n",
      "2022-05-07 01:09:22,848 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:09:22,849 EPOCH 167 done: loss 0.0093 - lr 0.006250\n",
      "2022-05-07 01:09:22,850 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 01:09:24,304 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:09:34,266 epoch 168 - iter 37/373 - loss 0.00775819 - samples/sec: 14.86 - lr: 0.006250\n",
      "2022-05-07 01:09:43,170 epoch 168 - iter 74/373 - loss 0.00730036 - samples/sec: 17.58 - lr: 0.006250\n",
      "2022-05-07 01:09:51,795 epoch 168 - iter 111/373 - loss 0.00959408 - samples/sec: 18.22 - lr: 0.006250\n",
      "2022-05-07 01:10:01,609 epoch 168 - iter 148/373 - loss 0.00917175 - samples/sec: 15.90 - lr: 0.006250\n",
      "2022-05-07 01:10:10,987 epoch 168 - iter 185/373 - loss 0.00864593 - samples/sec: 16.63 - lr: 0.006250\n",
      "2022-05-07 01:10:18,638 epoch 168 - iter 222/373 - loss 0.00955611 - samples/sec: 20.70 - lr: 0.006250\n",
      "2022-05-07 01:10:26,742 epoch 168 - iter 259/373 - loss 0.00989409 - samples/sec: 19.41 - lr: 0.006250\n",
      "2022-05-07 01:10:36,609 epoch 168 - iter 296/373 - loss 0.00962918 - samples/sec: 15.77 - lr: 0.006250\n",
      "2022-05-07 01:10:44,915 epoch 168 - iter 333/373 - loss 0.00958658 - samples/sec: 18.96 - lr: 0.006250\n",
      "2022-05-07 01:10:54,404 epoch 168 - iter 370/373 - loss 0.00943619 - samples/sec: 16.43 - lr: 0.006250\n",
      "2022-05-07 01:10:55,533 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:10:55,534 EPOCH 168 done: loss 0.0094 - lr 0.006250\n",
      "2022-05-07 01:10:55,534 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 01:10:57,009 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:11:05,976 epoch 169 - iter 37/373 - loss 0.00709133 - samples/sec: 16.51 - lr: 0.006250\n",
      "2022-05-07 01:11:14,979 epoch 169 - iter 74/373 - loss 0.01012368 - samples/sec: 17.38 - lr: 0.006250\n",
      "2022-05-07 01:11:23,757 epoch 169 - iter 111/373 - loss 0.01002706 - samples/sec: 17.85 - lr: 0.006250\n",
      "2022-05-07 01:11:32,876 epoch 169 - iter 148/373 - loss 0.00971781 - samples/sec: 17.18 - lr: 0.006250\n",
      "2022-05-07 01:11:43,275 epoch 169 - iter 185/373 - loss 0.00961464 - samples/sec: 14.95 - lr: 0.006250\n",
      "2022-05-07 01:11:51,098 epoch 169 - iter 222/373 - loss 0.00923149 - samples/sec: 20.15 - lr: 0.006250\n",
      "2022-05-07 01:11:59,099 epoch 169 - iter 259/373 - loss 0.00927471 - samples/sec: 19.70 - lr: 0.006250\n",
      "2022-05-07 01:12:07,870 epoch 169 - iter 296/373 - loss 0.00981937 - samples/sec: 17.87 - lr: 0.006250\n",
      "2022-05-07 01:12:16,818 epoch 169 - iter 333/373 - loss 0.01004051 - samples/sec: 17.50 - lr: 0.006250\n",
      "2022-05-07 01:12:25,467 epoch 169 - iter 370/373 - loss 0.00984222 - samples/sec: 18.17 - lr: 0.006250\n",
      "2022-05-07 01:12:26,563 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:12:26,564 EPOCH 169 done: loss 0.0098 - lr 0.006250\n",
      "2022-05-07 01:12:26,565 BAD EPOCHS (no improvement): 2\n",
      "2022-05-07 01:12:27,969 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:12:35,928 epoch 170 - iter 37/373 - loss 0.01101774 - samples/sec: 18.60 - lr: 0.006250\n",
      "2022-05-07 01:12:45,308 epoch 170 - iter 74/373 - loss 0.00845267 - samples/sec: 16.71 - lr: 0.006250\n",
      "2022-05-07 01:12:54,318 epoch 170 - iter 111/373 - loss 0.00918661 - samples/sec: 17.36 - lr: 0.006250\n",
      "2022-05-07 01:13:03,044 epoch 170 - iter 148/373 - loss 0.00933046 - samples/sec: 17.99 - lr: 0.006250\n",
      "2022-05-07 01:13:11,456 epoch 170 - iter 185/373 - loss 0.01006751 - samples/sec: 18.70 - lr: 0.006250\n",
      "2022-05-07 01:13:20,505 epoch 170 - iter 222/373 - loss 0.01091918 - samples/sec: 17.33 - lr: 0.006250\n",
      "2022-05-07 01:13:28,783 epoch 170 - iter 259/373 - loss 0.01052979 - samples/sec: 19.00 - lr: 0.006250\n",
      "2022-05-07 01:13:37,988 epoch 170 - iter 296/373 - loss 0.00989399 - samples/sec: 16.99 - lr: 0.006250\n",
      "2022-05-07 01:13:47,292 epoch 170 - iter 333/373 - loss 0.00947338 - samples/sec: 16.76 - lr: 0.006250\n",
      "2022-05-07 01:13:56,501 epoch 170 - iter 370/373 - loss 0.00964326 - samples/sec: 16.98 - lr: 0.006250\n",
      "2022-05-07 01:13:57,570 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:13:57,571 EPOCH 170 done: loss 0.0096 - lr 0.006250\n",
      "2022-05-07 01:13:57,572 BAD EPOCHS (no improvement): 3\n",
      "2022-05-07 01:13:58,972 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:14:06,802 epoch 171 - iter 37/373 - loss 0.00601300 - samples/sec: 18.91 - lr: 0.006250\n",
      "2022-05-07 01:14:16,067 epoch 171 - iter 74/373 - loss 0.01052924 - samples/sec: 16.90 - lr: 0.006250\n",
      "2022-05-07 01:14:24,786 epoch 171 - iter 111/373 - loss 0.01098179 - samples/sec: 17.99 - lr: 0.006250\n",
      "2022-05-07 01:14:33,750 epoch 171 - iter 148/373 - loss 0.01005280 - samples/sec: 17.47 - lr: 0.006250\n",
      "2022-05-07 01:14:43,427 epoch 171 - iter 185/373 - loss 0.01030454 - samples/sec: 16.14 - lr: 0.006250\n",
      "2022-05-07 01:14:53,238 epoch 171 - iter 222/373 - loss 0.00975891 - samples/sec: 15.89 - lr: 0.006250\n",
      "2022-05-07 01:15:02,299 epoch 171 - iter 259/373 - loss 0.01031869 - samples/sec: 17.28 - lr: 0.006250\n",
      "2022-05-07 01:15:11,067 epoch 171 - iter 296/373 - loss 0.01022900 - samples/sec: 17.84 - lr: 0.006250\n",
      "2022-05-07 01:15:19,388 epoch 171 - iter 333/373 - loss 0.01015979 - samples/sec: 18.87 - lr: 0.006250\n",
      "2022-05-07 01:15:28,187 epoch 171 - iter 370/373 - loss 0.00997220 - samples/sec: 17.86 - lr: 0.006250\n",
      "2022-05-07 01:15:29,500 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:15:29,501 EPOCH 171 done: loss 0.0101 - lr 0.006250\n",
      "2022-05-07 01:15:29,501 Epoch   171: reducing learning rate of group 0 to 3.1250e-03.\n",
      "2022-05-07 01:15:29,501 BAD EPOCHS (no improvement): 4\n",
      "2022-05-07 01:15:30,994 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:15:39,689 epoch 172 - iter 37/373 - loss 0.01027348 - samples/sec: 17.03 - lr: 0.003125\n",
      "2022-05-07 01:15:48,122 epoch 172 - iter 74/373 - loss 0.00853325 - samples/sec: 18.65 - lr: 0.003125\n",
      "2022-05-07 01:15:57,291 epoch 172 - iter 111/373 - loss 0.00918032 - samples/sec: 17.07 - lr: 0.003125\n",
      "2022-05-07 01:16:06,881 epoch 172 - iter 148/373 - loss 0.00911798 - samples/sec: 16.27 - lr: 0.003125\n",
      "2022-05-07 01:16:16,425 epoch 172 - iter 185/373 - loss 0.00901922 - samples/sec: 16.36 - lr: 0.003125\n",
      "2022-05-07 01:16:25,022 epoch 172 - iter 222/373 - loss 0.00865474 - samples/sec: 18.32 - lr: 0.003125\n",
      "2022-05-07 01:16:33,585 epoch 172 - iter 259/373 - loss 0.00881149 - samples/sec: 18.34 - lr: 0.003125\n",
      "2022-05-07 01:16:41,633 epoch 172 - iter 296/373 - loss 0.00847402 - samples/sec: 19.59 - lr: 0.003125\n",
      "2022-05-07 01:16:50,689 epoch 172 - iter 333/373 - loss 0.00843598 - samples/sec: 17.26 - lr: 0.003125\n",
      "2022-05-07 01:17:00,070 epoch 172 - iter 370/373 - loss 0.00882573 - samples/sec: 16.62 - lr: 0.003125\n",
      "2022-05-07 01:17:01,120 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:17:01,120 EPOCH 172 done: loss 0.0088 - lr 0.003125\n",
      "2022-05-07 01:17:01,121 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 01:17:02,542 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:17:11,359 epoch 173 - iter 37/373 - loss 0.00848431 - samples/sec: 16.79 - lr: 0.003125\n",
      "2022-05-07 01:17:20,129 epoch 173 - iter 74/373 - loss 0.00747229 - samples/sec: 17.89 - lr: 0.003125\n",
      "2022-05-07 01:17:29,024 epoch 173 - iter 111/373 - loss 0.00806368 - samples/sec: 17.61 - lr: 0.003125\n",
      "2022-05-07 01:17:38,528 epoch 173 - iter 148/373 - loss 0.00829512 - samples/sec: 16.45 - lr: 0.003125\n",
      "2022-05-07 01:17:47,054 epoch 173 - iter 185/373 - loss 0.00807641 - samples/sec: 18.45 - lr: 0.003125\n",
      "2022-05-07 01:17:55,697 epoch 173 - iter 222/373 - loss 0.00834391 - samples/sec: 18.17 - lr: 0.003125\n",
      "2022-05-07 01:18:04,388 epoch 173 - iter 259/373 - loss 0.00844435 - samples/sec: 18.05 - lr: 0.003125\n",
      "2022-05-07 01:18:13,335 epoch 173 - iter 296/373 - loss 0.00910476 - samples/sec: 17.51 - lr: 0.003125\n",
      "2022-05-07 01:18:22,264 epoch 173 - iter 333/373 - loss 0.00880449 - samples/sec: 17.51 - lr: 0.003125\n",
      "2022-05-07 01:18:31,189 epoch 173 - iter 370/373 - loss 0.00849224 - samples/sec: 17.56 - lr: 0.003125\n",
      "2022-05-07 01:18:32,437 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:18:32,438 EPOCH 173 done: loss 0.0084 - lr 0.003125\n",
      "2022-05-07 01:18:32,439 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 01:18:33,846 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:18:43,503 epoch 174 - iter 37/373 - loss 0.01149558 - samples/sec: 15.33 - lr: 0.003125\n",
      "2022-05-07 01:18:51,828 epoch 174 - iter 74/373 - loss 0.01098534 - samples/sec: 18.90 - lr: 0.003125\n",
      "2022-05-07 01:19:01,590 epoch 174 - iter 111/373 - loss 0.01070901 - samples/sec: 15.97 - lr: 0.003125\n",
      "2022-05-07 01:19:10,336 epoch 174 - iter 148/373 - loss 0.00957637 - samples/sec: 18.13 - lr: 0.003125\n",
      "2022-05-07 01:19:19,761 epoch 174 - iter 185/373 - loss 0.00925278 - samples/sec: 16.59 - lr: 0.003125\n",
      "2022-05-07 01:19:28,963 epoch 174 - iter 222/373 - loss 0.00930363 - samples/sec: 16.98 - lr: 0.003125\n",
      "2022-05-07 01:19:38,301 epoch 174 - iter 259/373 - loss 0.00907933 - samples/sec: 16.80 - lr: 0.003125\n",
      "2022-05-07 01:19:47,335 epoch 174 - iter 296/373 - loss 0.00854349 - samples/sec: 17.36 - lr: 0.003125\n",
      "2022-05-07 01:19:54,977 epoch 174 - iter 333/373 - loss 0.00839224 - samples/sec: 20.78 - lr: 0.003125\n",
      "2022-05-07 01:20:03,790 epoch 174 - iter 370/373 - loss 0.00809543 - samples/sec: 17.78 - lr: 0.003125\n",
      "2022-05-07 01:20:04,995 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:20:04,996 EPOCH 174 done: loss 0.0082 - lr 0.003125\n",
      "2022-05-07 01:20:04,997 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 01:20:06,392 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:20:14,703 epoch 175 - iter 37/373 - loss 0.00740020 - samples/sec: 17.82 - lr: 0.003125\n",
      "2022-05-07 01:20:24,238 epoch 175 - iter 74/373 - loss 0.00543361 - samples/sec: 16.36 - lr: 0.003125\n",
      "2022-05-07 01:20:32,641 epoch 175 - iter 111/373 - loss 0.00754163 - samples/sec: 18.69 - lr: 0.003125\n",
      "2022-05-07 01:20:41,913 epoch 175 - iter 148/373 - loss 0.00764774 - samples/sec: 16.89 - lr: 0.003125\n",
      "2022-05-07 01:20:50,748 epoch 175 - iter 185/373 - loss 0.00718798 - samples/sec: 17.77 - lr: 0.003125\n",
      "2022-05-07 01:20:59,707 epoch 175 - iter 222/373 - loss 0.00759965 - samples/sec: 17.50 - lr: 0.003125\n",
      "2022-05-07 01:21:08,120 epoch 175 - iter 259/373 - loss 0.00789264 - samples/sec: 18.71 - lr: 0.003125\n",
      "2022-05-07 01:21:16,418 epoch 175 - iter 296/373 - loss 0.00789275 - samples/sec: 18.97 - lr: 0.003125\n",
      "2022-05-07 01:21:26,053 epoch 175 - iter 333/373 - loss 0.00798160 - samples/sec: 16.17 - lr: 0.003125\n",
      "2022-05-07 01:21:34,739 epoch 175 - iter 370/373 - loss 0.00792492 - samples/sec: 18.06 - lr: 0.003125\n",
      "2022-05-07 01:21:35,744 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:21:35,745 EPOCH 175 done: loss 0.0079 - lr 0.003125\n",
      "2022-05-07 01:21:35,746 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 01:21:37,120 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:21:46,260 epoch 176 - iter 37/373 - loss 0.00860953 - samples/sec: 16.20 - lr: 0.003125\n",
      "2022-05-07 01:21:55,600 epoch 176 - iter 74/373 - loss 0.00733057 - samples/sec: 16.75 - lr: 0.003125\n",
      "2022-05-07 01:22:05,359 epoch 176 - iter 111/373 - loss 0.00706289 - samples/sec: 15.99 - lr: 0.003125\n",
      "2022-05-07 01:22:15,000 epoch 176 - iter 148/373 - loss 0.00819018 - samples/sec: 16.21 - lr: 0.003125\n",
      "2022-05-07 01:22:23,613 epoch 176 - iter 185/373 - loss 0.00747791 - samples/sec: 18.26 - lr: 0.003125\n",
      "2022-05-07 01:22:32,398 epoch 176 - iter 222/373 - loss 0.00785235 - samples/sec: 17.89 - lr: 0.003125\n",
      "2022-05-07 01:22:40,834 epoch 176 - iter 259/373 - loss 0.00801131 - samples/sec: 18.64 - lr: 0.003125\n",
      "2022-05-07 01:22:49,138 epoch 176 - iter 296/373 - loss 0.00782558 - samples/sec: 18.93 - lr: 0.003125\n",
      "2022-05-07 01:22:57,077 epoch 176 - iter 333/373 - loss 0.00778456 - samples/sec: 19.93 - lr: 0.003125\n",
      "2022-05-07 01:23:06,193 epoch 176 - iter 370/373 - loss 0.00780965 - samples/sec: 17.16 - lr: 0.003125\n",
      "2022-05-07 01:23:07,048 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:23:07,049 EPOCH 176 done: loss 0.0078 - lr 0.003125\n",
      "2022-05-07 01:23:07,049 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 01:23:08,436 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:23:16,549 epoch 177 - iter 37/373 - loss 0.01116289 - samples/sec: 18.25 - lr: 0.003125\n",
      "2022-05-07 01:23:25,320 epoch 177 - iter 74/373 - loss 0.01106885 - samples/sec: 17.88 - lr: 0.003125\n",
      "2022-05-07 01:23:34,013 epoch 177 - iter 111/373 - loss 0.00917764 - samples/sec: 18.04 - lr: 0.003125\n",
      "2022-05-07 01:23:42,991 epoch 177 - iter 148/373 - loss 0.00868795 - samples/sec: 17.46 - lr: 0.003125\n",
      "2022-05-07 01:23:51,594 epoch 177 - iter 185/373 - loss 0.00817557 - samples/sec: 18.26 - lr: 0.003125\n",
      "2022-05-07 01:24:00,353 epoch 177 - iter 222/373 - loss 0.00813206 - samples/sec: 17.92 - lr: 0.003125\n",
      "2022-05-07 01:24:09,235 epoch 177 - iter 259/373 - loss 0.00767306 - samples/sec: 17.69 - lr: 0.003125\n",
      "2022-05-07 01:24:17,933 epoch 177 - iter 296/373 - loss 0.00756406 - samples/sec: 18.12 - lr: 0.003125\n",
      "2022-05-07 01:24:27,040 epoch 177 - iter 333/373 - loss 0.00770903 - samples/sec: 17.23 - lr: 0.003125\n",
      "2022-05-07 01:24:36,456 epoch 177 - iter 370/373 - loss 0.00773255 - samples/sec: 16.57 - lr: 0.003125\n",
      "2022-05-07 01:24:37,551 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:24:37,552 EPOCH 177 done: loss 0.0077 - lr 0.003125\n",
      "2022-05-07 01:24:37,552 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 01:24:38,926 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:24:47,161 epoch 178 - iter 37/373 - loss 0.01158889 - samples/sec: 17.98 - lr: 0.003125\n",
      "2022-05-07 01:24:55,407 epoch 178 - iter 74/373 - loss 0.00956570 - samples/sec: 19.06 - lr: 0.003125\n",
      "2022-05-07 01:25:04,984 epoch 178 - iter 111/373 - loss 0.01010873 - samples/sec: 16.31 - lr: 0.003125\n",
      "2022-05-07 01:25:14,462 epoch 178 - iter 148/373 - loss 0.00904448 - samples/sec: 16.48 - lr: 0.003125\n",
      "2022-05-07 01:25:23,120 epoch 178 - iter 185/373 - loss 0.00859003 - samples/sec: 18.14 - lr: 0.003125\n",
      "2022-05-07 01:25:32,885 epoch 178 - iter 222/373 - loss 0.00827981 - samples/sec: 15.96 - lr: 0.003125\n",
      "2022-05-07 01:25:42,341 epoch 178 - iter 259/373 - loss 0.00825707 - samples/sec: 16.56 - lr: 0.003125\n",
      "2022-05-07 01:25:50,319 epoch 178 - iter 296/373 - loss 0.00802296 - samples/sec: 19.77 - lr: 0.003125\n",
      "2022-05-07 01:25:58,659 epoch 178 - iter 333/373 - loss 0.00789024 - samples/sec: 18.90 - lr: 0.003125\n",
      "2022-05-07 01:26:07,436 epoch 178 - iter 370/373 - loss 0.00773897 - samples/sec: 17.88 - lr: 0.003125\n",
      "2022-05-07 01:26:08,531 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:26:08,531 EPOCH 178 done: loss 0.0077 - lr 0.003125\n",
      "2022-05-07 01:26:08,532 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 01:26:09,921 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:26:18,778 epoch 179 - iter 37/373 - loss 0.01026397 - samples/sec: 16.72 - lr: 0.003125\n",
      "2022-05-07 01:26:27,530 epoch 179 - iter 74/373 - loss 0.00954300 - samples/sec: 17.89 - lr: 0.003125\n",
      "2022-05-07 01:26:35,612 epoch 179 - iter 111/373 - loss 0.00840585 - samples/sec: 19.49 - lr: 0.003125\n",
      "2022-05-07 01:26:43,604 epoch 179 - iter 148/373 - loss 0.00869537 - samples/sec: 19.73 - lr: 0.003125\n",
      "2022-05-07 01:26:52,909 epoch 179 - iter 185/373 - loss 0.00870837 - samples/sec: 16.78 - lr: 0.003125\n",
      "2022-05-07 01:27:02,655 epoch 179 - iter 222/373 - loss 0.00867151 - samples/sec: 16.03 - lr: 0.003125\n",
      "2022-05-07 01:27:11,669 epoch 179 - iter 259/373 - loss 0.00854082 - samples/sec: 17.42 - lr: 0.003125\n",
      "2022-05-07 01:27:20,794 epoch 179 - iter 296/373 - loss 0.00807973 - samples/sec: 17.21 - lr: 0.003125\n",
      "2022-05-07 01:27:29,841 epoch 179 - iter 333/373 - loss 0.00773321 - samples/sec: 17.35 - lr: 0.003125\n",
      "2022-05-07 01:27:39,350 epoch 179 - iter 370/373 - loss 0.00753935 - samples/sec: 16.48 - lr: 0.003125\n",
      "2022-05-07 01:27:40,503 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:27:40,504 EPOCH 179 done: loss 0.0075 - lr 0.003125\n",
      "2022-05-07 01:27:40,505 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 01:27:41,964 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:27:51,555 epoch 180 - iter 37/373 - loss 0.00778676 - samples/sec: 15.44 - lr: 0.003125\n",
      "2022-05-07 01:28:00,350 epoch 180 - iter 74/373 - loss 0.00765145 - samples/sec: 17.84 - lr: 0.003125\n",
      "2022-05-07 01:28:09,381 epoch 180 - iter 111/373 - loss 0.00798657 - samples/sec: 17.35 - lr: 0.003125\n",
      "2022-05-07 01:28:18,758 epoch 180 - iter 148/373 - loss 0.00867522 - samples/sec: 16.76 - lr: 0.003125\n",
      "2022-05-07 01:28:28,671 epoch 180 - iter 185/373 - loss 0.00847182 - samples/sec: 15.74 - lr: 0.003125\n",
      "2022-05-07 01:28:37,875 epoch 180 - iter 222/373 - loss 0.00843747 - samples/sec: 17.03 - lr: 0.003125\n",
      "2022-05-07 01:28:46,756 epoch 180 - iter 259/373 - loss 0.00835329 - samples/sec: 17.69 - lr: 0.003125\n",
      "2022-05-07 01:28:55,292 epoch 180 - iter 296/373 - loss 0.00815158 - samples/sec: 18.44 - lr: 0.003125\n",
      "2022-05-07 01:29:04,432 epoch 180 - iter 333/373 - loss 0.00805669 - samples/sec: 17.17 - lr: 0.003125\n",
      "2022-05-07 01:29:13,457 epoch 180 - iter 370/373 - loss 0.00799256 - samples/sec: 17.40 - lr: 0.003125\n",
      "2022-05-07 01:29:14,483 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:29:14,484 EPOCH 180 done: loss 0.0080 - lr 0.003125\n",
      "2022-05-07 01:29:14,484 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 01:29:15,951 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:29:24,267 epoch 181 - iter 37/373 - loss 0.00931224 - samples/sec: 17.80 - lr: 0.003125\n",
      "2022-05-07 01:29:33,623 epoch 181 - iter 74/373 - loss 0.00659283 - samples/sec: 16.71 - lr: 0.003125\n",
      "2022-05-07 01:29:43,417 epoch 181 - iter 111/373 - loss 0.00666955 - samples/sec: 15.93 - lr: 0.003125\n",
      "2022-05-07 01:29:50,960 epoch 181 - iter 148/373 - loss 0.00717749 - samples/sec: 21.00 - lr: 0.003125\n",
      "2022-05-07 01:30:00,521 epoch 181 - iter 185/373 - loss 0.00730990 - samples/sec: 16.40 - lr: 0.003125\n",
      "2022-05-07 01:30:09,837 epoch 181 - iter 222/373 - loss 0.00724906 - samples/sec: 16.81 - lr: 0.003125\n",
      "2022-05-07 01:30:19,413 epoch 181 - iter 259/373 - loss 0.00791546 - samples/sec: 16.33 - lr: 0.003125\n",
      "2022-05-07 01:30:29,538 epoch 181 - iter 296/373 - loss 0.00737121 - samples/sec: 15.42 - lr: 0.003125\n",
      "2022-05-07 01:30:38,109 epoch 181 - iter 333/373 - loss 0.00728731 - samples/sec: 18.41 - lr: 0.003125\n",
      "2022-05-07 01:30:47,644 epoch 181 - iter 370/373 - loss 0.00708776 - samples/sec: 16.41 - lr: 0.003125\n",
      "2022-05-07 01:30:49,026 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:30:49,027 EPOCH 181 done: loss 0.0070 - lr 0.003125\n",
      "2022-05-07 01:30:49,028 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 01:30:50,485 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:30:58,305 epoch 182 - iter 37/373 - loss 0.00882738 - samples/sec: 18.94 - lr: 0.003125\n",
      "2022-05-07 01:31:07,807 epoch 182 - iter 74/373 - loss 0.01041355 - samples/sec: 16.46 - lr: 0.003125\n",
      "2022-05-07 01:31:17,606 epoch 182 - iter 111/373 - loss 0.00906078 - samples/sec: 15.95 - lr: 0.003125\n",
      "2022-05-07 01:31:26,680 epoch 182 - iter 148/373 - loss 0.00875740 - samples/sec: 17.26 - lr: 0.003125\n",
      "2022-05-07 01:31:35,446 epoch 182 - iter 185/373 - loss 0.00935763 - samples/sec: 17.90 - lr: 0.003125\n",
      "2022-05-07 01:31:45,392 epoch 182 - iter 222/373 - loss 0.00913820 - samples/sec: 15.69 - lr: 0.003125\n",
      "2022-05-07 01:31:54,131 epoch 182 - iter 259/373 - loss 0.00861566 - samples/sec: 17.99 - lr: 0.003125\n",
      "2022-05-07 01:32:04,087 epoch 182 - iter 296/373 - loss 0.00835456 - samples/sec: 15.73 - lr: 0.003125\n",
      "2022-05-07 01:32:12,791 epoch 182 - iter 333/373 - loss 0.00801952 - samples/sec: 18.05 - lr: 0.003125\n",
      "2022-05-07 01:32:21,543 epoch 182 - iter 370/373 - loss 0.00805010 - samples/sec: 18.00 - lr: 0.003125\n",
      "2022-05-07 01:32:22,800 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:32:22,801 EPOCH 182 done: loss 0.0080 - lr 0.003125\n",
      "2022-05-07 01:32:22,802 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 01:32:24,259 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:32:31,750 epoch 183 - iter 37/373 - loss 0.00759110 - samples/sec: 19.76 - lr: 0.003125\n",
      "2022-05-07 01:32:41,682 epoch 183 - iter 74/373 - loss 0.00819223 - samples/sec: 15.70 - lr: 0.003125\n",
      "2022-05-07 01:32:52,499 epoch 183 - iter 111/373 - loss 0.00801279 - samples/sec: 14.32 - lr: 0.003125\n",
      "2022-05-07 01:33:01,618 epoch 183 - iter 148/373 - loss 0.00785957 - samples/sec: 17.18 - lr: 0.003125\n",
      "2022-05-07 01:33:10,596 epoch 183 - iter 185/373 - loss 0.00789171 - samples/sec: 17.43 - lr: 0.003125\n",
      "2022-05-07 01:33:19,208 epoch 183 - iter 222/373 - loss 0.00734171 - samples/sec: 18.29 - lr: 0.003125\n",
      "2022-05-07 01:33:29,124 epoch 183 - iter 259/373 - loss 0.00691191 - samples/sec: 15.75 - lr: 0.003125\n",
      "2022-05-07 01:33:36,831 epoch 183 - iter 296/373 - loss 0.00698811 - samples/sec: 20.58 - lr: 0.003125\n",
      "2022-05-07 01:33:45,297 epoch 183 - iter 333/373 - loss 0.00690666 - samples/sec: 18.61 - lr: 0.003125\n",
      "2022-05-07 01:33:54,451 epoch 183 - iter 370/373 - loss 0.00686976 - samples/sec: 17.14 - lr: 0.003125\n",
      "2022-05-07 01:33:55,388 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:33:55,388 EPOCH 183 done: loss 0.0069 - lr 0.003125\n",
      "2022-05-07 01:33:55,389 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 01:33:56,808 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:34:05,361 epoch 184 - iter 37/373 - loss 0.00761260 - samples/sec: 17.31 - lr: 0.003125\n",
      "2022-05-07 01:34:14,112 epoch 184 - iter 74/373 - loss 0.00786090 - samples/sec: 17.96 - lr: 0.003125\n",
      "2022-05-07 01:34:22,498 epoch 184 - iter 111/373 - loss 0.00799759 - samples/sec: 18.79 - lr: 0.003125\n",
      "2022-05-07 01:34:31,826 epoch 184 - iter 148/373 - loss 0.00737308 - samples/sec: 16.75 - lr: 0.003125\n",
      "2022-05-07 01:34:41,307 epoch 184 - iter 185/373 - loss 0.00771677 - samples/sec: 16.46 - lr: 0.003125\n",
      "2022-05-07 01:34:50,263 epoch 184 - iter 222/373 - loss 0.00740722 - samples/sec: 17.49 - lr: 0.003125\n",
      "2022-05-07 01:34:58,988 epoch 184 - iter 259/373 - loss 0.00733511 - samples/sec: 18.09 - lr: 0.003125\n",
      "2022-05-07 01:35:07,803 epoch 184 - iter 296/373 - loss 0.00701457 - samples/sec: 17.84 - lr: 0.003125\n",
      "2022-05-07 01:35:17,728 epoch 184 - iter 333/373 - loss 0.00737661 - samples/sec: 15.72 - lr: 0.003125\n",
      "2022-05-07 01:35:27,241 epoch 184 - iter 370/373 - loss 0.00739654 - samples/sec: 16.46 - lr: 0.003125\n",
      "2022-05-07 01:35:28,491 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:35:28,492 EPOCH 184 done: loss 0.0074 - lr 0.003125\n",
      "2022-05-07 01:35:28,492 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 01:35:29,978 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:35:39,573 epoch 185 - iter 37/373 - loss 0.00661991 - samples/sec: 15.43 - lr: 0.003125\n",
      "2022-05-07 01:35:47,854 epoch 185 - iter 74/373 - loss 0.00586740 - samples/sec: 19.06 - lr: 0.003125\n",
      "2022-05-07 01:35:57,013 epoch 185 - iter 111/373 - loss 0.00805874 - samples/sec: 17.12 - lr: 0.003125\n",
      "2022-05-07 01:36:06,500 epoch 185 - iter 148/373 - loss 0.00787849 - samples/sec: 16.51 - lr: 0.003125\n",
      "2022-05-07 01:36:15,601 epoch 185 - iter 185/373 - loss 0.00753054 - samples/sec: 17.21 - lr: 0.003125\n",
      "2022-05-07 01:36:23,300 epoch 185 - iter 222/373 - loss 0.00743768 - samples/sec: 20.53 - lr: 0.003125\n",
      "2022-05-07 01:36:32,972 epoch 185 - iter 259/373 - loss 0.00734630 - samples/sec: 16.11 - lr: 0.003125\n",
      "2022-05-07 01:36:41,939 epoch 185 - iter 296/373 - loss 0.00691049 - samples/sec: 17.50 - lr: 0.003125\n",
      "2022-05-07 01:36:52,558 epoch 185 - iter 333/373 - loss 0.00672908 - samples/sec: 14.65 - lr: 0.003125\n",
      "2022-05-07 01:37:01,426 epoch 185 - iter 370/373 - loss 0.00669370 - samples/sec: 17.71 - lr: 0.003125\n",
      "2022-05-07 01:37:02,353 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:37:02,354 EPOCH 185 done: loss 0.0067 - lr 0.003125\n",
      "2022-05-07 01:37:02,355 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 01:37:03,798 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:37:12,578 epoch 186 - iter 37/373 - loss 0.01150945 - samples/sec: 16.86 - lr: 0.003125\n",
      "2022-05-07 01:37:20,578 epoch 186 - iter 74/373 - loss 0.01041585 - samples/sec: 19.72 - lr: 0.003125\n",
      "2022-05-07 01:37:29,743 epoch 186 - iter 111/373 - loss 0.01001336 - samples/sec: 17.07 - lr: 0.003125\n",
      "2022-05-07 01:37:38,752 epoch 186 - iter 148/373 - loss 0.01000604 - samples/sec: 17.38 - lr: 0.003125\n",
      "2022-05-07 01:37:47,701 epoch 186 - iter 185/373 - loss 0.00907000 - samples/sec: 17.48 - lr: 0.003125\n",
      "2022-05-07 01:37:57,176 epoch 186 - iter 222/373 - loss 0.00848772 - samples/sec: 16.45 - lr: 0.003125\n",
      "2022-05-07 01:38:06,290 epoch 186 - iter 259/373 - loss 0.00857285 - samples/sec: 17.14 - lr: 0.003125\n",
      "2022-05-07 01:38:15,498 epoch 186 - iter 296/373 - loss 0.00818699 - samples/sec: 17.00 - lr: 0.003125\n",
      "2022-05-07 01:38:24,248 epoch 186 - iter 333/373 - loss 0.00792800 - samples/sec: 17.92 - lr: 0.003125\n",
      "2022-05-07 01:38:33,225 epoch 186 - iter 370/373 - loss 0.00787537 - samples/sec: 17.45 - lr: 0.003125\n",
      "2022-05-07 01:38:34,379 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:38:34,380 EPOCH 186 done: loss 0.0079 - lr 0.003125\n",
      "2022-05-07 01:38:34,381 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 01:38:35,794 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:38:43,426 epoch 187 - iter 37/373 - loss 0.00711418 - samples/sec: 19.40 - lr: 0.003125\n",
      "2022-05-07 01:38:51,146 epoch 187 - iter 74/373 - loss 0.00798544 - samples/sec: 20.48 - lr: 0.003125\n",
      "2022-05-07 01:39:00,584 epoch 187 - iter 111/373 - loss 0.00817242 - samples/sec: 16.53 - lr: 0.003125\n",
      "2022-05-07 01:39:10,132 epoch 187 - iter 148/373 - loss 0.00752676 - samples/sec: 16.42 - lr: 0.003125\n",
      "2022-05-07 01:39:18,690 epoch 187 - iter 185/373 - loss 0.00716488 - samples/sec: 18.36 - lr: 0.003125\n",
      "2022-05-07 01:39:27,484 epoch 187 - iter 222/373 - loss 0.00733808 - samples/sec: 17.83 - lr: 0.003125\n",
      "2022-05-07 01:39:35,973 epoch 187 - iter 259/373 - loss 0.00709629 - samples/sec: 18.49 - lr: 0.003125\n",
      "2022-05-07 01:39:45,067 epoch 187 - iter 296/373 - loss 0.00689620 - samples/sec: 17.19 - lr: 0.003125\n",
      "2022-05-07 01:39:54,383 epoch 187 - iter 333/373 - loss 0.00707811 - samples/sec: 16.77 - lr: 0.003125\n",
      "2022-05-07 01:40:04,526 epoch 187 - iter 370/373 - loss 0.00698853 - samples/sec: 15.33 - lr: 0.003125\n",
      "2022-05-07 01:40:05,790 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:40:05,791 EPOCH 187 done: loss 0.0070 - lr 0.003125\n",
      "2022-05-07 01:40:05,792 BAD EPOCHS (no improvement): 2\n",
      "2022-05-07 01:40:07,353 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:40:15,333 epoch 188 - iter 37/373 - loss 0.00499399 - samples/sec: 18.55 - lr: 0.003125\n",
      "2022-05-07 01:40:24,708 epoch 188 - iter 74/373 - loss 0.00643760 - samples/sec: 16.68 - lr: 0.003125\n",
      "2022-05-07 01:40:34,444 epoch 188 - iter 111/373 - loss 0.00496775 - samples/sec: 16.02 - lr: 0.003125\n",
      "2022-05-07 01:40:44,013 epoch 188 - iter 148/373 - loss 0.00549094 - samples/sec: 16.32 - lr: 0.003125\n",
      "2022-05-07 01:40:52,443 epoch 188 - iter 185/373 - loss 0.00618480 - samples/sec: 18.68 - lr: 0.003125\n",
      "2022-05-07 01:41:00,409 epoch 188 - iter 222/373 - loss 0.00629789 - samples/sec: 19.86 - lr: 0.003125\n",
      "2022-05-07 01:41:09,090 epoch 188 - iter 259/373 - loss 0.00658838 - samples/sec: 18.12 - lr: 0.003125\n",
      "2022-05-07 01:41:18,194 epoch 188 - iter 296/373 - loss 0.00640498 - samples/sec: 17.17 - lr: 0.003125\n",
      "2022-05-07 01:41:26,436 epoch 188 - iter 333/373 - loss 0.00630434 - samples/sec: 19.06 - lr: 0.003125\n",
      "2022-05-07 01:41:36,515 epoch 188 - iter 370/373 - loss 0.00685059 - samples/sec: 15.45 - lr: 0.003125\n",
      "2022-05-07 01:41:37,596 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:41:37,597 EPOCH 188 done: loss 0.0069 - lr 0.003125\n",
      "2022-05-07 01:41:37,597 BAD EPOCHS (no improvement): 3\n",
      "2022-05-07 01:41:39,048 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:41:47,058 epoch 189 - iter 37/373 - loss 0.00560671 - samples/sec: 18.49 - lr: 0.003125\n",
      "2022-05-07 01:41:55,057 epoch 189 - iter 74/373 - loss 0.00774032 - samples/sec: 19.74 - lr: 0.003125\n",
      "2022-05-07 01:42:04,074 epoch 189 - iter 111/373 - loss 0.00757587 - samples/sec: 17.37 - lr: 0.003125\n",
      "2022-05-07 01:42:12,670 epoch 189 - iter 148/373 - loss 0.00703069 - samples/sec: 18.27 - lr: 0.003125\n",
      "2022-05-07 01:42:22,084 epoch 189 - iter 185/373 - loss 0.00775830 - samples/sec: 16.61 - lr: 0.003125\n",
      "2022-05-07 01:42:31,892 epoch 189 - iter 222/373 - loss 0.00742167 - samples/sec: 15.91 - lr: 0.003125\n",
      "2022-05-07 01:42:40,522 epoch 189 - iter 259/373 - loss 0.00728242 - samples/sec: 18.18 - lr: 0.003125\n",
      "2022-05-07 01:42:50,166 epoch 189 - iter 296/373 - loss 0.00717555 - samples/sec: 16.19 - lr: 0.003125\n",
      "2022-05-07 01:42:58,539 epoch 189 - iter 333/373 - loss 0.00696829 - samples/sec: 18.76 - lr: 0.003125\n",
      "2022-05-07 01:43:06,882 epoch 189 - iter 370/373 - loss 0.00670725 - samples/sec: 18.88 - lr: 0.003125\n",
      "2022-05-07 01:43:08,329 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:43:08,330 EPOCH 189 done: loss 0.0067 - lr 0.003125\n",
      "2022-05-07 01:43:08,331 Epoch   189: reducing learning rate of group 0 to 1.5625e-03.\n",
      "2022-05-07 01:43:08,331 BAD EPOCHS (no improvement): 4\n",
      "2022-05-07 01:43:09,768 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:43:18,457 epoch 190 - iter 37/373 - loss 0.00621156 - samples/sec: 17.04 - lr: 0.001563\n",
      "2022-05-07 01:43:28,228 epoch 190 - iter 74/373 - loss 0.00618375 - samples/sec: 15.97 - lr: 0.001563\n",
      "2022-05-07 01:43:37,886 epoch 190 - iter 111/373 - loss 0.00664982 - samples/sec: 16.14 - lr: 0.001563\n",
      "2022-05-07 01:43:46,346 epoch 190 - iter 148/373 - loss 0.00717590 - samples/sec: 18.57 - lr: 0.001563\n",
      "2022-05-07 01:43:55,093 epoch 190 - iter 185/373 - loss 0.00645750 - samples/sec: 17.92 - lr: 0.001563\n",
      "2022-05-07 01:44:03,313 epoch 190 - iter 222/373 - loss 0.00638113 - samples/sec: 19.18 - lr: 0.001563\n",
      "2022-05-07 01:44:13,978 epoch 190 - iter 259/373 - loss 0.00626144 - samples/sec: 14.54 - lr: 0.001563\n",
      "2022-05-07 01:44:24,266 epoch 190 - iter 296/373 - loss 0.00630177 - samples/sec: 15.13 - lr: 0.001563\n",
      "2022-05-07 01:44:32,487 epoch 190 - iter 333/373 - loss 0.00633914 - samples/sec: 19.23 - lr: 0.001563\n",
      "2022-05-07 01:44:41,473 epoch 190 - iter 370/373 - loss 0.00646945 - samples/sec: 17.44 - lr: 0.001563\n",
      "2022-05-07 01:44:42,651 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:44:42,652 EPOCH 190 done: loss 0.0064 - lr 0.001563\n",
      "2022-05-07 01:44:42,653 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 01:44:44,116 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:44:52,997 epoch 191 - iter 37/373 - loss 0.00700341 - samples/sec: 16.67 - lr: 0.001563\n",
      "2022-05-07 01:45:02,443 epoch 191 - iter 74/373 - loss 0.00679550 - samples/sec: 16.55 - lr: 0.001563\n",
      "2022-05-07 01:45:11,358 epoch 191 - iter 111/373 - loss 0.00605936 - samples/sec: 17.64 - lr: 0.001563\n",
      "2022-05-07 01:45:21,186 epoch 191 - iter 148/373 - loss 0.00715938 - samples/sec: 15.90 - lr: 0.001563\n",
      "2022-05-07 01:45:30,397 epoch 191 - iter 185/373 - loss 0.00679751 - samples/sec: 17.01 - lr: 0.001563\n",
      "2022-05-07 01:45:39,405 epoch 191 - iter 222/373 - loss 0.00646135 - samples/sec: 17.47 - lr: 0.001563\n",
      "2022-05-07 01:45:48,143 epoch 191 - iter 259/373 - loss 0.00646219 - samples/sec: 18.03 - lr: 0.001563\n",
      "2022-05-07 01:45:56,966 epoch 191 - iter 296/373 - loss 0.00603481 - samples/sec: 17.83 - lr: 0.001563\n",
      "2022-05-07 01:46:06,153 epoch 191 - iter 333/373 - loss 0.00603876 - samples/sec: 17.09 - lr: 0.001563\n",
      "2022-05-07 01:46:15,315 epoch 191 - iter 370/373 - loss 0.00616860 - samples/sec: 17.10 - lr: 0.001563\n",
      "2022-05-07 01:46:16,326 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:46:16,327 EPOCH 191 done: loss 0.0062 - lr 0.001563\n",
      "2022-05-07 01:46:16,328 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 01:46:17,772 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:46:27,403 epoch 192 - iter 37/373 - loss 0.00688809 - samples/sec: 15.37 - lr: 0.001563\n",
      "2022-05-07 01:46:36,918 epoch 192 - iter 74/373 - loss 0.00701405 - samples/sec: 16.41 - lr: 0.001563\n",
      "2022-05-07 01:46:45,404 epoch 192 - iter 111/373 - loss 0.00690964 - samples/sec: 18.54 - lr: 0.001563\n",
      "2022-05-07 01:46:54,590 epoch 192 - iter 148/373 - loss 0.00671030 - samples/sec: 17.06 - lr: 0.001563\n",
      "2022-05-07 01:47:04,062 epoch 192 - iter 185/373 - loss 0.00737177 - samples/sec: 16.51 - lr: 0.001563\n",
      "2022-05-07 01:47:12,485 epoch 192 - iter 222/373 - loss 0.00699912 - samples/sec: 18.68 - lr: 0.001563\n",
      "2022-05-07 01:47:21,505 epoch 192 - iter 259/373 - loss 0.00678756 - samples/sec: 17.41 - lr: 0.001563\n",
      "2022-05-07 01:47:31,166 epoch 192 - iter 296/373 - loss 0.00690480 - samples/sec: 16.20 - lr: 0.001563\n",
      "2022-05-07 01:47:39,394 epoch 192 - iter 333/373 - loss 0.00662907 - samples/sec: 19.17 - lr: 0.001563\n",
      "2022-05-07 01:47:49,057 epoch 192 - iter 370/373 - loss 0.00651224 - samples/sec: 16.16 - lr: 0.001563\n",
      "2022-05-07 01:47:50,472 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:47:50,473 EPOCH 192 done: loss 0.0065 - lr 0.001563\n",
      "2022-05-07 01:47:50,473 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 01:47:51,926 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:48:01,150 epoch 193 - iter 37/373 - loss 0.00529113 - samples/sec: 16.05 - lr: 0.001563\n",
      "2022-05-07 01:48:10,135 epoch 193 - iter 74/373 - loss 0.00478607 - samples/sec: 17.47 - lr: 0.001563\n",
      "2022-05-07 01:48:20,340 epoch 193 - iter 111/373 - loss 0.00481553 - samples/sec: 15.26 - lr: 0.001563\n",
      "2022-05-07 01:48:29,067 epoch 193 - iter 148/373 - loss 0.00468547 - samples/sec: 18.01 - lr: 0.001563\n",
      "2022-05-07 01:48:37,737 epoch 193 - iter 185/373 - loss 0.00545740 - samples/sec: 18.16 - lr: 0.001563\n",
      "2022-05-07 01:48:47,918 epoch 193 - iter 222/373 - loss 0.00553036 - samples/sec: 15.31 - lr: 0.001563\n",
      "2022-05-07 01:48:56,444 epoch 193 - iter 259/373 - loss 0.00533807 - samples/sec: 18.49 - lr: 0.001563\n",
      "2022-05-07 01:49:05,516 epoch 193 - iter 296/373 - loss 0.00530118 - samples/sec: 17.32 - lr: 0.001563\n",
      "2022-05-07 01:49:15,153 epoch 193 - iter 333/373 - loss 0.00576522 - samples/sec: 16.22 - lr: 0.001563\n",
      "2022-05-07 01:49:23,993 epoch 193 - iter 370/373 - loss 0.00593826 - samples/sec: 17.75 - lr: 0.001563\n",
      "2022-05-07 01:49:25,028 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:49:25,028 EPOCH 193 done: loss 0.0059 - lr 0.001563\n",
      "2022-05-07 01:49:25,029 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 01:49:26,497 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:49:35,077 epoch 194 - iter 37/373 - loss 0.00748197 - samples/sec: 17.26 - lr: 0.001563\n",
      "2022-05-07 01:49:44,189 epoch 194 - iter 74/373 - loss 0.00700523 - samples/sec: 17.17 - lr: 0.001563\n",
      "2022-05-07 01:49:53,184 epoch 194 - iter 111/373 - loss 0.00744334 - samples/sec: 17.44 - lr: 0.001563\n",
      "2022-05-07 01:50:01,483 epoch 194 - iter 148/373 - loss 0.00701241 - samples/sec: 19.01 - lr: 0.001563\n",
      "2022-05-07 01:50:10,271 epoch 194 - iter 185/373 - loss 0.00689405 - samples/sec: 17.87 - lr: 0.001563\n",
      "2022-05-07 01:50:18,685 epoch 194 - iter 222/373 - loss 0.00701669 - samples/sec: 18.69 - lr: 0.001563\n",
      "2022-05-07 01:50:27,788 epoch 194 - iter 259/373 - loss 0.00691685 - samples/sec: 17.22 - lr: 0.001563\n",
      "2022-05-07 01:50:38,311 epoch 194 - iter 296/373 - loss 0.00674059 - samples/sec: 14.79 - lr: 0.001563\n",
      "2022-05-07 01:50:47,675 epoch 194 - iter 333/373 - loss 0.00642887 - samples/sec: 16.74 - lr: 0.001563\n",
      "2022-05-07 01:50:57,232 epoch 194 - iter 370/373 - loss 0.00639034 - samples/sec: 16.35 - lr: 0.001563\n",
      "2022-05-07 01:50:58,710 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:50:58,710 EPOCH 194 done: loss 0.0064 - lr 0.001563\n",
      "2022-05-07 01:50:58,711 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 01:51:00,259 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:51:08,460 epoch 195 - iter 37/373 - loss 0.00712239 - samples/sec: 18.05 - lr: 0.001563\n",
      "2022-05-07 01:51:17,909 epoch 195 - iter 74/373 - loss 0.00637812 - samples/sec: 16.53 - lr: 0.001563\n",
      "2022-05-07 01:51:25,743 epoch 195 - iter 111/373 - loss 0.00577173 - samples/sec: 20.16 - lr: 0.001563\n",
      "2022-05-07 01:51:34,114 epoch 195 - iter 148/373 - loss 0.00610276 - samples/sec: 18.83 - lr: 0.001563\n",
      "2022-05-07 01:51:44,357 epoch 195 - iter 185/373 - loss 0.00572735 - samples/sec: 15.20 - lr: 0.001563\n",
      "2022-05-07 01:51:53,851 epoch 195 - iter 222/373 - loss 0.00575079 - samples/sec: 16.49 - lr: 0.001563\n",
      "2022-05-07 01:52:03,270 epoch 195 - iter 259/373 - loss 0.00596794 - samples/sec: 16.62 - lr: 0.001563\n",
      "2022-05-07 01:52:12,200 epoch 195 - iter 296/373 - loss 0.00638338 - samples/sec: 17.58 - lr: 0.001563\n",
      "2022-05-07 01:52:21,695 epoch 195 - iter 333/373 - loss 0.00609994 - samples/sec: 16.47 - lr: 0.001563\n",
      "2022-05-07 01:52:30,748 epoch 195 - iter 370/373 - loss 0.00635242 - samples/sec: 17.32 - lr: 0.001563\n",
      "2022-05-07 01:52:32,224 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:52:32,225 EPOCH 195 done: loss 0.0063 - lr 0.001563\n",
      "2022-05-07 01:52:32,226 BAD EPOCHS (no improvement): 2\n",
      "2022-05-07 01:52:33,678 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:52:42,378 epoch 196 - iter 37/373 - loss 0.00831330 - samples/sec: 17.02 - lr: 0.001563\n",
      "2022-05-07 01:52:50,790 epoch 196 - iter 74/373 - loss 0.00704236 - samples/sec: 18.72 - lr: 0.001563\n",
      "2022-05-07 01:52:59,111 epoch 196 - iter 111/373 - loss 0.00622176 - samples/sec: 18.91 - lr: 0.001563\n",
      "2022-05-07 01:53:08,482 epoch 196 - iter 148/373 - loss 0.00615620 - samples/sec: 16.66 - lr: 0.001563\n",
      "2022-05-07 01:53:17,481 epoch 196 - iter 185/373 - loss 0.00604233 - samples/sec: 17.44 - lr: 0.001563\n",
      "2022-05-07 01:53:26,782 epoch 196 - iter 222/373 - loss 0.00624002 - samples/sec: 16.86 - lr: 0.001563\n",
      "2022-05-07 01:53:36,190 epoch 196 - iter 259/373 - loss 0.00617076 - samples/sec: 16.63 - lr: 0.001563\n",
      "2022-05-07 01:53:44,896 epoch 196 - iter 296/373 - loss 0.00638061 - samples/sec: 18.05 - lr: 0.001563\n",
      "2022-05-07 01:53:54,674 epoch 196 - iter 333/373 - loss 0.00615657 - samples/sec: 15.98 - lr: 0.001563\n",
      "2022-05-07 01:54:04,164 epoch 196 - iter 370/373 - loss 0.00658587 - samples/sec: 16.50 - lr: 0.001563\n",
      "2022-05-07 01:54:05,516 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:54:05,516 EPOCH 196 done: loss 0.0066 - lr 0.001563\n",
      "2022-05-07 01:54:05,517 BAD EPOCHS (no improvement): 3\n",
      "2022-05-07 01:54:07,152 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:54:16,433 epoch 197 - iter 37/373 - loss 0.00961913 - samples/sec: 15.95 - lr: 0.001563\n",
      "2022-05-07 01:54:25,146 epoch 197 - iter 74/373 - loss 0.00806368 - samples/sec: 17.98 - lr: 0.001563\n",
      "2022-05-07 01:54:33,896 epoch 197 - iter 111/373 - loss 0.00730556 - samples/sec: 17.90 - lr: 0.001563\n",
      "2022-05-07 01:54:43,488 epoch 197 - iter 148/373 - loss 0.00650103 - samples/sec: 16.23 - lr: 0.001563\n",
      "2022-05-07 01:54:51,982 epoch 197 - iter 185/373 - loss 0.00671263 - samples/sec: 18.51 - lr: 0.001563\n",
      "2022-05-07 01:55:01,046 epoch 197 - iter 222/373 - loss 0.00675322 - samples/sec: 17.27 - lr: 0.001563\n",
      "2022-05-07 01:55:09,579 epoch 197 - iter 259/373 - loss 0.00693804 - samples/sec: 18.39 - lr: 0.001563\n",
      "2022-05-07 01:55:18,430 epoch 197 - iter 296/373 - loss 0.00693897 - samples/sec: 17.71 - lr: 0.001563\n",
      "2022-05-07 01:55:28,225 epoch 197 - iter 333/373 - loss 0.00670973 - samples/sec: 15.90 - lr: 0.001563\n",
      "2022-05-07 01:55:36,179 epoch 197 - iter 370/373 - loss 0.00645528 - samples/sec: 19.83 - lr: 0.001563\n",
      "2022-05-07 01:55:37,716 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:55:37,717 EPOCH 197 done: loss 0.0064 - lr 0.001563\n",
      "2022-05-07 01:55:37,717 Epoch   197: reducing learning rate of group 0 to 7.8125e-04.\n",
      "2022-05-07 01:55:37,718 BAD EPOCHS (no improvement): 4\n",
      "2022-05-07 01:55:39,189 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:55:46,969 epoch 198 - iter 37/373 - loss 0.00825761 - samples/sec: 19.03 - lr: 0.000781\n",
      "2022-05-07 01:55:55,921 epoch 198 - iter 74/373 - loss 0.00748326 - samples/sec: 17.49 - lr: 0.000781\n",
      "2022-05-07 01:56:05,625 epoch 198 - iter 111/373 - loss 0.00714966 - samples/sec: 16.04 - lr: 0.000781\n",
      "2022-05-07 01:56:14,602 epoch 198 - iter 148/373 - loss 0.00641796 - samples/sec: 17.43 - lr: 0.000781\n",
      "2022-05-07 01:56:22,698 epoch 198 - iter 185/373 - loss 0.00624414 - samples/sec: 19.43 - lr: 0.000781\n",
      "2022-05-07 01:56:31,349 epoch 198 - iter 222/373 - loss 0.00612815 - samples/sec: 18.16 - lr: 0.000781\n",
      "2022-05-07 01:56:40,134 epoch 198 - iter 259/373 - loss 0.00593478 - samples/sec: 17.84 - lr: 0.000781\n",
      "2022-05-07 01:56:49,151 epoch 198 - iter 296/373 - loss 0.00611184 - samples/sec: 17.37 - lr: 0.000781\n",
      "2022-05-07 01:56:58,434 epoch 198 - iter 333/373 - loss 0.00596642 - samples/sec: 16.85 - lr: 0.000781\n",
      "2022-05-07 01:57:07,331 epoch 198 - iter 370/373 - loss 0.00601675 - samples/sec: 17.61 - lr: 0.000781\n",
      "2022-05-07 01:57:08,298 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:57:08,298 EPOCH 198 done: loss 0.0060 - lr 0.000781\n",
      "2022-05-07 01:57:08,299 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 01:57:09,750 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:57:17,780 epoch 199 - iter 37/373 - loss 0.00625926 - samples/sec: 18.44 - lr: 0.000781\n",
      "2022-05-07 01:57:27,579 epoch 199 - iter 74/373 - loss 0.00541184 - samples/sec: 15.93 - lr: 0.000781\n",
      "2022-05-07 01:57:36,440 epoch 199 - iter 111/373 - loss 0.00633588 - samples/sec: 17.72 - lr: 0.000781\n",
      "2022-05-07 01:57:44,482 epoch 199 - iter 148/373 - loss 0.00627047 - samples/sec: 19.62 - lr: 0.000781\n",
      "2022-05-07 01:57:53,914 epoch 199 - iter 185/373 - loss 0.00652109 - samples/sec: 16.52 - lr: 0.000781\n",
      "2022-05-07 01:58:02,796 epoch 199 - iter 222/373 - loss 0.00624434 - samples/sec: 17.62 - lr: 0.000781\n",
      "2022-05-07 01:58:11,245 epoch 199 - iter 259/373 - loss 0.00662058 - samples/sec: 18.61 - lr: 0.000781\n",
      "2022-05-07 01:58:20,317 epoch 199 - iter 296/373 - loss 0.00650441 - samples/sec: 17.25 - lr: 0.000781\n",
      "2022-05-07 01:58:30,409 epoch 199 - iter 333/373 - loss 0.00633693 - samples/sec: 15.42 - lr: 0.000781\n",
      "2022-05-07 01:58:38,651 epoch 199 - iter 370/373 - loss 0.00661710 - samples/sec: 19.10 - lr: 0.000781\n",
      "2022-05-07 01:58:39,954 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:58:39,954 EPOCH 199 done: loss 0.0066 - lr 0.000781\n",
      "2022-05-07 01:58:39,955 BAD EPOCHS (no improvement): 2\n",
      "2022-05-07 01:58:41,391 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 01:58:50,637 epoch 200 - iter 37/373 - loss 0.00636513 - samples/sec: 16.01 - lr: 0.000781\n",
      "2022-05-07 01:58:59,599 epoch 200 - iter 74/373 - loss 0.00541207 - samples/sec: 17.45 - lr: 0.000781\n",
      "2022-05-07 01:59:07,366 epoch 200 - iter 111/373 - loss 0.00500520 - samples/sec: 20.31 - lr: 0.000781\n",
      "2022-05-07 01:59:17,135 epoch 200 - iter 148/373 - loss 0.00581857 - samples/sec: 15.95 - lr: 0.000781\n",
      "2022-05-07 01:59:25,316 epoch 200 - iter 185/373 - loss 0.00631741 - samples/sec: 19.29 - lr: 0.000781\n",
      "2022-05-07 01:59:35,181 epoch 200 - iter 222/373 - loss 0.00569498 - samples/sec: 15.77 - lr: 0.000781\n",
      "2022-05-07 01:59:43,552 epoch 200 - iter 259/373 - loss 0.00579277 - samples/sec: 18.75 - lr: 0.000781\n",
      "2022-05-07 01:59:51,153 epoch 200 - iter 296/373 - loss 0.00572849 - samples/sec: 20.82 - lr: 0.000781\n",
      "2022-05-07 02:00:00,938 epoch 200 - iter 333/373 - loss 0.00541959 - samples/sec: 15.94 - lr: 0.000781\n",
      "2022-05-07 02:00:10,748 epoch 200 - iter 370/373 - loss 0.00527649 - samples/sec: 15.86 - lr: 0.000781\n",
      "2022-05-07 02:00:12,093 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:00:12,094 EPOCH 200 done: loss 0.0052 - lr 0.000781\n",
      "2022-05-07 02:00:12,095 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 02:00:14,866 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:00:14,868 Testing using last state of model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 135/135 [00:21<00:00,  6.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-07 02:00:36,311 Evaluating as a multi-label problem: False\n",
      "2022-05-07 02:00:36,323 0.702\t0.6803\t0.6909\t0.5307\n",
      "2022-05-07 02:00:36,324 \n",
      "Results:\n",
      "- F-score (micro) 0.6909\n",
      "- F-score (macro) 0.7097\n",
      "- Accuracy 0.5307\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         DIS     0.6943    0.6528    0.6729       769\n",
      "        DRUG     0.7240    0.7702    0.7464       235\n",
      "\n",
      "   micro avg     0.7020    0.6803    0.6909      1004\n",
      "   macro avg     0.7092    0.7115    0.7097      1004\n",
      "weighted avg     0.7013    0.6803    0.6901      1004\n",
      "\n",
      "2022-05-07 02:00:36,325 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:00:36,329 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:00:36,330 WARNING: No LOSS found for test split in this data.\n",
      "2022-05-07 02:00:36,331 Are you sure you want to plot LOSS and not another value?\n",
      "2022-05-07 02:00:36,331 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:00:36,348 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:00:36,349 WARNING: No F1 found for test split in this data.\n",
      "2022-05-07 02:00:36,350 Are you sure you want to plot F1 and not another value?\n",
      "2022-05-07 02:00:36,350 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-07 02:00:36,966 Loss and F1 plots are saved in ..\\resources\\taggers\\FA_MedRed_glove_roberta\\training.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = 'MedRed'\n",
    "selected_embeddings = defaultdict(lambda: 0, {'glove':1, 'roberta':1})\n",
    "train(model, selected_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "389c57cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-07 02:01:58,799 Reading data from ..\\data\\Micromed\n",
      "2022-05-07 02:01:58,800 Train: ..\\data\\Micromed\\NER_Micromed_labels_train.csv\n",
      "2022-05-07 02:01:58,801 Dev: ..\\data\\Micromed\\NER_Micromed_labels_dev.csv\n",
      "2022-05-07 02:01:58,802 Test: ..\\data\\Micromed\\NER_Micromed_labels_test.csv\n",
      "2022-05-07 02:01:58,872 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "133it [00:00, 33258.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-07 02:01:58,879 Dictionary created for label 'ner' with 3 values: DIS (seen 148 times), DRUG (seen 56 times)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-07 02:02:08,035 SequenceTagger predicts: Dictionary with 9 tags: O, S-DIS, B-DIS, E-DIS, I-DIS, S-DRUG, B-DRUG, E-DRUG, I-DRUG\n",
      "2022-05-07 02:02:08,070 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:02:08,073 Model: \"SequenceTagger(\n",
      "  (embeddings): StackedEmbeddings(\n",
      "    (list_embedding_0): WordEmbeddings(\n",
      "      'glove'\n",
      "      (embedding): Embedding(400001, 100)\n",
      "    )\n",
      "    (list_embedding_1): TransformerWordEmbeddings(\n",
      "      (model): RobertaModel(\n",
      "        (embeddings): RobertaEmbeddings(\n",
      "          (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "          (token_type_embeddings): Embedding(1, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): RobertaEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (3): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (4): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (5): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (6): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (7): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (8): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (9): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (10): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (11): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (pooler): RobertaPooler(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (activation): Tanh()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=868, out_features=868, bias=True)\n",
      "  (rnn): LSTM(868, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=11, bias=True)\n",
      "  (loss_function): ViterbiLoss()\n",
      "  (crf): CRF()\n",
      ")\"\n",
      "2022-05-07 02:02:08,075 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:02:08,076 Corpus: \"Corpus: 133 train + 75 dev + 68 test sentences\"\n",
      "2022-05-07 02:02:08,077 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:02:08,078 Parameters:\n",
      "2022-05-07 02:02:08,078  - learning_rate: \"0.100000\"\n",
      "2022-05-07 02:02:08,079  - mini_batch_size: \"4\"\n",
      "2022-05-07 02:02:08,080  - patience: \"3\"\n",
      "2022-05-07 02:02:08,081  - anneal_factor: \"0.5\"\n",
      "2022-05-07 02:02:08,082  - max_epochs: \"200\"\n",
      "2022-05-07 02:02:08,083  - shuffle: \"True\"\n",
      "2022-05-07 02:02:08,084  - train_with_dev: \"True\"\n",
      "2022-05-07 02:02:08,084  - batch_growth_annealing: \"False\"\n",
      "2022-05-07 02:02:08,085 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:02:08,086 Model training base path: \"..\\resources\\taggers\\FA_Micromed_glove_roberta\"\n",
      "2022-05-07 02:02:08,087 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:02:08,088 Device: cuda:0\n",
      "2022-05-07 02:02:08,089 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:02:08,090 Embeddings storage mode: cpu\n",
      "2022-05-07 02:02:08,091 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:02:09,071 epoch 1 - iter 5/52 - loss 0.92052121 - samples/sec: 20.45 - lr: 0.100000\n",
      "2022-05-07 02:02:10,979 epoch 1 - iter 10/52 - loss 0.68668143 - samples/sec: 24.59 - lr: 0.100000\n",
      "2022-05-07 02:02:12,051 epoch 1 - iter 15/52 - loss 0.65360589 - samples/sec: 40.57 - lr: 0.100000\n",
      "2022-05-07 02:02:13,028 epoch 1 - iter 20/52 - loss 0.64293737 - samples/sec: 43.38 - lr: 0.100000\n",
      "2022-05-07 02:02:14,074 epoch 1 - iter 25/52 - loss 0.62244279 - samples/sec: 38.20 - lr: 0.100000\n",
      "2022-05-07 02:02:15,086 epoch 1 - iter 30/52 - loss 0.60687585 - samples/sec: 41.45 - lr: 0.100000\n",
      "2022-05-07 02:02:16,006 epoch 1 - iter 35/52 - loss 0.59340512 - samples/sec: 51.41 - lr: 0.100000\n",
      "2022-05-07 02:02:16,942 epoch 1 - iter 40/52 - loss 0.58154784 - samples/sec: 47.50 - lr: 0.100000\n",
      "2022-05-07 02:02:17,865 epoch 1 - iter 45/52 - loss 0.56372567 - samples/sec: 47.79 - lr: 0.100000\n",
      "2022-05-07 02:02:18,797 epoch 1 - iter 50/52 - loss 0.56071011 - samples/sec: 46.95 - lr: 0.100000\n",
      "2022-05-07 02:02:19,496 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:02:19,497 EPOCH 1 done: loss 0.5512 - lr 0.100000\n",
      "2022-05-07 02:02:19,498 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 02:02:20,936 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:02:21,434 epoch 2 - iter 5/52 - loss 0.40148243 - samples/sec: 40.32 - lr: 0.100000\n",
      "2022-05-07 02:02:22,360 epoch 2 - iter 10/52 - loss 0.38953413 - samples/sec: 47.28 - lr: 0.100000\n",
      "2022-05-07 02:02:23,284 epoch 2 - iter 15/52 - loss 0.36973391 - samples/sec: 47.22 - lr: 0.100000\n",
      "2022-05-07 02:02:24,230 epoch 2 - iter 20/52 - loss 0.38063057 - samples/sec: 45.30 - lr: 0.100000\n",
      "2022-05-07 02:02:25,193 epoch 2 - iter 25/52 - loss 0.35863323 - samples/sec: 45.10 - lr: 0.100000\n",
      "2022-05-07 02:02:26,147 epoch 2 - iter 30/52 - loss 0.37820464 - samples/sec: 44.94 - lr: 0.100000\n",
      "2022-05-07 02:02:27,064 epoch 2 - iter 35/52 - loss 0.39568654 - samples/sec: 49.02 - lr: 0.100000\n",
      "2022-05-07 02:02:27,991 epoch 2 - iter 40/52 - loss 0.37818769 - samples/sec: 48.13 - lr: 0.100000\n",
      "2022-05-07 02:02:28,910 epoch 2 - iter 45/52 - loss 0.38072237 - samples/sec: 46.90 - lr: 0.100000\n",
      "2022-05-07 02:02:29,854 epoch 2 - iter 50/52 - loss 0.37779259 - samples/sec: 45.25 - lr: 0.100000\n",
      "2022-05-07 02:02:30,544 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:02:30,545 EPOCH 2 done: loss 0.3806 - lr 0.100000\n",
      "2022-05-07 02:02:30,546 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 02:02:32,064 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:02:32,561 epoch 3 - iter 5/52 - loss 0.37270319 - samples/sec: 40.45 - lr: 0.100000\n",
      "2022-05-07 02:02:33,495 epoch 3 - iter 10/52 - loss 0.37546123 - samples/sec: 46.90 - lr: 0.100000\n",
      "2022-05-07 02:02:34,418 epoch 3 - iter 15/52 - loss 0.36079048 - samples/sec: 47.06 - lr: 0.100000\n",
      "2022-05-07 02:02:35,392 epoch 3 - iter 20/52 - loss 0.35113296 - samples/sec: 43.43 - lr: 0.100000\n",
      "2022-05-07 02:02:36,409 epoch 3 - iter 25/52 - loss 0.36151398 - samples/sec: 41.54 - lr: 0.100000\n",
      "2022-05-07 02:02:37,362 epoch 3 - iter 30/52 - loss 0.35514217 - samples/sec: 46.84 - lr: 0.100000\n",
      "2022-05-07 02:02:38,287 epoch 3 - iter 35/52 - loss 0.33623604 - samples/sec: 48.33 - lr: 0.100000\n",
      "2022-05-07 02:02:39,192 epoch 3 - iter 40/52 - loss 0.33730898 - samples/sec: 48.54 - lr: 0.100000\n",
      "2022-05-07 02:02:40,092 epoch 3 - iter 45/52 - loss 0.34886410 - samples/sec: 48.96 - lr: 0.100000\n",
      "2022-05-07 02:02:40,996 epoch 3 - iter 50/52 - loss 0.34133790 - samples/sec: 48.84 - lr: 0.100000\n",
      "2022-05-07 02:02:41,657 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:02:41,658 EPOCH 3 done: loss 0.3311 - lr 0.100000\n",
      "2022-05-07 02:02:41,658 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 02:02:43,089 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:02:43,521 epoch 4 - iter 5/52 - loss 0.33106506 - samples/sec: 46.57 - lr: 0.100000\n",
      "2022-05-07 02:02:44,525 epoch 4 - iter 10/52 - loss 0.33896981 - samples/sec: 43.34 - lr: 0.100000\n",
      "2022-05-07 02:02:45,428 epoch 4 - iter 15/52 - loss 0.35852905 - samples/sec: 48.96 - lr: 0.100000\n",
      "2022-05-07 02:02:46,348 epoch 4 - iter 20/52 - loss 0.37735529 - samples/sec: 46.67 - lr: 0.100000\n",
      "2022-05-07 02:02:47,263 epoch 4 - iter 25/52 - loss 0.34977293 - samples/sec: 46.08 - lr: 0.100000\n",
      "2022-05-07 02:02:48,178 epoch 4 - iter 30/52 - loss 0.34035440 - samples/sec: 48.05 - lr: 0.100000\n",
      "2022-05-07 02:02:49,069 epoch 4 - iter 35/52 - loss 0.34671256 - samples/sec: 50.00 - lr: 0.100000\n",
      "2022-05-07 02:02:50,000 epoch 4 - iter 40/52 - loss 0.33149050 - samples/sec: 46.13 - lr: 0.100000\n",
      "2022-05-07 02:02:50,947 epoch 4 - iter 45/52 - loss 0.33620912 - samples/sec: 46.78 - lr: 0.100000\n",
      "2022-05-07 02:02:51,840 epoch 4 - iter 50/52 - loss 0.32710391 - samples/sec: 49.50 - lr: 0.100000\n",
      "2022-05-07 02:02:52,499 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:02:52,500 EPOCH 4 done: loss 0.3279 - lr 0.100000\n",
      "2022-05-07 02:02:52,500 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 02:02:53,946 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:02:54,406 epoch 5 - iter 5/52 - loss 0.36003652 - samples/sec: 43.67 - lr: 0.100000\n",
      "2022-05-07 02:02:55,308 epoch 5 - iter 10/52 - loss 0.31148641 - samples/sec: 50.26 - lr: 0.100000\n",
      "2022-05-07 02:02:56,209 epoch 5 - iter 15/52 - loss 0.34283586 - samples/sec: 49.20 - lr: 0.100000\n",
      "2022-05-07 02:02:57,124 epoch 5 - iter 20/52 - loss 0.31038775 - samples/sec: 46.98 - lr: 0.100000\n",
      "2022-05-07 02:02:58,060 epoch 5 - iter 25/52 - loss 0.30066667 - samples/sec: 45.98 - lr: 0.100000\n",
      "2022-05-07 02:02:58,981 epoch 5 - iter 30/52 - loss 0.30031071 - samples/sec: 46.73 - lr: 0.100000\n",
      "2022-05-07 02:02:59,876 epoch 5 - iter 35/52 - loss 0.30289146 - samples/sec: 50.19 - lr: 0.100000\n",
      "2022-05-07 02:03:00,824 epoch 5 - iter 40/52 - loss 0.30355720 - samples/sec: 47.28 - lr: 0.100000\n",
      "2022-05-07 02:03:01,729 epoch 5 - iter 45/52 - loss 0.29745558 - samples/sec: 48.54 - lr: 0.100000\n",
      "2022-05-07 02:03:02,704 epoch 5 - iter 50/52 - loss 0.29129770 - samples/sec: 45.77 - lr: 0.100000\n",
      "2022-05-07 02:03:03,384 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:03:03,384 EPOCH 5 done: loss 0.3001 - lr 0.100000\n",
      "2022-05-07 02:03:03,385 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 02:03:04,883 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:03:05,341 epoch 6 - iter 5/52 - loss 0.28959711 - samples/sec: 43.96 - lr: 0.100000\n",
      "2022-05-07 02:03:06,253 epoch 6 - iter 10/52 - loss 0.29443279 - samples/sec: 50.44 - lr: 0.100000\n",
      "2022-05-07 02:03:07,181 epoch 6 - iter 15/52 - loss 0.25697953 - samples/sec: 47.45 - lr: 0.100000\n",
      "2022-05-07 02:03:08,105 epoch 6 - iter 20/52 - loss 0.26980881 - samples/sec: 48.25 - lr: 0.100000\n",
      "2022-05-07 02:03:09,037 epoch 6 - iter 25/52 - loss 0.26627968 - samples/sec: 46.08 - lr: 0.100000\n",
      "2022-05-07 02:03:09,993 epoch 6 - iter 30/52 - loss 0.27315890 - samples/sec: 46.08 - lr: 0.100000\n",
      "2022-05-07 02:03:10,938 epoch 6 - iter 35/52 - loss 0.27702379 - samples/sec: 46.73 - lr: 0.100000\n",
      "2022-05-07 02:03:11,897 epoch 6 - iter 40/52 - loss 0.27340960 - samples/sec: 44.05 - lr: 0.100000\n",
      "2022-05-07 02:03:12,880 epoch 6 - iter 45/52 - loss 0.26618430 - samples/sec: 44.25 - lr: 0.100000\n",
      "2022-05-07 02:03:13,836 epoch 6 - iter 50/52 - loss 0.26833111 - samples/sec: 47.00 - lr: 0.100000\n",
      "2022-05-07 02:03:14,511 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:03:14,512 EPOCH 6 done: loss 0.2702 - lr 0.100000\n",
      "2022-05-07 02:03:14,513 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 02:03:15,949 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:03:16,414 epoch 7 - iter 5/52 - loss 0.30188474 - samples/sec: 43.20 - lr: 0.100000\n",
      "2022-05-07 02:03:17,352 epoch 7 - iter 10/52 - loss 0.31125763 - samples/sec: 46.95 - lr: 0.100000\n",
      "2022-05-07 02:03:18,302 epoch 7 - iter 15/52 - loss 0.27670039 - samples/sec: 45.35 - lr: 0.100000\n",
      "2022-05-07 02:03:19,251 epoch 7 - iter 20/52 - loss 0.27465119 - samples/sec: 46.46 - lr: 0.100000\n",
      "2022-05-07 02:03:20,195 epoch 7 - iter 25/52 - loss 0.27172197 - samples/sec: 46.95 - lr: 0.100000\n",
      "2022-05-07 02:03:21,137 epoch 7 - iter 30/52 - loss 0.27248209 - samples/sec: 46.62 - lr: 0.100000\n",
      "2022-05-07 02:03:22,078 epoch 7 - iter 35/52 - loss 0.28115501 - samples/sec: 46.57 - lr: 0.100000\n",
      "2022-05-07 02:03:22,988 epoch 7 - iter 40/52 - loss 0.29096443 - samples/sec: 49.32 - lr: 0.100000\n",
      "2022-05-07 02:03:23,961 epoch 7 - iter 45/52 - loss 0.28144371 - samples/sec: 46.46 - lr: 0.100000\n",
      "2022-05-07 02:03:24,943 epoch 7 - iter 50/52 - loss 0.27660795 - samples/sec: 43.67 - lr: 0.100000\n",
      "2022-05-07 02:03:25,633 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:03:25,633 EPOCH 7 done: loss 0.2842 - lr 0.100000\n",
      "2022-05-07 02:03:25,634 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 02:03:27,052 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:03:27,507 epoch 8 - iter 5/52 - loss 0.22070031 - samples/sec: 44.10 - lr: 0.100000\n",
      "2022-05-07 02:03:28,461 epoch 8 - iter 10/52 - loss 0.25965579 - samples/sec: 44.54 - lr: 0.100000\n",
      "2022-05-07 02:03:29,385 epoch 8 - iter 15/52 - loss 0.25752034 - samples/sec: 48.43 - lr: 0.100000\n",
      "2022-05-07 02:03:30,354 epoch 8 - iter 20/52 - loss 0.24484993 - samples/sec: 45.30 - lr: 0.100000\n",
      "2022-05-07 02:03:31,306 epoch 8 - iter 25/52 - loss 0.26537981 - samples/sec: 45.98 - lr: 0.100000\n",
      "2022-05-07 02:03:32,244 epoch 8 - iter 30/52 - loss 0.26911301 - samples/sec: 46.95 - lr: 0.100000\n",
      "2022-05-07 02:03:33,199 epoch 8 - iter 35/52 - loss 0.26652795 - samples/sec: 45.66 - lr: 0.100000\n",
      "2022-05-07 02:03:34,128 epoch 8 - iter 40/52 - loss 0.26437544 - samples/sec: 46.62 - lr: 0.100000\n",
      "2022-05-07 02:03:35,060 epoch 8 - iter 45/52 - loss 0.25577971 - samples/sec: 45.87 - lr: 0.100000\n",
      "2022-05-07 02:03:35,996 epoch 8 - iter 50/52 - loss 0.26309935 - samples/sec: 47.00 - lr: 0.100000\n",
      "2022-05-07 02:03:36,691 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:03:36,692 EPOCH 8 done: loss 0.2648 - lr 0.100000\n",
      "2022-05-07 02:03:36,693 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 02:03:38,181 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:03:38,654 epoch 9 - iter 5/52 - loss 0.19218754 - samples/sec: 42.46 - lr: 0.100000\n",
      "2022-05-07 02:03:39,605 epoch 9 - iter 10/52 - loss 0.20786292 - samples/sec: 45.05 - lr: 0.100000\n",
      "2022-05-07 02:03:40,547 epoch 9 - iter 15/52 - loss 0.18972002 - samples/sec: 45.56 - lr: 0.100000\n",
      "2022-05-07 02:03:41,473 epoch 9 - iter 20/52 - loss 0.22729821 - samples/sec: 48.72 - lr: 0.100000\n",
      "2022-05-07 02:03:42,381 epoch 9 - iter 25/52 - loss 0.24868605 - samples/sec: 49.26 - lr: 0.100000\n",
      "2022-05-07 02:03:43,334 epoch 9 - iter 30/52 - loss 0.24021991 - samples/sec: 45.40 - lr: 0.100000\n",
      "2022-05-07 02:03:44,269 epoch 9 - iter 35/52 - loss 0.23932738 - samples/sec: 46.89 - lr: 0.100000\n",
      "2022-05-07 02:03:45,194 epoch 9 - iter 40/52 - loss 0.23330838 - samples/sec: 48.19 - lr: 0.100000\n",
      "2022-05-07 02:03:46,180 epoch 9 - iter 45/52 - loss 0.23264850 - samples/sec: 46.45 - lr: 0.100000\n",
      "2022-05-07 02:03:47,142 epoch 9 - iter 50/52 - loss 0.23333834 - samples/sec: 45.77 - lr: 0.100000\n",
      "2022-05-07 02:03:47,822 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:03:47,823 EPOCH 9 done: loss 0.2400 - lr 0.100000\n",
      "2022-05-07 02:03:47,824 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 02:03:49,283 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:03:49,736 epoch 10 - iter 5/52 - loss 0.29282990 - samples/sec: 44.34 - lr: 0.100000\n",
      "2022-05-07 02:03:50,725 epoch 10 - iter 10/52 - loss 0.25514175 - samples/sec: 44.99 - lr: 0.100000\n",
      "2022-05-07 02:03:51,684 epoch 10 - iter 15/52 - loss 0.26917355 - samples/sec: 47.17 - lr: 0.100000\n",
      "2022-05-07 02:03:52,617 epoch 10 - iter 20/52 - loss 0.26688869 - samples/sec: 49.02 - lr: 0.100000\n",
      "2022-05-07 02:03:53,584 epoch 10 - iter 25/52 - loss 0.22743398 - samples/sec: 43.91 - lr: 0.100000\n",
      "2022-05-07 02:03:54,537 epoch 10 - iter 30/52 - loss 0.22064836 - samples/sec: 45.77 - lr: 0.100000\n",
      "2022-05-07 02:03:55,484 epoch 10 - iter 35/52 - loss 0.25186306 - samples/sec: 45.98 - lr: 0.100000\n",
      "2022-05-07 02:03:56,427 epoch 10 - iter 40/52 - loss 0.24761757 - samples/sec: 43.86 - lr: 0.100000\n",
      "2022-05-07 02:03:57,361 epoch 10 - iter 45/52 - loss 0.24882078 - samples/sec: 47.90 - lr: 0.100000\n",
      "2022-05-07 02:03:58,355 epoch 10 - iter 50/52 - loss 0.24390843 - samples/sec: 44.44 - lr: 0.100000\n",
      "2022-05-07 02:03:59,044 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:03:59,045 EPOCH 10 done: loss 0.2427 - lr 0.100000\n",
      "2022-05-07 02:03:59,045 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 02:04:00,534 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:04:00,986 epoch 11 - iter 5/52 - loss 0.25004628 - samples/sec: 44.44 - lr: 0.100000\n",
      "2022-05-07 02:04:01,969 epoch 11 - iter 10/52 - loss 0.28067905 - samples/sec: 43.76 - lr: 0.100000\n",
      "2022-05-07 02:04:02,911 epoch 11 - iter 15/52 - loss 0.27494324 - samples/sec: 45.98 - lr: 0.100000\n",
      "2022-05-07 02:04:03,825 epoch 11 - iter 20/52 - loss 0.25704620 - samples/sec: 48.02 - lr: 0.100000\n",
      "2022-05-07 02:04:04,777 epoch 11 - iter 25/52 - loss 0.24188283 - samples/sec: 45.71 - lr: 0.100000\n",
      "2022-05-07 02:04:05,723 epoch 11 - iter 30/52 - loss 0.23442341 - samples/sec: 46.40 - lr: 0.100000\n",
      "2022-05-07 02:04:06,659 epoch 11 - iter 35/52 - loss 0.23201640 - samples/sec: 47.22 - lr: 0.100000\n",
      "2022-05-07 02:04:07,606 epoch 11 - iter 40/52 - loss 0.23350473 - samples/sec: 45.77 - lr: 0.100000\n",
      "2022-05-07 02:04:08,551 epoch 11 - iter 45/52 - loss 0.23133828 - samples/sec: 45.71 - lr: 0.100000\n",
      "2022-05-07 02:04:09,482 epoch 11 - iter 50/52 - loss 0.22937943 - samples/sec: 48.19 - lr: 0.100000\n",
      "2022-05-07 02:04:10,159 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:04:10,160 EPOCH 11 done: loss 0.2270 - lr 0.100000\n",
      "2022-05-07 02:04:10,161 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 02:04:11,636 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:04:12,124 epoch 12 - iter 5/52 - loss 0.23549017 - samples/sec: 41.19 - lr: 0.100000\n",
      "2022-05-07 02:04:13,055 epoch 12 - iter 10/52 - loss 0.21233178 - samples/sec: 46.67 - lr: 0.100000\n",
      "2022-05-07 02:04:13,982 epoch 12 - iter 15/52 - loss 0.19330371 - samples/sec: 45.77 - lr: 0.100000\n",
      "2022-05-07 02:04:14,920 epoch 12 - iter 20/52 - loss 0.20682156 - samples/sec: 48.02 - lr: 0.100000\n",
      "2022-05-07 02:04:15,888 epoch 12 - iter 25/52 - loss 0.23167884 - samples/sec: 43.86 - lr: 0.100000\n",
      "2022-05-07 02:04:16,833 epoch 12 - iter 30/52 - loss 0.22127700 - samples/sec: 45.51 - lr: 0.100000\n",
      "2022-05-07 02:04:17,742 epoch 12 - iter 35/52 - loss 0.23437943 - samples/sec: 50.31 - lr: 0.100000\n",
      "2022-05-07 02:04:18,690 epoch 12 - iter 40/52 - loss 0.22785525 - samples/sec: 47.62 - lr: 0.100000\n",
      "2022-05-07 02:04:19,593 epoch 12 - iter 45/52 - loss 0.22325619 - samples/sec: 50.76 - lr: 0.100000\n",
      "2022-05-07 02:04:20,532 epoch 12 - iter 50/52 - loss 0.22309723 - samples/sec: 47.28 - lr: 0.100000\n",
      "2022-05-07 02:04:21,204 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:04:21,204 EPOCH 12 done: loss 0.2227 - lr 0.100000\n",
      "2022-05-07 02:04:21,205 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 02:04:22,625 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:04:23,082 epoch 13 - iter 5/52 - loss 0.19000534 - samples/sec: 43.96 - lr: 0.100000\n",
      "2022-05-07 02:04:24,035 epoch 13 - iter 10/52 - loss 0.17847162 - samples/sec: 45.56 - lr: 0.100000\n",
      "2022-05-07 02:04:24,990 epoch 13 - iter 15/52 - loss 0.16321542 - samples/sec: 44.84 - lr: 0.100000\n",
      "2022-05-07 02:04:25,940 epoch 13 - iter 20/52 - loss 0.17114777 - samples/sec: 47.11 - lr: 0.100000\n",
      "2022-05-07 02:04:26,869 epoch 13 - iter 25/52 - loss 0.19048199 - samples/sec: 50.57 - lr: 0.100000\n",
      "2022-05-07 02:04:27,808 epoch 13 - iter 30/52 - loss 0.19790608 - samples/sec: 49.62 - lr: 0.100000\n",
      "2022-05-07 02:04:28,736 epoch 13 - iter 35/52 - loss 0.20728344 - samples/sec: 46.57 - lr: 0.100000\n",
      "2022-05-07 02:04:29,664 epoch 13 - iter 40/52 - loss 0.21225846 - samples/sec: 45.29 - lr: 0.100000\n",
      "2022-05-07 02:04:30,554 epoch 13 - iter 45/52 - loss 0.21581510 - samples/sec: 50.51 - lr: 0.100000\n",
      "2022-05-07 02:04:31,497 epoch 13 - iter 50/52 - loss 0.21516537 - samples/sec: 46.89 - lr: 0.100000\n",
      "2022-05-07 02:04:32,134 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:04:32,135 EPOCH 13 done: loss 0.2166 - lr 0.100000\n",
      "2022-05-07 02:04:32,135 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 02:04:33,560 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:04:33,995 epoch 14 - iter 5/52 - loss 0.14771085 - samples/sec: 46.08 - lr: 0.100000\n",
      "2022-05-07 02:04:34,915 epoch 14 - iter 10/52 - loss 0.15344234 - samples/sec: 48.27 - lr: 0.100000\n",
      "2022-05-07 02:04:35,822 epoch 14 - iter 15/52 - loss 0.13570063 - samples/sec: 48.19 - lr: 0.100000\n",
      "2022-05-07 02:04:36,726 epoch 14 - iter 20/52 - loss 0.16233186 - samples/sec: 47.68 - lr: 0.100000\n",
      "2022-05-07 02:04:37,612 epoch 14 - iter 25/52 - loss 0.17538763 - samples/sec: 49.69 - lr: 0.100000\n",
      "2022-05-07 02:04:38,546 epoch 14 - iter 30/52 - loss 0.17886297 - samples/sec: 45.11 - lr: 0.100000\n",
      "2022-05-07 02:04:39,469 epoch 14 - iter 35/52 - loss 0.17798078 - samples/sec: 47.11 - lr: 0.100000\n",
      "2022-05-07 02:04:40,396 epoch 14 - iter 40/52 - loss 0.18754571 - samples/sec: 45.69 - lr: 0.100000\n",
      "2022-05-07 02:04:41,267 epoch 14 - iter 45/52 - loss 0.19059533 - samples/sec: 50.94 - lr: 0.100000\n",
      "2022-05-07 02:04:42,168 epoch 14 - iter 50/52 - loss 0.19360649 - samples/sec: 48.66 - lr: 0.100000\n",
      "2022-05-07 02:04:42,829 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:04:42,829 EPOCH 14 done: loss 0.1959 - lr 0.100000\n",
      "2022-05-07 02:04:42,830 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 02:04:44,258 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:04:44,696 epoch 15 - iter 5/52 - loss 0.27419795 - samples/sec: 45.92 - lr: 0.100000\n",
      "2022-05-07 02:04:45,704 epoch 15 - iter 10/52 - loss 0.17805441 - samples/sec: 46.19 - lr: 0.100000\n",
      "2022-05-07 02:04:46,659 epoch 15 - iter 15/52 - loss 0.19721391 - samples/sec: 43.91 - lr: 0.100000\n",
      "2022-05-07 02:04:47,613 epoch 15 - iter 20/52 - loss 0.20335499 - samples/sec: 44.69 - lr: 0.100000\n",
      "2022-05-07 02:04:48,557 epoch 15 - iter 25/52 - loss 0.19116760 - samples/sec: 46.24 - lr: 0.100000\n",
      "2022-05-07 02:04:49,510 epoch 15 - iter 30/52 - loss 0.19403255 - samples/sec: 44.30 - lr: 0.100000\n",
      "2022-05-07 02:04:50,466 epoch 15 - iter 35/52 - loss 0.19257841 - samples/sec: 45.05 - lr: 0.100000\n",
      "2022-05-07 02:04:51,393 epoch 15 - iter 40/52 - loss 0.19811295 - samples/sec: 48.60 - lr: 0.100000\n",
      "2022-05-07 02:04:52,344 epoch 15 - iter 45/52 - loss 0.19632348 - samples/sec: 46.19 - lr: 0.100000\n",
      "2022-05-07 02:04:53,287 epoch 15 - iter 50/52 - loss 0.19492477 - samples/sec: 45.82 - lr: 0.100000\n",
      "2022-05-07 02:04:53,986 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:04:53,987 EPOCH 15 done: loss 0.1978 - lr 0.100000\n",
      "2022-05-07 02:04:53,987 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 02:04:55,463 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:04:55,928 epoch 16 - iter 5/52 - loss 0.22078520 - samples/sec: 43.15 - lr: 0.100000\n",
      "2022-05-07 02:04:56,895 epoch 16 - iter 10/52 - loss 0.20043709 - samples/sec: 44.15 - lr: 0.100000\n",
      "2022-05-07 02:04:57,860 epoch 16 - iter 15/52 - loss 0.19247066 - samples/sec: 43.86 - lr: 0.100000\n",
      "2022-05-07 02:04:58,763 epoch 16 - iter 20/52 - loss 0.18917629 - samples/sec: 51.35 - lr: 0.100000\n",
      "2022-05-07 02:04:59,699 epoch 16 - iter 25/52 - loss 0.18794496 - samples/sec: 47.39 - lr: 0.100000\n",
      "2022-05-07 02:05:00,697 epoch 16 - iter 30/52 - loss 0.18473121 - samples/sec: 42.06 - lr: 0.100000\n",
      "2022-05-07 02:05:01,604 epoch 16 - iter 35/52 - loss 0.19632734 - samples/sec: 48.90 - lr: 0.100000\n",
      "2022-05-07 02:05:02,528 epoch 16 - iter 40/52 - loss 0.20205294 - samples/sec: 47.34 - lr: 0.100000\n",
      "2022-05-07 02:05:03,475 epoch 16 - iter 45/52 - loss 0.19912294 - samples/sec: 45.77 - lr: 0.100000\n",
      "2022-05-07 02:05:04,414 epoch 16 - iter 50/52 - loss 0.19763863 - samples/sec: 45.10 - lr: 0.100000\n",
      "2022-05-07 02:05:05,083 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:05:05,084 EPOCH 16 done: loss 0.2009 - lr 0.100000\n",
      "2022-05-07 02:05:05,085 BAD EPOCHS (no improvement): 2\n",
      "2022-05-07 02:05:06,563 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:05:07,003 epoch 17 - iter 5/52 - loss 0.18003435 - samples/sec: 45.56 - lr: 0.100000\n",
      "2022-05-07 02:05:07,967 epoch 17 - iter 10/52 - loss 0.20532151 - samples/sec: 44.05 - lr: 0.100000\n",
      "2022-05-07 02:05:08,917 epoch 17 - iter 15/52 - loss 0.21872645 - samples/sec: 45.82 - lr: 0.100000\n",
      "2022-05-07 02:05:09,853 epoch 17 - iter 20/52 - loss 0.20279562 - samples/sec: 45.77 - lr: 0.100000\n",
      "2022-05-07 02:05:10,804 epoch 17 - iter 25/52 - loss 0.21106015 - samples/sec: 45.40 - lr: 0.100000\n",
      "2022-05-07 02:05:11,737 epoch 17 - iter 30/52 - loss 0.19920898 - samples/sec: 46.78 - lr: 0.100000\n",
      "2022-05-07 02:05:12,666 epoch 17 - iter 35/52 - loss 0.20014386 - samples/sec: 49.38 - lr: 0.100000\n",
      "2022-05-07 02:05:13,613 epoch 17 - iter 40/52 - loss 0.19991861 - samples/sec: 47.45 - lr: 0.100000\n",
      "2022-05-07 02:05:14,529 epoch 17 - iter 45/52 - loss 0.20738897 - samples/sec: 47.28 - lr: 0.100000\n",
      "2022-05-07 02:05:15,470 epoch 17 - iter 50/52 - loss 0.20564954 - samples/sec: 46.73 - lr: 0.100000\n",
      "2022-05-07 02:05:16,134 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:05:16,135 EPOCH 17 done: loss 0.2059 - lr 0.100000\n",
      "2022-05-07 02:05:16,135 BAD EPOCHS (no improvement): 3\n",
      "2022-05-07 02:05:17,701 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:05:18,152 epoch 18 - iter 5/52 - loss 0.20541424 - samples/sec: 44.44 - lr: 0.100000\n",
      "2022-05-07 02:05:19,122 epoch 18 - iter 10/52 - loss 0.17457801 - samples/sec: 44.69 - lr: 0.100000\n",
      "2022-05-07 02:05:20,074 epoch 18 - iter 15/52 - loss 0.18829872 - samples/sec: 45.98 - lr: 0.100000\n",
      "2022-05-07 02:05:21,010 epoch 18 - iter 20/52 - loss 0.19201260 - samples/sec: 48.19 - lr: 0.100000\n",
      "2022-05-07 02:05:21,950 epoch 18 - iter 25/52 - loss 0.20616221 - samples/sec: 47.06 - lr: 0.100000\n",
      "2022-05-07 02:05:22,862 epoch 18 - iter 30/52 - loss 0.21342353 - samples/sec: 51.61 - lr: 0.100000\n",
      "2022-05-07 02:05:23,787 epoch 18 - iter 35/52 - loss 0.20653950 - samples/sec: 47.28 - lr: 0.100000\n",
      "2022-05-07 02:05:24,723 epoch 18 - iter 40/52 - loss 0.20614149 - samples/sec: 45.87 - lr: 0.100000\n",
      "2022-05-07 02:05:25,645 epoch 18 - iter 45/52 - loss 0.20377283 - samples/sec: 48.54 - lr: 0.100000\n",
      "2022-05-07 02:05:26,576 epoch 18 - iter 50/52 - loss 0.19743420 - samples/sec: 45.82 - lr: 0.100000\n",
      "2022-05-07 02:05:27,261 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:05:27,261 EPOCH 18 done: loss 0.1955 - lr 0.100000\n",
      "2022-05-07 02:05:27,262 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 02:05:28,723 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:05:29,179 epoch 19 - iter 5/52 - loss 0.24103195 - samples/sec: 44.05 - lr: 0.100000\n",
      "2022-05-07 02:05:30,115 epoch 19 - iter 10/52 - loss 0.20582702 - samples/sec: 46.67 - lr: 0.100000\n",
      "2022-05-07 02:05:31,048 epoch 19 - iter 15/52 - loss 0.17785854 - samples/sec: 47.17 - lr: 0.100000\n",
      "2022-05-07 02:05:31,961 epoch 19 - iter 20/52 - loss 0.18002374 - samples/sec: 48.43 - lr: 0.100000\n",
      "2022-05-07 02:05:32,890 epoch 19 - iter 25/52 - loss 0.18852583 - samples/sec: 47.85 - lr: 0.100000\n",
      "2022-05-07 02:05:33,823 epoch 19 - iter 30/52 - loss 0.18488972 - samples/sec: 45.20 - lr: 0.100000\n",
      "2022-05-07 02:05:34,752 epoch 19 - iter 35/52 - loss 0.18136061 - samples/sec: 47.00 - lr: 0.100000\n",
      "2022-05-07 02:05:35,711 epoch 19 - iter 40/52 - loss 0.18877056 - samples/sec: 45.40 - lr: 0.100000\n",
      "2022-05-07 02:05:36,669 epoch 19 - iter 45/52 - loss 0.18755615 - samples/sec: 44.59 - lr: 0.100000\n",
      "2022-05-07 02:05:37,631 epoch 19 - iter 50/52 - loss 0.18651896 - samples/sec: 44.49 - lr: 0.100000\n",
      "2022-05-07 02:05:38,305 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:05:38,306 EPOCH 19 done: loss 0.1856 - lr 0.100000\n",
      "2022-05-07 02:05:38,307 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 02:05:39,809 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:05:40,280 epoch 20 - iter 5/52 - loss 0.17871966 - samples/sec: 42.60 - lr: 0.100000\n",
      "2022-05-07 02:05:41,223 epoch 20 - iter 10/52 - loss 0.17045358 - samples/sec: 47.79 - lr: 0.100000\n",
      "2022-05-07 02:05:42,164 epoch 20 - iter 15/52 - loss 0.17646643 - samples/sec: 46.08 - lr: 0.100000\n",
      "2022-05-07 02:05:43,129 epoch 20 - iter 20/52 - loss 0.18129677 - samples/sec: 43.81 - lr: 0.100000\n",
      "2022-05-07 02:05:44,095 epoch 20 - iter 25/52 - loss 0.18370603 - samples/sec: 43.76 - lr: 0.100000\n",
      "2022-05-07 02:05:45,063 epoch 20 - iter 30/52 - loss 0.18250834 - samples/sec: 43.67 - lr: 0.100000\n",
      "2022-05-07 02:05:45,990 epoch 20 - iter 35/52 - loss 0.18234466 - samples/sec: 48.66 - lr: 0.100000\n",
      "2022-05-07 02:05:46,914 epoch 20 - iter 40/52 - loss 0.17571826 - samples/sec: 48.43 - lr: 0.100000\n",
      "2022-05-07 02:05:47,839 epoch 20 - iter 45/52 - loss 0.18176294 - samples/sec: 46.40 - lr: 0.100000\n",
      "2022-05-07 02:05:48,771 epoch 20 - iter 50/52 - loss 0.17716219 - samples/sec: 46.35 - lr: 0.100000\n",
      "2022-05-07 02:05:49,445 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:05:49,446 EPOCH 20 done: loss 0.1816 - lr 0.100000\n",
      "2022-05-07 02:05:49,447 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 02:05:50,898 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:05:51,367 epoch 21 - iter 5/52 - loss 0.21009140 - samples/sec: 42.78 - lr: 0.100000\n",
      "2022-05-07 02:05:52,318 epoch 21 - iter 10/52 - loss 0.17907350 - samples/sec: 46.19 - lr: 0.100000\n",
      "2022-05-07 02:05:53,218 epoch 21 - iter 15/52 - loss 0.17200161 - samples/sec: 51.22 - lr: 0.100000\n",
      "2022-05-07 02:05:54,141 epoch 21 - iter 20/52 - loss 0.17968663 - samples/sec: 49.32 - lr: 0.100000\n",
      "2022-05-07 02:05:55,092 epoch 21 - iter 25/52 - loss 0.17260234 - samples/sec: 46.14 - lr: 0.100000\n",
      "2022-05-07 02:05:56,013 epoch 21 - iter 30/52 - loss 0.17781848 - samples/sec: 49.44 - lr: 0.100000\n",
      "2022-05-07 02:05:56,984 epoch 21 - iter 35/52 - loss 0.16802235 - samples/sec: 44.79 - lr: 0.100000\n",
      "2022-05-07 02:05:57,996 epoch 21 - iter 40/52 - loss 0.16383327 - samples/sec: 42.28 - lr: 0.100000\n",
      "2022-05-07 02:05:58,923 epoch 21 - iter 45/52 - loss 0.16590965 - samples/sec: 47.85 - lr: 0.100000\n",
      "2022-05-07 02:05:59,829 epoch 21 - iter 50/52 - loss 0.17083994 - samples/sec: 49.51 - lr: 0.100000\n",
      "2022-05-07 02:06:00,545 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:06:00,545 EPOCH 21 done: loss 0.1711 - lr 0.100000\n",
      "2022-05-07 02:06:00,546 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 02:06:02,015 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:06:02,445 epoch 22 - iter 5/52 - loss 0.20212463 - samples/sec: 46.67 - lr: 0.100000\n",
      "2022-05-07 02:06:03,383 epoch 22 - iter 10/52 - loss 0.18545985 - samples/sec: 47.17 - lr: 0.100000\n",
      "2022-05-07 02:06:04,327 epoch 22 - iter 15/52 - loss 0.16850898 - samples/sec: 45.97 - lr: 0.100000\n",
      "2022-05-07 02:06:05,228 epoch 22 - iter 20/52 - loss 0.20555796 - samples/sec: 48.78 - lr: 0.100000\n",
      "2022-05-07 02:06:06,125 epoch 22 - iter 25/52 - loss 0.19319087 - samples/sec: 50.76 - lr: 0.100000\n",
      "2022-05-07 02:06:07,053 epoch 22 - iter 30/52 - loss 0.19833823 - samples/sec: 47.50 - lr: 0.100000\n",
      "2022-05-07 02:06:08,001 epoch 22 - iter 35/52 - loss 0.19097019 - samples/sec: 45.14 - lr: 0.100000\n",
      "2022-05-07 02:06:08,946 epoch 22 - iter 40/52 - loss 0.18993342 - samples/sec: 46.95 - lr: 0.100000\n",
      "2022-05-07 02:06:09,882 epoch 22 - iter 45/52 - loss 0.18647987 - samples/sec: 47.62 - lr: 0.100000\n",
      "2022-05-07 02:06:10,797 epoch 22 - iter 50/52 - loss 0.18561710 - samples/sec: 47.61 - lr: 0.100000\n",
      "2022-05-07 02:06:11,455 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:06:11,455 EPOCH 22 done: loss 0.1850 - lr 0.100000\n",
      "2022-05-07 02:06:11,456 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 02:06:12,899 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:06:13,339 epoch 23 - iter 5/52 - loss 0.10608133 - samples/sec: 45.71 - lr: 0.100000\n",
      "2022-05-07 02:06:14,241 epoch 23 - iter 10/52 - loss 0.12511569 - samples/sec: 50.96 - lr: 0.100000\n",
      "2022-05-07 02:06:15,162 epoch 23 - iter 15/52 - loss 0.14956980 - samples/sec: 46.35 - lr: 0.100000\n",
      "2022-05-07 02:06:16,116 epoch 23 - iter 20/52 - loss 0.14649847 - samples/sec: 42.96 - lr: 0.100000\n",
      "2022-05-07 02:06:17,008 epoch 23 - iter 25/52 - loss 0.15053121 - samples/sec: 50.06 - lr: 0.100000\n",
      "2022-05-07 02:06:17,890 epoch 23 - iter 30/52 - loss 0.15645616 - samples/sec: 50.83 - lr: 0.100000\n",
      "2022-05-07 02:06:18,777 epoch 23 - iter 35/52 - loss 0.14620979 - samples/sec: 49.62 - lr: 0.100000\n",
      "2022-05-07 02:06:19,679 epoch 23 - iter 40/52 - loss 0.15164526 - samples/sec: 48.14 - lr: 0.100000\n",
      "2022-05-07 02:06:20,579 epoch 23 - iter 45/52 - loss 0.15907777 - samples/sec: 48.78 - lr: 0.100000\n",
      "2022-05-07 02:06:21,500 epoch 23 - iter 50/52 - loss 0.16150705 - samples/sec: 46.40 - lr: 0.100000\n",
      "2022-05-07 02:06:22,158 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:06:22,158 EPOCH 23 done: loss 0.1618 - lr 0.100000\n",
      "2022-05-07 02:06:22,159 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 02:06:23,578 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:06:24,057 epoch 24 - iter 5/52 - loss 0.15662788 - samples/sec: 42.01 - lr: 0.100000\n",
      "2022-05-07 02:06:24,986 epoch 24 - iter 10/52 - loss 0.18229078 - samples/sec: 47.68 - lr: 0.100000\n",
      "2022-05-07 02:06:25,884 epoch 24 - iter 15/52 - loss 0.16463728 - samples/sec: 49.14 - lr: 0.100000\n",
      "2022-05-07 02:06:26,779 epoch 24 - iter 20/52 - loss 0.16127038 - samples/sec: 48.42 - lr: 0.100000\n",
      "2022-05-07 02:06:27,676 epoch 24 - iter 25/52 - loss 0.17301960 - samples/sec: 50.06 - lr: 0.100000\n",
      "2022-05-07 02:06:28,635 epoch 24 - iter 30/52 - loss 0.17496874 - samples/sec: 46.73 - lr: 0.100000\n",
      "2022-05-07 02:06:29,559 epoch 24 - iter 35/52 - loss 0.17770742 - samples/sec: 47.00 - lr: 0.100000\n",
      "2022-05-07 02:06:30,482 epoch 24 - iter 40/52 - loss 0.18174585 - samples/sec: 47.56 - lr: 0.100000\n",
      "2022-05-07 02:06:31,426 epoch 24 - iter 45/52 - loss 0.17792747 - samples/sec: 46.84 - lr: 0.100000\n",
      "2022-05-07 02:06:32,384 epoch 24 - iter 50/52 - loss 0.18125357 - samples/sec: 44.79 - lr: 0.100000\n",
      "2022-05-07 02:06:33,066 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:06:33,067 EPOCH 24 done: loss 0.1781 - lr 0.100000\n",
      "2022-05-07 02:06:33,067 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 02:06:34,513 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:06:34,969 epoch 25 - iter 5/52 - loss 0.20490790 - samples/sec: 44.00 - lr: 0.100000\n",
      "2022-05-07 02:06:35,907 epoch 25 - iter 10/52 - loss 0.19731153 - samples/sec: 47.73 - lr: 0.100000\n",
      "2022-05-07 02:06:36,864 epoch 25 - iter 15/52 - loss 0.18272969 - samples/sec: 44.94 - lr: 0.100000\n",
      "2022-05-07 02:06:37,792 epoch 25 - iter 20/52 - loss 0.17738483 - samples/sec: 46.78 - lr: 0.100000\n",
      "2022-05-07 02:06:38,710 epoch 25 - iter 25/52 - loss 0.17681058 - samples/sec: 48.48 - lr: 0.100000\n",
      "2022-05-07 02:06:39,625 epoch 25 - iter 30/52 - loss 0.17258923 - samples/sec: 48.72 - lr: 0.100000\n",
      "2022-05-07 02:06:40,553 epoch 25 - iter 35/52 - loss 0.16456948 - samples/sec: 47.56 - lr: 0.100000\n",
      "2022-05-07 02:06:41,526 epoch 25 - iter 40/52 - loss 0.15856322 - samples/sec: 43.06 - lr: 0.100000\n",
      "2022-05-07 02:06:42,681 epoch 25 - iter 45/52 - loss 0.17317306 - samples/sec: 36.93 - lr: 0.100000\n",
      "2022-05-07 02:06:43,652 epoch 25 - iter 50/52 - loss 0.16931335 - samples/sec: 43.76 - lr: 0.100000\n",
      "2022-05-07 02:06:44,323 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:06:44,323 EPOCH 25 done: loss 0.1741 - lr 0.100000\n",
      "2022-05-07 02:06:44,324 BAD EPOCHS (no improvement): 2\n",
      "2022-05-07 02:06:45,747 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:06:46,209 epoch 26 - iter 5/52 - loss 0.20579518 - samples/sec: 43.43 - lr: 0.100000\n",
      "2022-05-07 02:06:47,134 epoch 26 - iter 10/52 - loss 0.17794205 - samples/sec: 47.34 - lr: 0.100000\n",
      "2022-05-07 02:06:48,081 epoch 26 - iter 15/52 - loss 0.16810046 - samples/sec: 45.71 - lr: 0.100000\n",
      "2022-05-07 02:06:49,047 epoch 26 - iter 20/52 - loss 0.17463225 - samples/sec: 43.72 - lr: 0.100000\n",
      "2022-05-07 02:06:50,019 epoch 26 - iter 25/52 - loss 0.17683825 - samples/sec: 45.45 - lr: 0.100000\n",
      "2022-05-07 02:06:50,993 epoch 26 - iter 30/52 - loss 0.17502324 - samples/sec: 45.72 - lr: 0.100000\n",
      "2022-05-07 02:06:51,918 epoch 26 - iter 35/52 - loss 0.17497437 - samples/sec: 47.23 - lr: 0.100000\n",
      "2022-05-07 02:06:52,821 epoch 26 - iter 40/52 - loss 0.17177656 - samples/sec: 48.96 - lr: 0.100000\n",
      "2022-05-07 02:06:53,774 epoch 26 - iter 45/52 - loss 0.17622674 - samples/sec: 48.66 - lr: 0.100000\n",
      "2022-05-07 02:06:54,682 epoch 26 - iter 50/52 - loss 0.17721177 - samples/sec: 49.38 - lr: 0.100000\n",
      "2022-05-07 02:06:55,347 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:06:55,348 EPOCH 26 done: loss 0.1779 - lr 0.100000\n",
      "2022-05-07 02:06:55,348 BAD EPOCHS (no improvement): 3\n",
      "2022-05-07 02:06:56,752 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:06:57,199 epoch 27 - iter 5/52 - loss 0.16917618 - samples/sec: 45.00 - lr: 0.100000\n",
      "2022-05-07 02:06:58,164 epoch 27 - iter 10/52 - loss 0.13287013 - samples/sec: 43.86 - lr: 0.100000\n",
      "2022-05-07 02:06:59,143 epoch 27 - iter 15/52 - loss 0.16040826 - samples/sec: 44.10 - lr: 0.100000\n",
      "2022-05-07 02:07:00,112 epoch 27 - iter 20/52 - loss 0.16764783 - samples/sec: 44.15 - lr: 0.100000\n",
      "2022-05-07 02:07:01,076 epoch 27 - iter 25/52 - loss 0.17011135 - samples/sec: 48.19 - lr: 0.100000\n",
      "2022-05-07 02:07:02,020 epoch 27 - iter 30/52 - loss 0.16471967 - samples/sec: 45.77 - lr: 0.100000\n",
      "2022-05-07 02:07:02,937 epoch 27 - iter 35/52 - loss 0.16298139 - samples/sec: 48.48 - lr: 0.100000\n",
      "2022-05-07 02:07:03,886 epoch 27 - iter 40/52 - loss 0.16078721 - samples/sec: 45.45 - lr: 0.100000\n",
      "2022-05-07 02:07:04,813 epoch 27 - iter 45/52 - loss 0.15883355 - samples/sec: 46.67 - lr: 0.100000\n",
      "2022-05-07 02:07:05,770 epoch 27 - iter 50/52 - loss 0.16258683 - samples/sec: 47.51 - lr: 0.100000\n",
      "2022-05-07 02:07:06,454 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:07:06,455 EPOCH 27 done: loss 0.1611 - lr 0.100000\n",
      "2022-05-07 02:07:06,455 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 02:07:07,889 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:07:08,353 epoch 28 - iter 5/52 - loss 0.17475365 - samples/sec: 43.25 - lr: 0.100000\n",
      "2022-05-07 02:07:09,319 epoch 28 - iter 10/52 - loss 0.15634735 - samples/sec: 47.06 - lr: 0.100000\n",
      "2022-05-07 02:07:10,253 epoch 28 - iter 15/52 - loss 0.14863342 - samples/sec: 47.73 - lr: 0.100000\n",
      "2022-05-07 02:07:11,185 epoch 28 - iter 20/52 - loss 0.14954717 - samples/sec: 47.33 - lr: 0.100000\n",
      "2022-05-07 02:07:12,126 epoch 28 - iter 25/52 - loss 0.14489964 - samples/sec: 45.92 - lr: 0.100000\n",
      "2022-05-07 02:07:13,078 epoch 28 - iter 30/52 - loss 0.15097671 - samples/sec: 44.79 - lr: 0.100000\n",
      "2022-05-07 02:07:14,054 epoch 28 - iter 35/52 - loss 0.15859904 - samples/sec: 42.92 - lr: 0.100000\n",
      "2022-05-07 02:07:14,998 epoch 28 - iter 40/52 - loss 0.15910093 - samples/sec: 47.51 - lr: 0.100000\n",
      "2022-05-07 02:07:15,932 epoch 28 - iter 45/52 - loss 0.15818513 - samples/sec: 48.13 - lr: 0.100000\n",
      "2022-05-07 02:07:16,845 epoch 28 - iter 50/52 - loss 0.15874097 - samples/sec: 49.94 - lr: 0.100000\n",
      "2022-05-07 02:07:17,541 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:07:17,541 EPOCH 28 done: loss 0.1623 - lr 0.100000\n",
      "2022-05-07 02:07:17,542 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 02:07:19,009 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:07:19,533 epoch 29 - iter 5/52 - loss 0.20478280 - samples/sec: 38.35 - lr: 0.100000\n",
      "2022-05-07 02:07:20,479 epoch 29 - iter 10/52 - loss 0.19430750 - samples/sec: 46.84 - lr: 0.100000\n",
      "2022-05-07 02:07:21,424 epoch 29 - iter 15/52 - loss 0.17225805 - samples/sec: 45.56 - lr: 0.100000\n",
      "2022-05-07 02:07:22,362 epoch 29 - iter 20/52 - loss 0.16336483 - samples/sec: 48.60 - lr: 0.100000\n",
      "2022-05-07 02:07:23,302 epoch 29 - iter 25/52 - loss 0.16395642 - samples/sec: 46.84 - lr: 0.100000\n",
      "2022-05-07 02:07:24,248 epoch 29 - iter 30/52 - loss 0.16893105 - samples/sec: 44.44 - lr: 0.100000\n",
      "2022-05-07 02:07:25,207 epoch 29 - iter 35/52 - loss 0.16155189 - samples/sec: 46.67 - lr: 0.100000\n",
      "2022-05-07 02:07:26,147 epoch 29 - iter 40/52 - loss 0.16026629 - samples/sec: 47.79 - lr: 0.100000\n",
      "2022-05-07 02:07:27,087 epoch 29 - iter 45/52 - loss 0.15598549 - samples/sec: 45.45 - lr: 0.100000\n",
      "2022-05-07 02:07:28,064 epoch 29 - iter 50/52 - loss 0.15126143 - samples/sec: 43.67 - lr: 0.100000\n",
      "2022-05-07 02:07:28,740 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:07:28,741 EPOCH 29 done: loss 0.1516 - lr 0.100000\n",
      "2022-05-07 02:07:28,742 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 02:07:30,179 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:07:30,619 epoch 30 - iter 5/52 - loss 0.14986784 - samples/sec: 45.61 - lr: 0.100000\n",
      "2022-05-07 02:07:31,551 epoch 30 - iter 10/52 - loss 0.18447353 - samples/sec: 48.13 - lr: 0.100000\n",
      "2022-05-07 02:07:32,488 epoch 30 - iter 15/52 - loss 0.16301751 - samples/sec: 46.35 - lr: 0.100000\n",
      "2022-05-07 02:07:33,462 epoch 30 - iter 20/52 - loss 0.17743929 - samples/sec: 43.20 - lr: 0.100000\n",
      "2022-05-07 02:07:34,397 epoch 30 - iter 25/52 - loss 0.17467496 - samples/sec: 47.21 - lr: 0.100000\n",
      "2022-05-07 02:07:35,325 epoch 30 - iter 30/52 - loss 0.17665944 - samples/sec: 46.73 - lr: 0.100000\n",
      "2022-05-07 02:07:36,248 epoch 30 - iter 35/52 - loss 0.18079403 - samples/sec: 47.45 - lr: 0.100000\n",
      "2022-05-07 02:07:37,163 epoch 30 - iter 40/52 - loss 0.17717934 - samples/sec: 48.60 - lr: 0.100000\n",
      "2022-05-07 02:07:38,107 epoch 30 - iter 45/52 - loss 0.16462462 - samples/sec: 45.41 - lr: 0.100000\n",
      "2022-05-07 02:07:39,044 epoch 30 - iter 50/52 - loss 0.16562425 - samples/sec: 46.84 - lr: 0.100000\n",
      "2022-05-07 02:07:39,745 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:07:39,746 EPOCH 30 done: loss 0.1664 - lr 0.100000\n",
      "2022-05-07 02:07:39,747 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 02:07:41,201 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:07:41,686 epoch 31 - iter 5/52 - loss 0.11008295 - samples/sec: 41.41 - lr: 0.100000\n",
      "2022-05-07 02:07:42,630 epoch 31 - iter 10/52 - loss 0.11873427 - samples/sec: 46.14 - lr: 0.100000\n",
      "2022-05-07 02:07:43,581 epoch 31 - iter 15/52 - loss 0.13681561 - samples/sec: 46.62 - lr: 0.100000\n",
      "2022-05-07 02:07:44,515 epoch 31 - iter 20/52 - loss 0.12664971 - samples/sec: 45.82 - lr: 0.100000\n",
      "2022-05-07 02:07:45,471 epoch 31 - iter 25/52 - loss 0.14666127 - samples/sec: 44.94 - lr: 0.100000\n",
      "2022-05-07 02:07:46,371 epoch 31 - iter 30/52 - loss 0.15118248 - samples/sec: 48.25 - lr: 0.100000\n",
      "2022-05-07 02:07:47,267 epoch 31 - iter 35/52 - loss 0.14777807 - samples/sec: 49.50 - lr: 0.100000\n",
      "2022-05-07 02:07:48,188 epoch 31 - iter 40/52 - loss 0.15776478 - samples/sec: 48.08 - lr: 0.100000\n",
      "2022-05-07 02:07:49,122 epoch 31 - iter 45/52 - loss 0.15861816 - samples/sec: 45.35 - lr: 0.100000\n",
      "2022-05-07 02:07:50,044 epoch 31 - iter 50/52 - loss 0.15390493 - samples/sec: 48.31 - lr: 0.100000\n",
      "2022-05-07 02:07:50,710 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:07:50,712 EPOCH 31 done: loss 0.1548 - lr 0.100000\n",
      "2022-05-07 02:07:50,712 BAD EPOCHS (no improvement): 2\n",
      "2022-05-07 02:07:52,145 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:07:52,653 epoch 32 - iter 5/52 - loss 0.15162544 - samples/sec: 39.49 - lr: 0.100000\n",
      "2022-05-07 02:07:53,593 epoch 32 - iter 10/52 - loss 0.14241856 - samples/sec: 47.12 - lr: 0.100000\n",
      "2022-05-07 02:07:54,507 epoch 32 - iter 15/52 - loss 0.13645421 - samples/sec: 46.73 - lr: 0.100000\n",
      "2022-05-07 02:07:55,397 epoch 32 - iter 20/52 - loss 0.14971956 - samples/sec: 51.73 - lr: 0.100000\n",
      "2022-05-07 02:07:56,339 epoch 32 - iter 25/52 - loss 0.15383654 - samples/sec: 44.84 - lr: 0.100000\n",
      "2022-05-07 02:07:57,267 epoch 32 - iter 30/52 - loss 0.16057564 - samples/sec: 44.99 - lr: 0.100000\n",
      "2022-05-07 02:07:58,167 epoch 32 - iter 35/52 - loss 0.15022068 - samples/sec: 50.13 - lr: 0.100000\n",
      "2022-05-07 02:07:59,084 epoch 32 - iter 40/52 - loss 0.14506107 - samples/sec: 48.84 - lr: 0.100000\n",
      "2022-05-07 02:07:59,984 epoch 32 - iter 45/52 - loss 0.13815087 - samples/sec: 48.62 - lr: 0.100000\n",
      "2022-05-07 02:08:00,906 epoch 32 - iter 50/52 - loss 0.14873825 - samples/sec: 49.51 - lr: 0.100000\n",
      "2022-05-07 02:08:01,568 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:08:01,569 EPOCH 32 done: loss 0.1498 - lr 0.100000\n",
      "2022-05-07 02:08:01,569 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 02:08:02,974 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:08:03,420 epoch 33 - iter 5/52 - loss 0.17849007 - samples/sec: 44.99 - lr: 0.100000\n",
      "2022-05-07 02:08:04,318 epoch 33 - iter 10/52 - loss 0.18151055 - samples/sec: 49.75 - lr: 0.100000\n",
      "2022-05-07 02:08:05,220 epoch 33 - iter 15/52 - loss 0.16634000 - samples/sec: 48.31 - lr: 0.100000\n",
      "2022-05-07 02:08:06,120 epoch 33 - iter 20/52 - loss 0.16162485 - samples/sec: 48.96 - lr: 0.100000\n",
      "2022-05-07 02:08:07,079 epoch 33 - iter 25/52 - loss 0.15550477 - samples/sec: 45.29 - lr: 0.100000\n",
      "2022-05-07 02:08:07,996 epoch 33 - iter 30/52 - loss 0.16100504 - samples/sec: 47.28 - lr: 0.100000\n",
      "2022-05-07 02:08:08,875 epoch 33 - iter 35/52 - loss 0.15640040 - samples/sec: 50.06 - lr: 0.100000\n",
      "2022-05-07 02:08:09,767 epoch 33 - iter 40/52 - loss 0.15667396 - samples/sec: 49.15 - lr: 0.100000\n",
      "2022-05-07 02:08:10,673 epoch 33 - iter 45/52 - loss 0.16158054 - samples/sec: 47.16 - lr: 0.100000\n",
      "2022-05-07 02:08:11,596 epoch 33 - iter 50/52 - loss 0.16300067 - samples/sec: 47.73 - lr: 0.100000\n",
      "2022-05-07 02:08:12,318 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:08:12,319 EPOCH 33 done: loss 0.1594 - lr 0.100000\n",
      "2022-05-07 02:08:12,319 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 02:08:13,779 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:08:14,245 epoch 34 - iter 5/52 - loss 0.10755513 - samples/sec: 43.15 - lr: 0.100000\n",
      "2022-05-07 02:08:15,175 epoch 34 - iter 10/52 - loss 0.10355937 - samples/sec: 47.79 - lr: 0.100000\n",
      "2022-05-07 02:08:16,103 epoch 34 - iter 15/52 - loss 0.11866443 - samples/sec: 48.78 - lr: 0.100000\n",
      "2022-05-07 02:08:17,015 epoch 34 - iter 20/52 - loss 0.12871599 - samples/sec: 49.81 - lr: 0.100000\n",
      "2022-05-07 02:08:17,958 epoch 34 - iter 25/52 - loss 0.14439844 - samples/sec: 45.56 - lr: 0.100000\n",
      "2022-05-07 02:08:18,913 epoch 34 - iter 30/52 - loss 0.14978874 - samples/sec: 44.99 - lr: 0.100000\n",
      "2022-05-07 02:08:19,841 epoch 34 - iter 35/52 - loss 0.14049438 - samples/sec: 46.03 - lr: 0.100000\n",
      "2022-05-07 02:08:20,763 epoch 34 - iter 40/52 - loss 0.13834640 - samples/sec: 48.60 - lr: 0.100000\n",
      "2022-05-07 02:08:21,694 epoch 34 - iter 45/52 - loss 0.12944886 - samples/sec: 46.89 - lr: 0.100000\n",
      "2022-05-07 02:08:22,638 epoch 34 - iter 50/52 - loss 0.13383125 - samples/sec: 45.35 - lr: 0.100000\n",
      "2022-05-07 02:08:23,312 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:08:23,312 EPOCH 34 done: loss 0.1364 - lr 0.100000\n",
      "2022-05-07 02:08:23,313 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 02:08:24,774 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:08:25,249 epoch 35 - iter 5/52 - loss 0.09890685 - samples/sec: 42.37 - lr: 0.100000\n",
      "2022-05-07 02:08:26,184 epoch 35 - iter 10/52 - loss 0.14536393 - samples/sec: 48.31 - lr: 0.100000\n",
      "2022-05-07 02:08:27,088 epoch 35 - iter 15/52 - loss 0.14573371 - samples/sec: 49.94 - lr: 0.100000\n",
      "2022-05-07 02:08:28,028 epoch 35 - iter 20/52 - loss 0.14724808 - samples/sec: 48.02 - lr: 0.100000\n",
      "2022-05-07 02:08:28,971 epoch 35 - iter 25/52 - loss 0.13400204 - samples/sec: 45.40 - lr: 0.100000\n",
      "2022-05-07 02:08:29,904 epoch 35 - iter 30/52 - loss 0.14439190 - samples/sec: 47.73 - lr: 0.100000\n",
      "2022-05-07 02:08:30,872 epoch 35 - iter 35/52 - loss 0.14342303 - samples/sec: 45.82 - lr: 0.100000\n",
      "2022-05-07 02:08:31,782 epoch 35 - iter 40/52 - loss 0.14383120 - samples/sec: 48.84 - lr: 0.100000\n",
      "2022-05-07 02:08:32,738 epoch 35 - iter 45/52 - loss 0.15003597 - samples/sec: 45.61 - lr: 0.100000\n",
      "2022-05-07 02:08:33,690 epoch 35 - iter 50/52 - loss 0.14650764 - samples/sec: 44.99 - lr: 0.100000\n",
      "2022-05-07 02:08:34,361 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:08:34,362 EPOCH 35 done: loss 0.1460 - lr 0.100000\n",
      "2022-05-07 02:08:34,362 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 02:08:35,793 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:08:36,262 epoch 36 - iter 5/52 - loss 0.10642004 - samples/sec: 42.82 - lr: 0.100000\n",
      "2022-05-07 02:08:37,199 epoch 36 - iter 10/52 - loss 0.12245921 - samples/sec: 46.56 - lr: 0.100000\n",
      "2022-05-07 02:08:38,174 epoch 36 - iter 15/52 - loss 0.10854687 - samples/sec: 44.49 - lr: 0.100000\n",
      "2022-05-07 02:08:39,059 epoch 36 - iter 20/52 - loss 0.12041209 - samples/sec: 51.28 - lr: 0.100000\n",
      "2022-05-07 02:08:39,998 epoch 36 - iter 25/52 - loss 0.12932489 - samples/sec: 46.78 - lr: 0.100000\n",
      "2022-05-07 02:08:40,939 epoch 36 - iter 30/52 - loss 0.13437481 - samples/sec: 45.76 - lr: 0.100000\n",
      "2022-05-07 02:08:41,862 epoch 36 - iter 35/52 - loss 0.13689320 - samples/sec: 49.14 - lr: 0.100000\n",
      "2022-05-07 02:08:42,795 epoch 36 - iter 40/52 - loss 0.14490043 - samples/sec: 46.95 - lr: 0.100000\n",
      "2022-05-07 02:08:43,716 epoch 36 - iter 45/52 - loss 0.14555795 - samples/sec: 47.68 - lr: 0.100000\n",
      "2022-05-07 02:08:44,658 epoch 36 - iter 50/52 - loss 0.14501647 - samples/sec: 45.30 - lr: 0.100000\n",
      "2022-05-07 02:08:45,321 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:08:45,322 EPOCH 36 done: loss 0.1435 - lr 0.100000\n",
      "2022-05-07 02:08:45,323 BAD EPOCHS (no improvement): 2\n",
      "2022-05-07 02:08:46,765 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:08:47,234 epoch 37 - iter 5/52 - loss 0.14718637 - samples/sec: 42.78 - lr: 0.100000\n",
      "2022-05-07 02:08:48,162 epoch 37 - iter 10/52 - loss 0.16196773 - samples/sec: 48.84 - lr: 0.100000\n",
      "2022-05-07 02:08:49,115 epoch 37 - iter 15/52 - loss 0.14703335 - samples/sec: 45.97 - lr: 0.100000\n",
      "2022-05-07 02:08:50,037 epoch 37 - iter 20/52 - loss 0.16079220 - samples/sec: 47.28 - lr: 0.100000\n",
      "2022-05-07 02:08:50,973 epoch 37 - iter 25/52 - loss 0.15410912 - samples/sec: 47.34 - lr: 0.100000\n",
      "2022-05-07 02:08:51,925 epoch 37 - iter 30/52 - loss 0.15055564 - samples/sec: 45.56 - lr: 0.100000\n",
      "2022-05-07 02:08:52,892 epoch 37 - iter 35/52 - loss 0.16568621 - samples/sec: 44.39 - lr: 0.100000\n",
      "2022-05-07 02:08:53,822 epoch 37 - iter 40/52 - loss 0.15524342 - samples/sec: 47.85 - lr: 0.100000\n",
      "2022-05-07 02:08:54,758 epoch 37 - iter 45/52 - loss 0.15150119 - samples/sec: 47.96 - lr: 0.100000\n",
      "2022-05-07 02:08:55,713 epoch 37 - iter 50/52 - loss 0.15319328 - samples/sec: 44.54 - lr: 0.100000\n",
      "2022-05-07 02:08:56,385 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:08:56,386 EPOCH 37 done: loss 0.1526 - lr 0.100000\n",
      "2022-05-07 02:08:56,387 BAD EPOCHS (no improvement): 3\n",
      "2022-05-07 02:08:57,827 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:08:58,316 epoch 38 - iter 5/52 - loss 0.13654939 - samples/sec: 41.02 - lr: 0.100000\n",
      "2022-05-07 02:08:59,257 epoch 38 - iter 10/52 - loss 0.13519318 - samples/sec: 48.54 - lr: 0.100000\n",
      "2022-05-07 02:09:00,229 epoch 38 - iter 15/52 - loss 0.12344598 - samples/sec: 45.15 - lr: 0.100000\n",
      "2022-05-07 02:09:01,196 epoch 38 - iter 20/52 - loss 0.13723574 - samples/sec: 45.82 - lr: 0.100000\n",
      "2022-05-07 02:09:02,118 epoch 38 - iter 25/52 - loss 0.13520865 - samples/sec: 47.51 - lr: 0.100000\n",
      "2022-05-07 02:09:03,049 epoch 38 - iter 30/52 - loss 0.15076364 - samples/sec: 47.00 - lr: 0.100000\n",
      "2022-05-07 02:09:03,983 epoch 38 - iter 35/52 - loss 0.15775432 - samples/sec: 46.95 - lr: 0.100000\n",
      "2022-05-07 02:09:04,942 epoch 38 - iter 40/52 - loss 0.15051640 - samples/sec: 45.71 - lr: 0.100000\n",
      "2022-05-07 02:09:05,855 epoch 38 - iter 45/52 - loss 0.14585292 - samples/sec: 49.87 - lr: 0.100000\n",
      "2022-05-07 02:09:06,789 epoch 38 - iter 50/52 - loss 0.14616806 - samples/sec: 48.31 - lr: 0.100000\n",
      "2022-05-07 02:09:07,453 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:09:07,453 EPOCH 38 done: loss 0.1475 - lr 0.100000\n",
      "2022-05-07 02:09:07,454 Epoch    38: reducing learning rate of group 0 to 5.0000e-02.\n",
      "2022-05-07 02:09:07,455 BAD EPOCHS (no improvement): 4\n",
      "2022-05-07 02:09:08,906 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:09:09,372 epoch 39 - iter 5/52 - loss 0.09765432 - samples/sec: 43.15 - lr: 0.050000\n",
      "2022-05-07 02:09:10,350 epoch 39 - iter 10/52 - loss 0.11692500 - samples/sec: 44.54 - lr: 0.050000\n",
      "2022-05-07 02:09:11,303 epoch 39 - iter 15/52 - loss 0.11699654 - samples/sec: 45.45 - lr: 0.050000\n",
      "2022-05-07 02:09:12,215 epoch 39 - iter 20/52 - loss 0.11388565 - samples/sec: 49.88 - lr: 0.050000\n",
      "2022-05-07 02:09:13,157 epoch 39 - iter 25/52 - loss 0.12272328 - samples/sec: 47.62 - lr: 0.050000\n",
      "2022-05-07 02:09:14,110 epoch 39 - iter 30/52 - loss 0.12280439 - samples/sec: 45.56 - lr: 0.050000\n",
      "2022-05-07 02:09:15,002 epoch 39 - iter 35/52 - loss 0.12771960 - samples/sec: 50.32 - lr: 0.050000\n",
      "2022-05-07 02:09:15,947 epoch 39 - iter 40/52 - loss 0.12321787 - samples/sec: 45.30 - lr: 0.050000\n",
      "2022-05-07 02:09:16,893 epoch 39 - iter 45/52 - loss 0.12221325 - samples/sec: 47.68 - lr: 0.050000\n",
      "2022-05-07 02:09:17,827 epoch 39 - iter 50/52 - loss 0.11873853 - samples/sec: 47.62 - lr: 0.050000\n",
      "2022-05-07 02:09:18,506 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:09:18,507 EPOCH 39 done: loss 0.1197 - lr 0.050000\n",
      "2022-05-07 02:09:18,508 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 02:09:19,955 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:09:20,411 epoch 40 - iter 5/52 - loss 0.07180158 - samples/sec: 44.05 - lr: 0.050000\n",
      "2022-05-07 02:09:21,345 epoch 40 - iter 10/52 - loss 0.12345093 - samples/sec: 48.43 - lr: 0.050000\n",
      "2022-05-07 02:09:22,312 epoch 40 - iter 15/52 - loss 0.11964081 - samples/sec: 45.61 - lr: 0.050000\n",
      "2022-05-07 02:09:23,231 epoch 40 - iter 20/52 - loss 0.11860041 - samples/sec: 49.88 - lr: 0.050000\n",
      "2022-05-07 02:09:24,156 epoch 40 - iter 25/52 - loss 0.12023851 - samples/sec: 47.00 - lr: 0.050000\n",
      "2022-05-07 02:09:25,143 epoch 40 - iter 30/52 - loss 0.12151232 - samples/sec: 46.73 - lr: 0.050000\n",
      "2022-05-07 02:09:26,075 epoch 40 - iter 35/52 - loss 0.12885924 - samples/sec: 47.62 - lr: 0.050000\n",
      "2022-05-07 02:09:27,022 epoch 40 - iter 40/52 - loss 0.12418126 - samples/sec: 45.14 - lr: 0.050000\n",
      "2022-05-07 02:09:27,994 epoch 40 - iter 45/52 - loss 0.12506004 - samples/sec: 46.08 - lr: 0.050000\n",
      "2022-05-07 02:09:28,927 epoch 40 - iter 50/52 - loss 0.12296674 - samples/sec: 46.46 - lr: 0.050000\n",
      "2022-05-07 02:09:29,593 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:09:29,593 EPOCH 40 done: loss 0.1205 - lr 0.050000\n",
      "2022-05-07 02:09:29,594 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 02:09:30,992 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:09:31,455 epoch 41 - iter 5/52 - loss 0.15435115 - samples/sec: 43.34 - lr: 0.050000\n",
      "2022-05-07 02:09:32,342 epoch 41 - iter 10/52 - loss 0.16124933 - samples/sec: 49.32 - lr: 0.050000\n",
      "2022-05-07 02:09:33,276 epoch 41 - iter 15/52 - loss 0.14127292 - samples/sec: 46.35 - lr: 0.050000\n",
      "2022-05-07 02:09:34,199 epoch 41 - iter 20/52 - loss 0.12689020 - samples/sec: 48.49 - lr: 0.050000\n",
      "2022-05-07 02:09:35,139 epoch 41 - iter 25/52 - loss 0.12111539 - samples/sec: 46.19 - lr: 0.050000\n",
      "2022-05-07 02:09:36,099 epoch 41 - iter 30/52 - loss 0.11494390 - samples/sec: 45.87 - lr: 0.050000\n",
      "2022-05-07 02:09:37,065 epoch 41 - iter 35/52 - loss 0.11764593 - samples/sec: 44.30 - lr: 0.050000\n",
      "2022-05-07 02:09:37,964 epoch 41 - iter 40/52 - loss 0.11605244 - samples/sec: 50.51 - lr: 0.050000\n",
      "2022-05-07 02:09:38,885 epoch 41 - iter 45/52 - loss 0.11479957 - samples/sec: 46.03 - lr: 0.050000\n",
      "2022-05-07 02:09:39,781 epoch 41 - iter 50/52 - loss 0.11415098 - samples/sec: 47.90 - lr: 0.050000\n",
      "2022-05-07 02:09:40,434 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:09:40,434 EPOCH 41 done: loss 0.1154 - lr 0.050000\n",
      "2022-05-07 02:09:40,435 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 02:09:41,852 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:09:42,311 epoch 42 - iter 5/52 - loss 0.13961276 - samples/sec: 43.72 - lr: 0.050000\n",
      "2022-05-07 02:09:43,211 epoch 42 - iter 10/52 - loss 0.13713376 - samples/sec: 48.66 - lr: 0.050000\n",
      "2022-05-07 02:09:44,137 epoch 42 - iter 15/52 - loss 0.12350438 - samples/sec: 50.63 - lr: 0.050000\n",
      "2022-05-07 02:09:45,030 epoch 42 - iter 20/52 - loss 0.12972122 - samples/sec: 49.69 - lr: 0.050000\n",
      "2022-05-07 02:09:45,900 epoch 42 - iter 25/52 - loss 0.12803029 - samples/sec: 52.36 - lr: 0.050000\n",
      "2022-05-07 02:09:46,836 epoch 42 - iter 30/52 - loss 0.13236780 - samples/sec: 46.08 - lr: 0.050000\n",
      "2022-05-07 02:09:47,769 epoch 42 - iter 35/52 - loss 0.11901180 - samples/sec: 45.40 - lr: 0.050000\n",
      "2022-05-07 02:09:48,641 epoch 42 - iter 40/52 - loss 0.12407335 - samples/sec: 51.41 - lr: 0.050000\n",
      "2022-05-07 02:09:49,573 epoch 42 - iter 45/52 - loss 0.12459365 - samples/sec: 47.00 - lr: 0.050000\n",
      "2022-05-07 02:09:50,492 epoch 42 - iter 50/52 - loss 0.12294824 - samples/sec: 46.56 - lr: 0.050000\n",
      "2022-05-07 02:09:51,131 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:09:51,132 EPOCH 42 done: loss 0.1225 - lr 0.050000\n",
      "2022-05-07 02:09:51,132 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 02:09:52,565 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:09:52,978 epoch 43 - iter 5/52 - loss 0.07574936 - samples/sec: 48.69 - lr: 0.050000\n",
      "2022-05-07 02:09:53,922 epoch 43 - iter 10/52 - loss 0.08840646 - samples/sec: 45.25 - lr: 0.050000\n",
      "2022-05-07 02:09:54,859 epoch 43 - iter 15/52 - loss 0.08754438 - samples/sec: 44.05 - lr: 0.050000\n",
      "2022-05-07 02:09:55,803 epoch 43 - iter 20/52 - loss 0.10329847 - samples/sec: 45.35 - lr: 0.050000\n",
      "2022-05-07 02:09:56,735 epoch 43 - iter 25/52 - loss 0.10441684 - samples/sec: 47.06 - lr: 0.050000\n",
      "2022-05-07 02:09:57,671 epoch 43 - iter 30/52 - loss 0.10674184 - samples/sec: 46.95 - lr: 0.050000\n",
      "2022-05-07 02:09:58,636 epoch 43 - iter 35/52 - loss 0.11108759 - samples/sec: 43.34 - lr: 0.050000\n",
      "2022-05-07 02:09:59,571 epoch 43 - iter 40/52 - loss 0.10833229 - samples/sec: 46.84 - lr: 0.050000\n",
      "2022-05-07 02:10:00,506 epoch 43 - iter 45/52 - loss 0.11067203 - samples/sec: 46.51 - lr: 0.050000\n",
      "2022-05-07 02:10:01,447 epoch 43 - iter 50/52 - loss 0.10716567 - samples/sec: 45.61 - lr: 0.050000\n",
      "2022-05-07 02:10:02,108 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:10:02,109 EPOCH 43 done: loss 0.1096 - lr 0.050000\n",
      "2022-05-07 02:10:02,109 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 02:10:03,580 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:10:04,012 epoch 44 - iter 5/52 - loss 0.11036880 - samples/sec: 46.57 - lr: 0.050000\n",
      "2022-05-07 02:10:04,960 epoch 44 - iter 10/52 - loss 0.09920414 - samples/sec: 47.22 - lr: 0.050000\n",
      "2022-05-07 02:10:05,892 epoch 44 - iter 15/52 - loss 0.10164665 - samples/sec: 47.51 - lr: 0.050000\n",
      "2022-05-07 02:10:06,861 epoch 44 - iter 20/52 - loss 0.10365395 - samples/sec: 44.20 - lr: 0.050000\n",
      "2022-05-07 02:10:07,815 epoch 44 - iter 25/52 - loss 0.10195905 - samples/sec: 45.51 - lr: 0.050000\n",
      "2022-05-07 02:10:08,739 epoch 44 - iter 30/52 - loss 0.11020751 - samples/sec: 48.54 - lr: 0.050000\n",
      "2022-05-07 02:10:09,686 epoch 44 - iter 35/52 - loss 0.10416383 - samples/sec: 44.69 - lr: 0.050000\n",
      "2022-05-07 02:10:10,640 epoch 44 - iter 40/52 - loss 0.10126467 - samples/sec: 44.39 - lr: 0.050000\n",
      "2022-05-07 02:10:11,567 epoch 44 - iter 45/52 - loss 0.10004423 - samples/sec: 48.02 - lr: 0.050000\n",
      "2022-05-07 02:10:12,480 epoch 44 - iter 50/52 - loss 0.10288546 - samples/sec: 48.78 - lr: 0.050000\n",
      "2022-05-07 02:10:13,189 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:10:13,190 EPOCH 44 done: loss 0.1042 - lr 0.050000\n",
      "2022-05-07 02:10:13,190 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 02:10:14,665 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:10:15,115 epoch 45 - iter 5/52 - loss 0.14409732 - samples/sec: 44.54 - lr: 0.050000\n",
      "2022-05-07 02:10:16,060 epoch 45 - iter 10/52 - loss 0.13227924 - samples/sec: 45.77 - lr: 0.050000\n",
      "2022-05-07 02:10:16,995 epoch 45 - iter 15/52 - loss 0.12587908 - samples/sec: 47.56 - lr: 0.050000\n",
      "2022-05-07 02:10:17,928 epoch 45 - iter 20/52 - loss 0.13210786 - samples/sec: 46.62 - lr: 0.050000\n",
      "2022-05-07 02:10:18,860 epoch 45 - iter 25/52 - loss 0.12117624 - samples/sec: 47.45 - lr: 0.050000\n",
      "2022-05-07 02:10:19,806 epoch 45 - iter 30/52 - loss 0.11907127 - samples/sec: 45.15 - lr: 0.050000\n",
      "2022-05-07 02:10:20,714 epoch 45 - iter 35/52 - loss 0.12078897 - samples/sec: 49.50 - lr: 0.050000\n",
      "2022-05-07 02:10:21,673 epoch 45 - iter 40/52 - loss 0.11684432 - samples/sec: 44.79 - lr: 0.050000\n",
      "2022-05-07 02:10:22,602 epoch 45 - iter 45/52 - loss 0.11661021 - samples/sec: 49.88 - lr: 0.050000\n",
      "2022-05-07 02:10:23,551 epoch 45 - iter 50/52 - loss 0.11257248 - samples/sec: 47.11 - lr: 0.050000\n",
      "2022-05-07 02:10:24,246 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:10:24,246 EPOCH 45 done: loss 0.1122 - lr 0.050000\n",
      "2022-05-07 02:10:24,248 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 02:10:25,744 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:10:26,209 epoch 46 - iter 5/52 - loss 0.13762869 - samples/sec: 43.24 - lr: 0.050000\n",
      "2022-05-07 02:10:27,141 epoch 46 - iter 10/52 - loss 0.11885605 - samples/sec: 47.73 - lr: 0.050000\n",
      "2022-05-07 02:10:28,097 epoch 46 - iter 15/52 - loss 0.13675143 - samples/sec: 44.00 - lr: 0.050000\n",
      "2022-05-07 02:10:29,045 epoch 46 - iter 20/52 - loss 0.14054501 - samples/sec: 47.56 - lr: 0.050000\n",
      "2022-05-07 02:10:30,004 epoch 46 - iter 25/52 - loss 0.13285998 - samples/sec: 46.24 - lr: 0.050000\n",
      "2022-05-07 02:10:30,947 epoch 46 - iter 30/52 - loss 0.12640873 - samples/sec: 46.40 - lr: 0.050000\n",
      "2022-05-07 02:10:31,879 epoch 46 - iter 35/52 - loss 0.12913819 - samples/sec: 48.19 - lr: 0.050000\n",
      "2022-05-07 02:10:32,824 epoch 46 - iter 40/52 - loss 0.12627523 - samples/sec: 48.84 - lr: 0.050000\n",
      "2022-05-07 02:10:33,767 epoch 46 - iter 45/52 - loss 0.12589190 - samples/sec: 46.19 - lr: 0.050000\n",
      "2022-05-07 02:10:34,718 epoch 46 - iter 50/52 - loss 0.11864885 - samples/sec: 44.84 - lr: 0.050000\n",
      "2022-05-07 02:10:35,407 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:10:35,408 EPOCH 46 done: loss 0.1189 - lr 0.050000\n",
      "2022-05-07 02:10:35,408 BAD EPOCHS (no improvement): 2\n",
      "2022-05-07 02:10:36,865 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:10:37,350 epoch 47 - iter 5/52 - loss 0.09135733 - samples/sec: 41.41 - lr: 0.050000\n",
      "2022-05-07 02:10:38,295 epoch 47 - iter 10/52 - loss 0.11564319 - samples/sec: 45.92 - lr: 0.050000\n",
      "2022-05-07 02:10:39,212 epoch 47 - iter 15/52 - loss 0.13689795 - samples/sec: 48.19 - lr: 0.050000\n",
      "2022-05-07 02:10:40,141 epoch 47 - iter 20/52 - loss 0.13438032 - samples/sec: 46.30 - lr: 0.050000\n",
      "2022-05-07 02:10:41,060 epoch 47 - iter 25/52 - loss 0.12919667 - samples/sec: 48.78 - lr: 0.050000\n",
      "2022-05-07 02:10:41,978 epoch 47 - iter 30/52 - loss 0.12085075 - samples/sec: 47.90 - lr: 0.050000\n",
      "2022-05-07 02:10:42,915 epoch 47 - iter 35/52 - loss 0.11870158 - samples/sec: 46.19 - lr: 0.050000\n",
      "2022-05-07 02:10:43,857 epoch 47 - iter 40/52 - loss 0.11738684 - samples/sec: 45.77 - lr: 0.050000\n",
      "2022-05-07 02:10:44,856 epoch 47 - iter 45/52 - loss 0.11344972 - samples/sec: 43.57 - lr: 0.050000\n",
      "2022-05-07 02:10:45,835 epoch 47 - iter 50/52 - loss 0.11689652 - samples/sec: 44.60 - lr: 0.050000\n",
      "2022-05-07 02:10:46,503 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:10:46,504 EPOCH 47 done: loss 0.1137 - lr 0.050000\n",
      "2022-05-07 02:10:46,505 BAD EPOCHS (no improvement): 3\n",
      "2022-05-07 02:10:47,954 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:10:48,412 epoch 48 - iter 5/52 - loss 0.13364066 - samples/sec: 43.95 - lr: 0.050000\n",
      "2022-05-07 02:10:49,425 epoch 48 - iter 10/52 - loss 0.09731459 - samples/sec: 40.90 - lr: 0.050000\n",
      "2022-05-07 02:10:50,374 epoch 48 - iter 15/52 - loss 0.09779033 - samples/sec: 46.08 - lr: 0.050000\n",
      "2022-05-07 02:10:51,303 epoch 48 - iter 20/52 - loss 0.09326045 - samples/sec: 47.96 - lr: 0.050000\n",
      "2022-05-07 02:10:52,263 epoch 48 - iter 25/52 - loss 0.09393587 - samples/sec: 45.30 - lr: 0.050000\n",
      "2022-05-07 02:10:53,255 epoch 48 - iter 30/52 - loss 0.09681848 - samples/sec: 41.58 - lr: 0.050000\n",
      "2022-05-07 02:10:54,184 epoch 48 - iter 35/52 - loss 0.10021256 - samples/sec: 48.25 - lr: 0.050000\n",
      "2022-05-07 02:10:55,101 epoch 48 - iter 40/52 - loss 0.10536707 - samples/sec: 48.90 - lr: 0.050000\n",
      "2022-05-07 02:10:55,999 epoch 48 - iter 45/52 - loss 0.10640810 - samples/sec: 50.19 - lr: 0.050000\n",
      "2022-05-07 02:10:56,921 epoch 48 - iter 50/52 - loss 0.10233754 - samples/sec: 49.02 - lr: 0.050000\n",
      "2022-05-07 02:10:57,631 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:10:57,632 EPOCH 48 done: loss 0.1064 - lr 0.050000\n",
      "2022-05-07 02:10:57,632 Epoch    48: reducing learning rate of group 0 to 2.5000e-02.\n",
      "2022-05-07 02:10:57,633 BAD EPOCHS (no improvement): 4\n",
      "2022-05-07 02:10:59,092 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:10:59,550 epoch 49 - iter 5/52 - loss 0.09714157 - samples/sec: 43.81 - lr: 0.025000\n",
      "2022-05-07 02:11:00,530 epoch 49 - iter 10/52 - loss 0.09833413 - samples/sec: 44.15 - lr: 0.025000\n",
      "2022-05-07 02:11:01,474 epoch 49 - iter 15/52 - loss 0.09371967 - samples/sec: 46.62 - lr: 0.025000\n",
      "2022-05-07 02:11:02,407 epoch 49 - iter 20/52 - loss 0.09399992 - samples/sec: 45.56 - lr: 0.025000\n",
      "2022-05-07 02:11:03,361 epoch 49 - iter 25/52 - loss 0.09067469 - samples/sec: 44.20 - lr: 0.025000\n",
      "2022-05-07 02:11:04,291 epoch 49 - iter 30/52 - loss 0.09301712 - samples/sec: 47.28 - lr: 0.025000\n",
      "2022-05-07 02:11:05,219 epoch 49 - iter 35/52 - loss 0.09369131 - samples/sec: 48.37 - lr: 0.025000\n",
      "2022-05-07 02:11:06,160 epoch 49 - iter 40/52 - loss 0.09678338 - samples/sec: 46.35 - lr: 0.025000\n",
      "2022-05-07 02:11:07,104 epoch 49 - iter 45/52 - loss 0.09062243 - samples/sec: 45.45 - lr: 0.025000\n",
      "2022-05-07 02:11:08,021 epoch 49 - iter 50/52 - loss 0.09226007 - samples/sec: 50.19 - lr: 0.025000\n",
      "2022-05-07 02:11:08,687 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:11:08,688 EPOCH 49 done: loss 0.0939 - lr 0.025000\n",
      "2022-05-07 02:11:08,688 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 02:11:10,129 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:11:10,552 epoch 50 - iter 5/52 - loss 0.12945537 - samples/sec: 47.45 - lr: 0.025000\n",
      "2022-05-07 02:11:11,545 epoch 50 - iter 10/52 - loss 0.09830684 - samples/sec: 44.64 - lr: 0.025000\n",
      "2022-05-07 02:11:12,482 epoch 50 - iter 15/52 - loss 0.11213088 - samples/sec: 46.95 - lr: 0.025000\n",
      "2022-05-07 02:11:13,383 epoch 50 - iter 20/52 - loss 0.10814859 - samples/sec: 49.09 - lr: 0.025000\n",
      "2022-05-07 02:11:14,293 epoch 50 - iter 25/52 - loss 0.10421536 - samples/sec: 46.53 - lr: 0.025000\n",
      "2022-05-07 02:11:15,215 epoch 50 - iter 30/52 - loss 0.10278449 - samples/sec: 46.78 - lr: 0.025000\n",
      "2022-05-07 02:11:16,118 epoch 50 - iter 35/52 - loss 0.10325978 - samples/sec: 48.72 - lr: 0.025000\n",
      "2022-05-07 02:11:17,075 epoch 50 - iter 40/52 - loss 0.10555447 - samples/sec: 45.30 - lr: 0.025000\n",
      "2022-05-07 02:11:18,017 epoch 50 - iter 45/52 - loss 0.10451965 - samples/sec: 44.79 - lr: 0.025000\n",
      "2022-05-07 02:11:18,942 epoch 50 - iter 50/52 - loss 0.10605294 - samples/sec: 49.50 - lr: 0.025000\n",
      "2022-05-07 02:11:19,619 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:11:19,619 EPOCH 50 done: loss 0.1053 - lr 0.025000\n",
      "2022-05-07 02:11:19,620 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 02:11:21,067 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:11:21,529 epoch 51 - iter 5/52 - loss 0.06564021 - samples/sec: 43.52 - lr: 0.025000\n",
      "2022-05-07 02:11:22,427 epoch 51 - iter 10/52 - loss 0.09816183 - samples/sec: 49.62 - lr: 0.025000\n",
      "2022-05-07 02:11:23,329 epoch 51 - iter 15/52 - loss 0.09890149 - samples/sec: 48.31 - lr: 0.025000\n",
      "2022-05-07 02:11:24,209 epoch 51 - iter 20/52 - loss 0.09856139 - samples/sec: 50.89 - lr: 0.025000\n",
      "2022-05-07 02:11:25,119 epoch 51 - iter 25/52 - loss 0.09030036 - samples/sec: 48.19 - lr: 0.025000\n",
      "2022-05-07 02:11:26,032 epoch 51 - iter 30/52 - loss 0.09298407 - samples/sec: 49.81 - lr: 0.025000\n",
      "2022-05-07 02:11:26,929 epoch 51 - iter 35/52 - loss 0.09582085 - samples/sec: 49.88 - lr: 0.025000\n",
      "2022-05-07 02:11:27,838 epoch 51 - iter 40/52 - loss 0.09487907 - samples/sec: 47.34 - lr: 0.025000\n",
      "2022-05-07 02:11:28,755 epoch 51 - iter 45/52 - loss 0.09657347 - samples/sec: 47.27 - lr: 0.025000\n",
      "2022-05-07 02:11:29,675 epoch 51 - iter 50/52 - loss 0.09162072 - samples/sec: 46.73 - lr: 0.025000\n",
      "2022-05-07 02:11:30,345 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:11:30,345 EPOCH 51 done: loss 0.0923 - lr 0.025000\n",
      "2022-05-07 02:11:30,346 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 02:11:31,776 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:11:32,222 epoch 52 - iter 5/52 - loss 0.09195038 - samples/sec: 45.05 - lr: 0.025000\n",
      "2022-05-07 02:11:33,155 epoch 52 - iter 10/52 - loss 0.11126320 - samples/sec: 46.89 - lr: 0.025000\n",
      "2022-05-07 02:11:34,048 epoch 52 - iter 15/52 - loss 0.10017664 - samples/sec: 48.97 - lr: 0.025000\n",
      "2022-05-07 02:11:34,949 epoch 52 - iter 20/52 - loss 0.10214043 - samples/sec: 49.57 - lr: 0.025000\n",
      "2022-05-07 02:11:35,824 epoch 52 - iter 25/52 - loss 0.10160824 - samples/sec: 51.95 - lr: 0.025000\n",
      "2022-05-07 02:11:36,738 epoch 52 - iter 30/52 - loss 0.09683754 - samples/sec: 47.43 - lr: 0.025000\n",
      "2022-05-07 02:11:37,665 epoch 52 - iter 35/52 - loss 0.09730216 - samples/sec: 46.45 - lr: 0.025000\n",
      "2022-05-07 02:11:38,584 epoch 52 - iter 40/52 - loss 0.09639447 - samples/sec: 46.73 - lr: 0.025000\n",
      "2022-05-07 02:11:39,516 epoch 52 - iter 45/52 - loss 0.09830734 - samples/sec: 43.67 - lr: 0.025000\n",
      "2022-05-07 02:11:40,437 epoch 52 - iter 50/52 - loss 0.09856024 - samples/sec: 47.28 - lr: 0.025000\n",
      "2022-05-07 02:11:41,110 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:11:41,111 EPOCH 52 done: loss 0.0984 - lr 0.025000\n",
      "2022-05-07 02:11:41,111 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 02:11:42,571 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:11:43,020 epoch 53 - iter 5/52 - loss 0.08131498 - samples/sec: 44.79 - lr: 0.025000\n",
      "2022-05-07 02:11:43,949 epoch 53 - iter 10/52 - loss 0.07069635 - samples/sec: 47.68 - lr: 0.025000\n",
      "2022-05-07 02:11:44,894 epoch 53 - iter 15/52 - loss 0.07416249 - samples/sec: 46.68 - lr: 0.025000\n",
      "2022-05-07 02:11:45,842 epoch 53 - iter 20/52 - loss 0.08165077 - samples/sec: 45.82 - lr: 0.025000\n",
      "2022-05-07 02:11:46,770 epoch 53 - iter 25/52 - loss 0.08531212 - samples/sec: 47.51 - lr: 0.025000\n",
      "2022-05-07 02:11:47,738 epoch 53 - iter 30/52 - loss 0.08732433 - samples/sec: 44.74 - lr: 0.025000\n",
      "2022-05-07 02:11:48,692 epoch 53 - iter 35/52 - loss 0.08919780 - samples/sec: 46.30 - lr: 0.025000\n",
      "2022-05-07 02:11:49,637 epoch 53 - iter 40/52 - loss 0.08644204 - samples/sec: 46.57 - lr: 0.025000\n",
      "2022-05-07 02:11:50,553 epoch 53 - iter 45/52 - loss 0.08953910 - samples/sec: 48.19 - lr: 0.025000\n",
      "2022-05-07 02:11:51,461 epoch 53 - iter 50/52 - loss 0.09405441 - samples/sec: 49.02 - lr: 0.025000\n",
      "2022-05-07 02:11:52,152 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:11:52,153 EPOCH 53 done: loss 0.0951 - lr 0.025000\n",
      "2022-05-07 02:11:52,153 BAD EPOCHS (no improvement): 2\n",
      "2022-05-07 02:11:53,644 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:11:54,102 epoch 54 - iter 5/52 - loss 0.08648147 - samples/sec: 43.81 - lr: 0.025000\n",
      "2022-05-07 02:11:55,063 epoch 54 - iter 10/52 - loss 0.08069483 - samples/sec: 46.51 - lr: 0.025000\n",
      "2022-05-07 02:11:56,019 epoch 54 - iter 15/52 - loss 0.07285446 - samples/sec: 45.05 - lr: 0.025000\n",
      "2022-05-07 02:11:56,964 epoch 54 - iter 20/52 - loss 0.08161110 - samples/sec: 46.24 - lr: 0.025000\n",
      "2022-05-07 02:11:57,875 epoch 54 - iter 25/52 - loss 0.08224062 - samples/sec: 49.63 - lr: 0.025000\n",
      "2022-05-07 02:11:58,820 epoch 54 - iter 30/52 - loss 0.09276581 - samples/sec: 45.82 - lr: 0.025000\n",
      "2022-05-07 02:11:59,748 epoch 54 - iter 35/52 - loss 0.09320550 - samples/sec: 46.84 - lr: 0.025000\n",
      "2022-05-07 02:12:00,739 epoch 54 - iter 40/52 - loss 0.09488835 - samples/sec: 41.84 - lr: 0.025000\n",
      "2022-05-07 02:12:01,658 epoch 54 - iter 45/52 - loss 0.09467324 - samples/sec: 48.72 - lr: 0.025000\n",
      "2022-05-07 02:12:02,598 epoch 54 - iter 50/52 - loss 0.09966804 - samples/sec: 44.69 - lr: 0.025000\n",
      "2022-05-07 02:12:03,265 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:12:03,266 EPOCH 54 done: loss 0.0980 - lr 0.025000\n",
      "2022-05-07 02:12:03,267 BAD EPOCHS (no improvement): 3\n",
      "2022-05-07 02:12:04,766 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:12:05,203 epoch 55 - iter 5/52 - loss 0.09292794 - samples/sec: 45.98 - lr: 0.025000\n",
      "2022-05-07 02:12:06,142 epoch 55 - iter 10/52 - loss 0.11070666 - samples/sec: 46.67 - lr: 0.025000\n",
      "2022-05-07 02:12:07,081 epoch 55 - iter 15/52 - loss 0.11065040 - samples/sec: 48.43 - lr: 0.025000\n",
      "2022-05-07 02:12:08,040 epoch 55 - iter 20/52 - loss 0.10311382 - samples/sec: 44.74 - lr: 0.025000\n",
      "2022-05-07 02:12:08,967 epoch 55 - iter 25/52 - loss 0.09940543 - samples/sec: 48.37 - lr: 0.025000\n",
      "2022-05-07 02:12:09,897 epoch 55 - iter 30/52 - loss 0.10196087 - samples/sec: 46.62 - lr: 0.025000\n",
      "2022-05-07 02:12:10,829 epoch 55 - iter 35/52 - loss 0.10234060 - samples/sec: 46.62 - lr: 0.025000\n",
      "2022-05-07 02:12:11,793 epoch 55 - iter 40/52 - loss 0.10345675 - samples/sec: 45.82 - lr: 0.025000\n",
      "2022-05-07 02:12:12,764 epoch 55 - iter 45/52 - loss 0.10425708 - samples/sec: 43.91 - lr: 0.025000\n",
      "2022-05-07 02:12:13,730 epoch 55 - iter 50/52 - loss 0.11064914 - samples/sec: 44.00 - lr: 0.025000\n",
      "2022-05-07 02:12:14,404 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:12:14,405 EPOCH 55 done: loss 0.1090 - lr 0.025000\n",
      "2022-05-07 02:12:14,406 Epoch    55: reducing learning rate of group 0 to 1.2500e-02.\n",
      "2022-05-07 02:12:14,406 BAD EPOCHS (no improvement): 4\n",
      "2022-05-07 02:12:15,851 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:12:16,315 epoch 56 - iter 5/52 - loss 0.13792898 - samples/sec: 43.24 - lr: 0.012500\n",
      "2022-05-07 02:12:17,250 epoch 56 - iter 10/52 - loss 0.10999436 - samples/sec: 47.56 - lr: 0.012500\n",
      "2022-05-07 02:12:18,241 epoch 56 - iter 15/52 - loss 0.09525501 - samples/sec: 43.19 - lr: 0.012500\n",
      "2022-05-07 02:12:19,169 epoch 56 - iter 20/52 - loss 0.08508902 - samples/sec: 47.45 - lr: 0.012500\n",
      "2022-05-07 02:12:20,097 epoch 56 - iter 25/52 - loss 0.08519271 - samples/sec: 48.72 - lr: 0.012500\n",
      "2022-05-07 02:12:21,022 epoch 56 - iter 30/52 - loss 0.08949093 - samples/sec: 47.56 - lr: 0.012500\n",
      "2022-05-07 02:12:21,977 epoch 56 - iter 35/52 - loss 0.09614606 - samples/sec: 46.14 - lr: 0.012500\n",
      "2022-05-07 02:12:22,918 epoch 56 - iter 40/52 - loss 0.09813595 - samples/sec: 47.28 - lr: 0.012500\n",
      "2022-05-07 02:12:23,860 epoch 56 - iter 45/52 - loss 0.09364488 - samples/sec: 48.19 - lr: 0.012500\n",
      "2022-05-07 02:12:24,789 epoch 56 - iter 50/52 - loss 0.09479049 - samples/sec: 47.00 - lr: 0.012500\n",
      "2022-05-07 02:12:25,462 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:12:25,463 EPOCH 56 done: loss 0.0929 - lr 0.012500\n",
      "2022-05-07 02:12:25,464 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 02:12:26,876 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:12:27,390 epoch 57 - iter 5/52 - loss 0.09699922 - samples/sec: 39.10 - lr: 0.012500\n",
      "2022-05-07 02:12:28,318 epoch 57 - iter 10/52 - loss 0.08556530 - samples/sec: 47.17 - lr: 0.012500\n",
      "2022-05-07 02:12:29,265 epoch 57 - iter 15/52 - loss 0.08144833 - samples/sec: 48.66 - lr: 0.012500\n",
      "2022-05-07 02:12:30,192 epoch 57 - iter 20/52 - loss 0.08277071 - samples/sec: 48.66 - lr: 0.012500\n",
      "2022-05-07 02:12:31,140 epoch 57 - iter 25/52 - loss 0.08292751 - samples/sec: 45.35 - lr: 0.012500\n",
      "2022-05-07 02:12:32,082 epoch 57 - iter 30/52 - loss 0.08153470 - samples/sec: 45.30 - lr: 0.012500\n",
      "2022-05-07 02:12:33,035 epoch 57 - iter 35/52 - loss 0.07745736 - samples/sec: 44.44 - lr: 0.012500\n",
      "2022-05-07 02:12:33,961 epoch 57 - iter 40/52 - loss 0.08348675 - samples/sec: 48.25 - lr: 0.012500\n",
      "2022-05-07 02:12:34,919 epoch 57 - iter 45/52 - loss 0.08478456 - samples/sec: 44.69 - lr: 0.012500\n",
      "2022-05-07 02:12:35,864 epoch 57 - iter 50/52 - loss 0.08895645 - samples/sec: 46.30 - lr: 0.012500\n",
      "2022-05-07 02:12:36,549 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:12:36,550 EPOCH 57 done: loss 0.0925 - lr 0.012500\n",
      "2022-05-07 02:12:36,551 BAD EPOCHS (no improvement): 2\n",
      "2022-05-07 02:12:38,094 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:12:38,614 epoch 58 - iter 5/52 - loss 0.07020007 - samples/sec: 38.57 - lr: 0.012500\n",
      "2022-05-07 02:12:39,567 epoch 58 - iter 10/52 - loss 0.07771572 - samples/sec: 45.72 - lr: 0.012500\n",
      "2022-05-07 02:12:40,488 epoch 58 - iter 15/52 - loss 0.08337550 - samples/sec: 48.19 - lr: 0.012500\n",
      "2022-05-07 02:12:41,405 epoch 58 - iter 20/52 - loss 0.08278722 - samples/sec: 48.72 - lr: 0.012500\n",
      "2022-05-07 02:12:42,368 epoch 58 - iter 25/52 - loss 0.07703598 - samples/sec: 44.79 - lr: 0.012500\n",
      "2022-05-07 02:12:43,307 epoch 58 - iter 30/52 - loss 0.07779825 - samples/sec: 45.30 - lr: 0.012500\n",
      "2022-05-07 02:12:44,218 epoch 58 - iter 35/52 - loss 0.08201942 - samples/sec: 48.96 - lr: 0.012500\n",
      "2022-05-07 02:12:45,140 epoch 58 - iter 40/52 - loss 0.08638520 - samples/sec: 46.89 - lr: 0.012500\n",
      "2022-05-07 02:12:46,057 epoch 58 - iter 45/52 - loss 0.08739492 - samples/sec: 46.89 - lr: 0.012500\n",
      "2022-05-07 02:12:46,953 epoch 58 - iter 50/52 - loss 0.08642771 - samples/sec: 49.08 - lr: 0.012500\n",
      "2022-05-07 02:12:47,613 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:12:47,614 EPOCH 58 done: loss 0.0861 - lr 0.012500\n",
      "2022-05-07 02:12:47,614 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 02:12:49,069 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:12:49,507 epoch 59 - iter 5/52 - loss 0.09273238 - samples/sec: 46.03 - lr: 0.012500\n",
      "2022-05-07 02:12:50,451 epoch 59 - iter 10/52 - loss 0.08002067 - samples/sec: 44.59 - lr: 0.012500\n",
      "2022-05-07 02:12:51,331 epoch 59 - iter 15/52 - loss 0.08088055 - samples/sec: 51.48 - lr: 0.012500\n",
      "2022-05-07 02:12:52,259 epoch 59 - iter 20/52 - loss 0.08400711 - samples/sec: 47.06 - lr: 0.012500\n",
      "2022-05-07 02:12:53,190 epoch 59 - iter 25/52 - loss 0.07767355 - samples/sec: 45.04 - lr: 0.012500\n",
      "2022-05-07 02:12:54,083 epoch 59 - iter 30/52 - loss 0.07303970 - samples/sec: 48.96 - lr: 0.012500\n",
      "2022-05-07 02:12:54,993 epoch 59 - iter 35/52 - loss 0.07221594 - samples/sec: 47.39 - lr: 0.012500\n",
      "2022-05-07 02:12:55,932 epoch 59 - iter 40/52 - loss 0.07193033 - samples/sec: 45.98 - lr: 0.012500\n",
      "2022-05-07 02:12:56,856 epoch 59 - iter 45/52 - loss 0.08231773 - samples/sec: 46.59 - lr: 0.012500\n",
      "2022-05-07 02:12:57,756 epoch 59 - iter 50/52 - loss 0.08052760 - samples/sec: 48.74 - lr: 0.012500\n",
      "2022-05-07 02:12:58,400 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:12:58,401 EPOCH 59 done: loss 0.0810 - lr 0.012500\n",
      "2022-05-07 02:12:58,401 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 02:12:59,812 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:13:00,265 epoch 60 - iter 5/52 - loss 0.07406890 - samples/sec: 44.45 - lr: 0.012500\n",
      "2022-05-07 02:13:01,172 epoch 60 - iter 10/52 - loss 0.07553877 - samples/sec: 48.14 - lr: 0.012500\n",
      "2022-05-07 02:13:02,073 epoch 60 - iter 15/52 - loss 0.08535750 - samples/sec: 48.55 - lr: 0.012500\n",
      "2022-05-07 02:13:02,969 epoch 60 - iter 20/52 - loss 0.08034013 - samples/sec: 48.36 - lr: 0.012500\n",
      "2022-05-07 02:13:03,910 epoch 60 - iter 25/52 - loss 0.07708454 - samples/sec: 44.44 - lr: 0.012500\n",
      "2022-05-07 02:13:04,790 epoch 60 - iter 30/52 - loss 0.07564823 - samples/sec: 50.57 - lr: 0.012500\n",
      "2022-05-07 02:13:05,703 epoch 60 - iter 35/52 - loss 0.07886174 - samples/sec: 47.17 - lr: 0.012500\n",
      "2022-05-07 02:13:06,609 epoch 60 - iter 40/52 - loss 0.08233794 - samples/sec: 49.38 - lr: 0.012500\n",
      "2022-05-07 02:13:07,521 epoch 60 - iter 45/52 - loss 0.08621900 - samples/sec: 47.39 - lr: 0.012500\n",
      "2022-05-07 02:13:08,442 epoch 60 - iter 50/52 - loss 0.09006673 - samples/sec: 46.89 - lr: 0.012500\n",
      "2022-05-07 02:13:09,092 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:13:09,093 EPOCH 60 done: loss 0.0892 - lr 0.012500\n",
      "2022-05-07 02:13:09,093 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 02:13:10,513 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:13:10,977 epoch 61 - iter 5/52 - loss 0.13788377 - samples/sec: 43.17 - lr: 0.012500\n",
      "2022-05-07 02:13:11,890 epoch 61 - iter 10/52 - loss 0.12324073 - samples/sec: 46.30 - lr: 0.012500\n",
      "2022-05-07 02:13:12,773 epoch 61 - iter 15/52 - loss 0.11466400 - samples/sec: 50.12 - lr: 0.012500\n",
      "2022-05-07 02:13:13,652 epoch 61 - iter 20/52 - loss 0.11599172 - samples/sec: 50.82 - lr: 0.012500\n",
      "2022-05-07 02:13:14,539 epoch 61 - iter 25/52 - loss 0.11198817 - samples/sec: 48.72 - lr: 0.012500\n",
      "2022-05-07 02:13:15,453 epoch 61 - iter 30/52 - loss 0.11160783 - samples/sec: 46.24 - lr: 0.012500\n",
      "2022-05-07 02:13:16,360 epoch 61 - iter 35/52 - loss 0.10346305 - samples/sec: 46.13 - lr: 0.012500\n",
      "2022-05-07 02:13:17,234 epoch 61 - iter 40/52 - loss 0.10199337 - samples/sec: 50.06 - lr: 0.012500\n",
      "2022-05-07 02:13:18,130 epoch 61 - iter 45/52 - loss 0.09656744 - samples/sec: 48.07 - lr: 0.012500\n",
      "2022-05-07 02:13:19,015 epoch 61 - iter 50/52 - loss 0.09292748 - samples/sec: 49.20 - lr: 0.012500\n",
      "2022-05-07 02:13:19,649 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:13:19,650 EPOCH 61 done: loss 0.0922 - lr 0.012500\n",
      "2022-05-07 02:13:19,651 BAD EPOCHS (no improvement): 2\n",
      "2022-05-07 02:13:21,033 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:13:21,483 epoch 62 - iter 5/52 - loss 0.15376333 - samples/sec: 44.76 - lr: 0.012500\n",
      "2022-05-07 02:13:22,414 epoch 62 - iter 10/52 - loss 0.09316939 - samples/sec: 47.00 - lr: 0.012500\n",
      "2022-05-07 02:13:23,350 epoch 62 - iter 15/52 - loss 0.09093168 - samples/sec: 47.51 - lr: 0.012500\n",
      "2022-05-07 02:13:24,260 epoch 62 - iter 20/52 - loss 0.08642954 - samples/sec: 49.26 - lr: 0.012500\n",
      "2022-05-07 02:13:25,178 epoch 62 - iter 25/52 - loss 0.08830908 - samples/sec: 48.96 - lr: 0.012500\n",
      "2022-05-07 02:13:26,082 epoch 62 - iter 30/52 - loss 0.08820763 - samples/sec: 49.82 - lr: 0.012500\n",
      "2022-05-07 02:13:26,980 epoch 62 - iter 35/52 - loss 0.08787632 - samples/sec: 48.66 - lr: 0.012500\n",
      "2022-05-07 02:13:27,897 epoch 62 - iter 40/52 - loss 0.08532628 - samples/sec: 47.00 - lr: 0.012500\n",
      "2022-05-07 02:13:28,838 epoch 62 - iter 45/52 - loss 0.08014807 - samples/sec: 45.76 - lr: 0.012500\n",
      "2022-05-07 02:13:29,772 epoch 62 - iter 50/52 - loss 0.08474555 - samples/sec: 45.30 - lr: 0.012500\n",
      "2022-05-07 02:13:30,433 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:13:30,434 EPOCH 62 done: loss 0.0872 - lr 0.012500\n",
      "2022-05-07 02:13:30,435 BAD EPOCHS (no improvement): 3\n",
      "2022-05-07 02:13:31,876 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:13:32,354 epoch 63 - iter 5/52 - loss 0.06635305 - samples/sec: 41.97 - lr: 0.012500\n",
      "2022-05-07 02:13:33,287 epoch 63 - iter 10/52 - loss 0.06701635 - samples/sec: 48.43 - lr: 0.012500\n",
      "2022-05-07 02:13:34,228 epoch 63 - iter 15/52 - loss 0.06981413 - samples/sec: 45.87 - lr: 0.012500\n",
      "2022-05-07 02:13:35,129 epoch 63 - iter 20/52 - loss 0.06583917 - samples/sec: 49.69 - lr: 0.012500\n",
      "2022-05-07 02:13:36,071 epoch 63 - iter 25/52 - loss 0.06851223 - samples/sec: 44.84 - lr: 0.012500\n",
      "2022-05-07 02:13:36,989 epoch 63 - iter 30/52 - loss 0.07618790 - samples/sec: 46.89 - lr: 0.012500\n",
      "2022-05-07 02:13:37,899 epoch 63 - iter 35/52 - loss 0.07214821 - samples/sec: 48.14 - lr: 0.012500\n",
      "2022-05-07 02:13:38,834 epoch 63 - iter 40/52 - loss 0.08080961 - samples/sec: 46.08 - lr: 0.012500\n",
      "2022-05-07 02:13:39,777 epoch 63 - iter 45/52 - loss 0.07918885 - samples/sec: 45.51 - lr: 0.012500\n",
      "2022-05-07 02:13:40,666 epoch 63 - iter 50/52 - loss 0.08134923 - samples/sec: 49.81 - lr: 0.012500\n",
      "2022-05-07 02:13:41,327 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:13:41,327 EPOCH 63 done: loss 0.0802 - lr 0.012500\n",
      "2022-05-07 02:13:41,328 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 02:13:42,784 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:13:43,259 epoch 64 - iter 5/52 - loss 0.12764164 - samples/sec: 42.37 - lr: 0.012500\n",
      "2022-05-07 02:13:44,187 epoch 64 - iter 10/52 - loss 0.11165168 - samples/sec: 46.89 - lr: 0.012500\n",
      "2022-05-07 02:13:45,116 epoch 64 - iter 15/52 - loss 0.09574250 - samples/sec: 46.03 - lr: 0.012500\n",
      "2022-05-07 02:13:46,031 epoch 64 - iter 20/52 - loss 0.08777003 - samples/sec: 48.08 - lr: 0.012500\n",
      "2022-05-07 02:13:46,950 epoch 64 - iter 25/52 - loss 0.09182130 - samples/sec: 51.75 - lr: 0.012500\n",
      "2022-05-07 02:13:47,864 epoch 64 - iter 30/52 - loss 0.09394772 - samples/sec: 46.89 - lr: 0.012500\n",
      "2022-05-07 02:13:48,772 epoch 64 - iter 35/52 - loss 0.08865286 - samples/sec: 47.34 - lr: 0.012500\n",
      "2022-05-07 02:13:49,696 epoch 64 - iter 40/52 - loss 0.08765521 - samples/sec: 47.62 - lr: 0.012500\n",
      "2022-05-07 02:13:50,631 epoch 64 - iter 45/52 - loss 0.08909528 - samples/sec: 45.92 - lr: 0.012500\n",
      "2022-05-07 02:13:51,594 epoch 64 - iter 50/52 - loss 0.09072335 - samples/sec: 44.25 - lr: 0.012500\n",
      "2022-05-07 02:13:52,270 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:13:52,271 EPOCH 64 done: loss 0.0910 - lr 0.012500\n",
      "2022-05-07 02:13:52,272 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 02:13:53,723 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:13:54,172 epoch 65 - iter 5/52 - loss 0.06925835 - samples/sec: 44.69 - lr: 0.012500\n",
      "2022-05-07 02:13:55,099 epoch 65 - iter 10/52 - loss 0.06658086 - samples/sec: 46.67 - lr: 0.012500\n",
      "2022-05-07 02:13:56,011 epoch 65 - iter 15/52 - loss 0.07457421 - samples/sec: 48.66 - lr: 0.012500\n",
      "2022-05-07 02:13:56,945 epoch 65 - iter 20/52 - loss 0.08661899 - samples/sec: 45.09 - lr: 0.012500\n",
      "2022-05-07 02:13:57,886 epoch 65 - iter 25/52 - loss 0.08666894 - samples/sec: 45.35 - lr: 0.012500\n",
      "2022-05-07 02:13:58,792 epoch 65 - iter 30/52 - loss 0.08792410 - samples/sec: 49.50 - lr: 0.012500\n",
      "2022-05-07 02:13:59,709 epoch 65 - iter 35/52 - loss 0.08424127 - samples/sec: 47.56 - lr: 0.012500\n",
      "2022-05-07 02:14:00,619 epoch 65 - iter 40/52 - loss 0.08653143 - samples/sec: 49.38 - lr: 0.012500\n",
      "2022-05-07 02:14:01,551 epoch 65 - iter 45/52 - loss 0.08823278 - samples/sec: 46.78 - lr: 0.012500\n",
      "2022-05-07 02:14:02,472 epoch 65 - iter 50/52 - loss 0.08718487 - samples/sec: 46.40 - lr: 0.012500\n",
      "2022-05-07 02:14:03,126 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:14:03,127 EPOCH 65 done: loss 0.0866 - lr 0.012500\n",
      "2022-05-07 02:14:03,127 BAD EPOCHS (no improvement): 2\n",
      "2022-05-07 02:14:04,607 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:14:05,091 epoch 66 - iter 5/52 - loss 0.09590399 - samples/sec: 41.54 - lr: 0.012500\n",
      "2022-05-07 02:14:06,042 epoch 66 - iter 10/52 - loss 0.09484754 - samples/sec: 45.45 - lr: 0.012500\n",
      "2022-05-07 02:14:06,972 epoch 66 - iter 15/52 - loss 0.08478747 - samples/sec: 46.51 - lr: 0.012500\n",
      "2022-05-07 02:14:07,915 epoch 66 - iter 20/52 - loss 0.09512029 - samples/sec: 44.64 - lr: 0.012500\n",
      "2022-05-07 02:14:08,826 epoch 66 - iter 25/52 - loss 0.09182041 - samples/sec: 47.73 - lr: 0.012500\n",
      "2022-05-07 02:14:09,742 epoch 66 - iter 30/52 - loss 0.08658915 - samples/sec: 47.62 - lr: 0.012500\n",
      "2022-05-07 02:14:10,665 epoch 66 - iter 35/52 - loss 0.08414085 - samples/sec: 46.35 - lr: 0.012500\n",
      "2022-05-07 02:14:11,569 epoch 66 - iter 40/52 - loss 0.08564625 - samples/sec: 48.84 - lr: 0.012500\n",
      "2022-05-07 02:14:12,492 epoch 66 - iter 45/52 - loss 0.09077082 - samples/sec: 47.51 - lr: 0.012500\n",
      "2022-05-07 02:14:13,412 epoch 66 - iter 50/52 - loss 0.09087531 - samples/sec: 49.44 - lr: 0.012500\n",
      "2022-05-07 02:14:14,097 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:14:14,098 EPOCH 66 done: loss 0.0900 - lr 0.012500\n",
      "2022-05-07 02:14:14,099 BAD EPOCHS (no improvement): 3\n",
      "2022-05-07 02:14:15,560 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:14:16,013 epoch 67 - iter 5/52 - loss 0.05299718 - samples/sec: 44.35 - lr: 0.012500\n",
      "2022-05-07 02:14:16,945 epoch 67 - iter 10/52 - loss 0.07022582 - samples/sec: 46.84 - lr: 0.012500\n",
      "2022-05-07 02:14:17,882 epoch 67 - iter 15/52 - loss 0.06849859 - samples/sec: 45.82 - lr: 0.012500\n",
      "2022-05-07 02:14:18,807 epoch 67 - iter 20/52 - loss 0.07450300 - samples/sec: 46.24 - lr: 0.012500\n",
      "2022-05-07 02:14:19,736 epoch 67 - iter 25/52 - loss 0.07850290 - samples/sec: 46.24 - lr: 0.012500\n",
      "2022-05-07 02:14:20,607 epoch 67 - iter 30/52 - loss 0.08067414 - samples/sec: 52.15 - lr: 0.012500\n",
      "2022-05-07 02:14:21,525 epoch 67 - iter 35/52 - loss 0.08262789 - samples/sec: 47.28 - lr: 0.012500\n",
      "2022-05-07 02:14:22,423 epoch 67 - iter 40/52 - loss 0.08548682 - samples/sec: 50.06 - lr: 0.012500\n",
      "2022-05-07 02:14:23,351 epoch 67 - iter 45/52 - loss 0.08781283 - samples/sec: 48.25 - lr: 0.012500\n",
      "2022-05-07 02:14:24,292 epoch 67 - iter 50/52 - loss 0.08677828 - samples/sec: 45.35 - lr: 0.012500\n",
      "2022-05-07 02:14:24,967 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:14:24,968 EPOCH 67 done: loss 0.0846 - lr 0.012500\n",
      "2022-05-07 02:14:24,968 Epoch    67: reducing learning rate of group 0 to 6.2500e-03.\n",
      "2022-05-07 02:14:24,969 BAD EPOCHS (no improvement): 4\n",
      "2022-05-07 02:14:26,423 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:14:26,856 epoch 68 - iter 5/52 - loss 0.08813079 - samples/sec: 46.35 - lr: 0.006250\n",
      "2022-05-07 02:14:27,777 epoch 68 - iter 10/52 - loss 0.08398461 - samples/sec: 49.32 - lr: 0.006250\n",
      "2022-05-07 02:14:28,715 epoch 68 - iter 15/52 - loss 0.09734808 - samples/sec: 45.98 - lr: 0.006250\n",
      "2022-05-07 02:14:29,617 epoch 68 - iter 20/52 - loss 0.09178613 - samples/sec: 50.51 - lr: 0.006250\n",
      "2022-05-07 02:14:30,585 epoch 68 - iter 25/52 - loss 0.08275521 - samples/sec: 44.44 - lr: 0.006250\n",
      "2022-05-07 02:14:31,498 epoch 68 - iter 30/52 - loss 0.08248780 - samples/sec: 48.60 - lr: 0.006250\n",
      "2022-05-07 02:14:32,439 epoch 68 - iter 35/52 - loss 0.08366430 - samples/sec: 45.45 - lr: 0.006250\n",
      "2022-05-07 02:14:33,394 epoch 68 - iter 40/52 - loss 0.09046805 - samples/sec: 46.24 - lr: 0.006250\n",
      "2022-05-07 02:14:34,325 epoch 68 - iter 45/52 - loss 0.08647394 - samples/sec: 45.35 - lr: 0.006250\n",
      "2022-05-07 02:14:35,238 epoch 68 - iter 50/52 - loss 0.08400183 - samples/sec: 48.13 - lr: 0.006250\n",
      "2022-05-07 02:14:35,924 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:14:35,924 EPOCH 68 done: loss 0.0821 - lr 0.006250\n",
      "2022-05-07 02:14:35,925 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 02:14:37,335 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:14:37,770 epoch 69 - iter 5/52 - loss 0.06251483 - samples/sec: 46.14 - lr: 0.006250\n",
      "2022-05-07 02:14:38,703 epoch 69 - iter 10/52 - loss 0.08294940 - samples/sec: 46.69 - lr: 0.006250\n",
      "2022-05-07 02:14:39,623 epoch 69 - iter 15/52 - loss 0.07485634 - samples/sec: 46.51 - lr: 0.006250\n",
      "2022-05-07 02:14:40,491 epoch 69 - iter 20/52 - loss 0.07749306 - samples/sec: 51.22 - lr: 0.006250\n",
      "2022-05-07 02:14:41,387 epoch 69 - iter 25/52 - loss 0.07321559 - samples/sec: 50.44 - lr: 0.006250\n",
      "2022-05-07 02:14:42,284 epoch 69 - iter 30/52 - loss 0.07793351 - samples/sec: 48.96 - lr: 0.006250\n",
      "2022-05-07 02:14:43,194 epoch 69 - iter 35/52 - loss 0.08102793 - samples/sec: 46.40 - lr: 0.006250\n",
      "2022-05-07 02:14:44,121 epoch 69 - iter 40/52 - loss 0.08015691 - samples/sec: 45.20 - lr: 0.006250\n",
      "2022-05-07 02:14:45,036 epoch 69 - iter 45/52 - loss 0.08190545 - samples/sec: 46.95 - lr: 0.006250\n",
      "2022-05-07 02:14:45,975 epoch 69 - iter 50/52 - loss 0.08097757 - samples/sec: 44.69 - lr: 0.006250\n",
      "2022-05-07 02:14:46,613 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:14:46,614 EPOCH 69 done: loss 0.0822 - lr 0.006250\n",
      "2022-05-07 02:14:46,614 BAD EPOCHS (no improvement): 2\n",
      "2022-05-07 02:14:48,002 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:14:48,442 epoch 70 - iter 5/52 - loss 0.06528037 - samples/sec: 45.61 - lr: 0.006250\n",
      "2022-05-07 02:14:49,339 epoch 70 - iter 10/52 - loss 0.07690559 - samples/sec: 49.32 - lr: 0.006250\n",
      "2022-05-07 02:14:50,199 epoch 70 - iter 15/52 - loss 0.07185061 - samples/sec: 51.12 - lr: 0.006250\n",
      "2022-05-07 02:14:51,109 epoch 70 - iter 20/52 - loss 0.07532218 - samples/sec: 46.29 - lr: 0.006250\n",
      "2022-05-07 02:14:52,028 epoch 70 - iter 25/52 - loss 0.08324742 - samples/sec: 46.19 - lr: 0.006250\n",
      "2022-05-07 02:14:52,910 epoch 70 - iter 30/52 - loss 0.08755127 - samples/sec: 48.96 - lr: 0.006250\n",
      "2022-05-07 02:14:53,801 epoch 70 - iter 35/52 - loss 0.08668216 - samples/sec: 49.28 - lr: 0.006250\n",
      "2022-05-07 02:14:54,688 epoch 70 - iter 40/52 - loss 0.08428070 - samples/sec: 50.39 - lr: 0.006250\n",
      "2022-05-07 02:14:55,606 epoch 70 - iter 45/52 - loss 0.08625612 - samples/sec: 46.95 - lr: 0.006250\n",
      "2022-05-07 02:14:56,487 epoch 70 - iter 50/52 - loss 0.08524231 - samples/sec: 50.00 - lr: 0.006250\n",
      "2022-05-07 02:14:57,125 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:14:57,126 EPOCH 70 done: loss 0.0842 - lr 0.006250\n",
      "2022-05-07 02:14:57,127 BAD EPOCHS (no improvement): 3\n",
      "2022-05-07 02:14:58,525 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:14:58,922 epoch 71 - iter 5/52 - loss 0.11622499 - samples/sec: 50.51 - lr: 0.006250\n",
      "2022-05-07 02:14:59,833 epoch 71 - iter 10/52 - loss 0.10154137 - samples/sec: 48.19 - lr: 0.006250\n",
      "2022-05-07 02:15:00,737 epoch 71 - iter 15/52 - loss 0.08944154 - samples/sec: 48.06 - lr: 0.006250\n",
      "2022-05-07 02:15:01,657 epoch 71 - iter 20/52 - loss 0.07916666 - samples/sec: 46.91 - lr: 0.006250\n",
      "2022-05-07 02:15:02,534 epoch 71 - iter 25/52 - loss 0.07772385 - samples/sec: 49.32 - lr: 0.006250\n",
      "2022-05-07 02:15:03,497 epoch 71 - iter 30/52 - loss 0.07659361 - samples/sec: 44.74 - lr: 0.006250\n",
      "2022-05-07 02:15:04,430 epoch 71 - iter 35/52 - loss 0.07502178 - samples/sec: 46.24 - lr: 0.006250\n",
      "2022-05-07 02:15:05,328 epoch 71 - iter 40/52 - loss 0.07626524 - samples/sec: 49.87 - lr: 0.006250\n",
      "2022-05-07 02:15:06,243 epoch 71 - iter 45/52 - loss 0.07694619 - samples/sec: 47.17 - lr: 0.006250\n",
      "2022-05-07 02:15:07,147 epoch 71 - iter 50/52 - loss 0.07795677 - samples/sec: 48.37 - lr: 0.006250\n",
      "2022-05-07 02:15:07,820 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:15:07,820 EPOCH 71 done: loss 0.0780 - lr 0.006250\n",
      "2022-05-07 02:15:07,821 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 02:15:09,290 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:15:09,745 epoch 72 - iter 5/52 - loss 0.04852372 - samples/sec: 44.05 - lr: 0.006250\n",
      "2022-05-07 02:15:10,668 epoch 72 - iter 10/52 - loss 0.04238414 - samples/sec: 47.39 - lr: 0.006250\n",
      "2022-05-07 02:15:11,578 epoch 72 - iter 15/52 - loss 0.07938058 - samples/sec: 47.28 - lr: 0.006250\n",
      "2022-05-07 02:15:12,519 epoch 72 - iter 20/52 - loss 0.08106122 - samples/sec: 46.19 - lr: 0.006250\n",
      "2022-05-07 02:15:13,448 epoch 72 - iter 25/52 - loss 0.08110792 - samples/sec: 46.78 - lr: 0.006250\n",
      "2022-05-07 02:15:14,367 epoch 72 - iter 30/52 - loss 0.08125396 - samples/sec: 48.13 - lr: 0.006250\n",
      "2022-05-07 02:15:15,315 epoch 72 - iter 35/52 - loss 0.07561842 - samples/sec: 44.59 - lr: 0.006250\n",
      "2022-05-07 02:15:16,235 epoch 72 - iter 40/52 - loss 0.07635348 - samples/sec: 46.57 - lr: 0.006250\n",
      "2022-05-07 02:15:17,156 epoch 72 - iter 45/52 - loss 0.07857901 - samples/sec: 45.35 - lr: 0.006250\n",
      "2022-05-07 02:15:18,067 epoch 72 - iter 50/52 - loss 0.08239763 - samples/sec: 47.39 - lr: 0.006250\n",
      "2022-05-07 02:15:18,778 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:15:18,779 EPOCH 72 done: loss 0.0829 - lr 0.006250\n",
      "2022-05-07 02:15:18,779 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 02:15:20,222 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:15:20,705 epoch 73 - iter 5/52 - loss 0.06592225 - samples/sec: 41.58 - lr: 0.006250\n",
      "2022-05-07 02:15:21,609 epoch 73 - iter 10/52 - loss 0.05165489 - samples/sec: 49.51 - lr: 0.006250\n",
      "2022-05-07 02:15:22,520 epoch 73 - iter 15/52 - loss 0.04444461 - samples/sec: 47.23 - lr: 0.006250\n",
      "2022-05-07 02:15:23,436 epoch 73 - iter 20/52 - loss 0.05997509 - samples/sec: 47.45 - lr: 0.006250\n",
      "2022-05-07 02:15:24,353 epoch 73 - iter 25/52 - loss 0.05801319 - samples/sec: 48.02 - lr: 0.006250\n",
      "2022-05-07 02:15:25,292 epoch 73 - iter 30/52 - loss 0.06221204 - samples/sec: 44.69 - lr: 0.006250\n",
      "2022-05-07 02:15:26,195 epoch 73 - iter 35/52 - loss 0.07047475 - samples/sec: 48.60 - lr: 0.006250\n",
      "2022-05-07 02:15:27,126 epoch 73 - iter 40/52 - loss 0.08367084 - samples/sec: 46.08 - lr: 0.006250\n",
      "2022-05-07 02:15:28,046 epoch 73 - iter 45/52 - loss 0.08430015 - samples/sec: 48.72 - lr: 0.006250\n",
      "2022-05-07 02:15:28,991 epoch 73 - iter 50/52 - loss 0.08548540 - samples/sec: 44.69 - lr: 0.006250\n",
      "2022-05-07 02:15:29,671 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:15:29,672 EPOCH 73 done: loss 0.0855 - lr 0.006250\n",
      "2022-05-07 02:15:29,673 BAD EPOCHS (no improvement): 2\n",
      "2022-05-07 02:15:31,099 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:15:31,551 epoch 74 - iter 5/52 - loss 0.06763040 - samples/sec: 44.44 - lr: 0.006250\n",
      "2022-05-07 02:15:32,481 epoch 74 - iter 10/52 - loss 0.08634680 - samples/sec: 46.56 - lr: 0.006250\n",
      "2022-05-07 02:15:33,399 epoch 74 - iter 15/52 - loss 0.07593842 - samples/sec: 45.61 - lr: 0.006250\n",
      "2022-05-07 02:15:34,301 epoch 74 - iter 20/52 - loss 0.07384836 - samples/sec: 48.66 - lr: 0.006250\n",
      "2022-05-07 02:15:35,227 epoch 74 - iter 25/52 - loss 0.08211004 - samples/sec: 46.29 - lr: 0.006250\n",
      "2022-05-07 02:15:36,159 epoch 74 - iter 30/52 - loss 0.08226748 - samples/sec: 49.38 - lr: 0.006250\n",
      "2022-05-07 02:15:37,049 epoch 74 - iter 35/52 - loss 0.07962108 - samples/sec: 50.06 - lr: 0.006250\n",
      "2022-05-07 02:15:37,999 epoch 74 - iter 40/52 - loss 0.07814836 - samples/sec: 43.10 - lr: 0.006250\n",
      "2022-05-07 02:15:38,905 epoch 74 - iter 45/52 - loss 0.07681232 - samples/sec: 47.85 - lr: 0.006250\n",
      "2022-05-07 02:15:39,825 epoch 74 - iter 50/52 - loss 0.07825793 - samples/sec: 47.45 - lr: 0.006250\n",
      "2022-05-07 02:15:40,484 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:15:40,485 EPOCH 74 done: loss 0.0806 - lr 0.006250\n",
      "2022-05-07 02:15:40,485 BAD EPOCHS (no improvement): 3\n",
      "2022-05-07 02:15:42,009 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:15:42,461 epoch 75 - iter 5/52 - loss 0.10969250 - samples/sec: 44.49 - lr: 0.006250\n",
      "2022-05-07 02:15:43,405 epoch 75 - iter 10/52 - loss 0.09511912 - samples/sec: 47.28 - lr: 0.006250\n",
      "2022-05-07 02:15:44,354 epoch 75 - iter 15/52 - loss 0.11261200 - samples/sec: 47.28 - lr: 0.006250\n",
      "2022-05-07 02:15:45,307 epoch 75 - iter 20/52 - loss 0.09962862 - samples/sec: 45.82 - lr: 0.006250\n",
      "2022-05-07 02:15:46,237 epoch 75 - iter 25/52 - loss 0.09826294 - samples/sec: 48.14 - lr: 0.006250\n",
      "2022-05-07 02:15:47,185 epoch 75 - iter 30/52 - loss 0.09521742 - samples/sec: 45.30 - lr: 0.006250\n",
      "2022-05-07 02:15:48,125 epoch 75 - iter 35/52 - loss 0.09339421 - samples/sec: 46.40 - lr: 0.006250\n",
      "2022-05-07 02:15:49,055 epoch 75 - iter 40/52 - loss 0.09221108 - samples/sec: 46.73 - lr: 0.006250\n",
      "2022-05-07 02:15:49,971 epoch 75 - iter 45/52 - loss 0.09214211 - samples/sec: 48.54 - lr: 0.006250\n",
      "2022-05-07 02:15:50,919 epoch 75 - iter 50/52 - loss 0.09083206 - samples/sec: 45.40 - lr: 0.006250\n",
      "2022-05-07 02:15:51,588 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:15:51,589 EPOCH 75 done: loss 0.0904 - lr 0.006250\n",
      "2022-05-07 02:15:51,590 Epoch    75: reducing learning rate of group 0 to 3.1250e-03.\n",
      "2022-05-07 02:15:51,590 BAD EPOCHS (no improvement): 4\n",
      "2022-05-07 02:15:53,037 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:15:53,506 epoch 76 - iter 5/52 - loss 0.07450945 - samples/sec: 42.87 - lr: 0.003125\n",
      "2022-05-07 02:15:54,410 epoch 76 - iter 10/52 - loss 0.08552857 - samples/sec: 49.20 - lr: 0.003125\n",
      "2022-05-07 02:15:55,322 epoch 76 - iter 15/52 - loss 0.09459207 - samples/sec: 47.96 - lr: 0.003125\n",
      "2022-05-07 02:15:56,269 epoch 76 - iter 20/52 - loss 0.09053325 - samples/sec: 45.40 - lr: 0.003125\n",
      "2022-05-07 02:15:57,224 epoch 76 - iter 25/52 - loss 0.08741874 - samples/sec: 46.67 - lr: 0.003125\n",
      "2022-05-07 02:15:58,148 epoch 76 - iter 30/52 - loss 0.08593949 - samples/sec: 47.62 - lr: 0.003125\n",
      "2022-05-07 02:15:59,096 epoch 76 - iter 35/52 - loss 0.08485072 - samples/sec: 46.13 - lr: 0.003125\n",
      "2022-05-07 02:15:59,990 epoch 76 - iter 40/52 - loss 0.08306437 - samples/sec: 50.37 - lr: 0.003125\n",
      "2022-05-07 02:16:00,927 epoch 76 - iter 45/52 - loss 0.08462125 - samples/sec: 45.30 - lr: 0.003125\n",
      "2022-05-07 02:16:01,831 epoch 76 - iter 50/52 - loss 0.08556201 - samples/sec: 48.07 - lr: 0.003125\n",
      "2022-05-07 02:16:02,503 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:16:02,504 EPOCH 76 done: loss 0.0852 - lr 0.003125\n",
      "2022-05-07 02:16:02,504 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 02:16:03,963 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:16:04,440 epoch 77 - iter 5/52 - loss 0.05719510 - samples/sec: 42.11 - lr: 0.003125\n",
      "2022-05-07 02:16:05,380 epoch 77 - iter 10/52 - loss 0.06763036 - samples/sec: 45.51 - lr: 0.003125\n",
      "2022-05-07 02:16:06,293 epoch 77 - iter 15/52 - loss 0.05920980 - samples/sec: 48.08 - lr: 0.003125\n",
      "2022-05-07 02:16:07,199 epoch 77 - iter 20/52 - loss 0.06090310 - samples/sec: 49.38 - lr: 0.003125\n",
      "2022-05-07 02:16:08,139 epoch 77 - iter 25/52 - loss 0.05993269 - samples/sec: 45.87 - lr: 0.003125\n",
      "2022-05-07 02:16:09,046 epoch 77 - iter 30/52 - loss 0.06808769 - samples/sec: 47.39 - lr: 0.003125\n",
      "2022-05-07 02:16:09,966 epoch 77 - iter 35/52 - loss 0.06828081 - samples/sec: 46.40 - lr: 0.003125\n",
      "2022-05-07 02:16:10,895 epoch 77 - iter 40/52 - loss 0.07246364 - samples/sec: 46.46 - lr: 0.003125\n",
      "2022-05-07 02:16:11,818 epoch 77 - iter 45/52 - loss 0.07450701 - samples/sec: 45.82 - lr: 0.003125\n",
      "2022-05-07 02:16:12,749 epoch 77 - iter 50/52 - loss 0.07461309 - samples/sec: 45.98 - lr: 0.003125\n",
      "2022-05-07 02:16:13,418 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:16:13,419 EPOCH 77 done: loss 0.0754 - lr 0.003125\n",
      "2022-05-07 02:16:13,420 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 02:16:14,902 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:16:15,366 epoch 78 - iter 5/52 - loss 0.06275989 - samples/sec: 43.29 - lr: 0.003125\n",
      "2022-05-07 02:16:16,297 epoch 78 - iter 10/52 - loss 0.09380830 - samples/sec: 47.11 - lr: 0.003125\n",
      "2022-05-07 02:16:17,216 epoch 78 - iter 15/52 - loss 0.09592228 - samples/sec: 47.17 - lr: 0.003125\n",
      "2022-05-07 02:16:18,150 epoch 78 - iter 20/52 - loss 0.08421874 - samples/sec: 46.73 - lr: 0.003125\n",
      "2022-05-07 02:16:19,054 epoch 78 - iter 25/52 - loss 0.08519197 - samples/sec: 47.62 - lr: 0.003125\n",
      "2022-05-07 02:16:19,936 epoch 78 - iter 30/52 - loss 0.08488343 - samples/sec: 49.51 - lr: 0.003125\n",
      "2022-05-07 02:16:20,802 epoch 78 - iter 35/52 - loss 0.09434169 - samples/sec: 51.39 - lr: 0.003125\n",
      "2022-05-07 02:16:21,690 epoch 78 - iter 40/52 - loss 0.09472828 - samples/sec: 49.75 - lr: 0.003125\n",
      "2022-05-07 02:16:22,576 epoch 78 - iter 45/52 - loss 0.08723864 - samples/sec: 47.90 - lr: 0.003125\n",
      "2022-05-07 02:16:23,492 epoch 78 - iter 50/52 - loss 0.08761576 - samples/sec: 48.37 - lr: 0.003125\n",
      "2022-05-07 02:16:24,146 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:16:24,147 EPOCH 78 done: loss 0.0859 - lr 0.003125\n",
      "2022-05-07 02:16:24,148 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 02:16:25,635 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:16:26,097 epoch 79 - iter 5/52 - loss 0.11793925 - samples/sec: 43.43 - lr: 0.003125\n",
      "2022-05-07 02:16:27,011 epoch 79 - iter 10/52 - loss 0.07897551 - samples/sec: 47.28 - lr: 0.003125\n",
      "2022-05-07 02:16:27,905 epoch 79 - iter 15/52 - loss 0.09601946 - samples/sec: 47.33 - lr: 0.003125\n",
      "2022-05-07 02:16:28,798 epoch 79 - iter 20/52 - loss 0.09185425 - samples/sec: 47.73 - lr: 0.003125\n",
      "2022-05-07 02:16:29,687 epoch 79 - iter 25/52 - loss 0.08907000 - samples/sec: 49.39 - lr: 0.003125\n",
      "2022-05-07 02:16:30,582 epoch 79 - iter 30/52 - loss 0.08889024 - samples/sec: 50.31 - lr: 0.003125\n",
      "2022-05-07 02:16:31,497 epoch 79 - iter 35/52 - loss 0.08576727 - samples/sec: 48.19 - lr: 0.003125\n",
      "2022-05-07 02:16:32,388 epoch 79 - iter 40/52 - loss 0.08568095 - samples/sec: 49.50 - lr: 0.003125\n",
      "2022-05-07 02:16:33,270 epoch 79 - iter 45/52 - loss 0.08538567 - samples/sec: 49.51 - lr: 0.003125\n",
      "2022-05-07 02:16:34,144 epoch 79 - iter 50/52 - loss 0.08508035 - samples/sec: 49.94 - lr: 0.003125\n",
      "2022-05-07 02:16:34,813 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:16:34,814 EPOCH 79 done: loss 0.0857 - lr 0.003125\n",
      "2022-05-07 02:16:34,814 BAD EPOCHS (no improvement): 2\n",
      "2022-05-07 02:16:36,199 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:16:36,630 epoch 80 - iter 5/52 - loss 0.05831851 - samples/sec: 46.62 - lr: 0.003125\n",
      "2022-05-07 02:16:37,520 epoch 80 - iter 10/52 - loss 0.07367280 - samples/sec: 49.26 - lr: 0.003125\n",
      "2022-05-07 02:16:38,447 epoch 80 - iter 15/52 - loss 0.07466150 - samples/sec: 46.25 - lr: 0.003125\n",
      "2022-05-07 02:16:39,312 epoch 80 - iter 20/52 - loss 0.07426374 - samples/sec: 52.08 - lr: 0.003125\n",
      "2022-05-07 02:16:40,211 epoch 80 - iter 25/52 - loss 0.07908327 - samples/sec: 48.32 - lr: 0.003125\n",
      "2022-05-07 02:16:41,097 epoch 80 - iter 30/52 - loss 0.07939557 - samples/sec: 49.63 - lr: 0.003125\n",
      "2022-05-07 02:16:41,966 epoch 80 - iter 35/52 - loss 0.07762909 - samples/sec: 51.17 - lr: 0.003125\n",
      "2022-05-07 02:16:42,842 epoch 80 - iter 40/52 - loss 0.07898542 - samples/sec: 49.59 - lr: 0.003125\n",
      "2022-05-07 02:16:43,780 epoch 80 - iter 45/52 - loss 0.07515133 - samples/sec: 44.99 - lr: 0.003125\n",
      "2022-05-07 02:16:44,717 epoch 80 - iter 50/52 - loss 0.07921415 - samples/sec: 44.99 - lr: 0.003125\n",
      "2022-05-07 02:16:45,382 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:16:45,383 EPOCH 80 done: loss 0.0796 - lr 0.003125\n",
      "2022-05-07 02:16:45,384 BAD EPOCHS (no improvement): 3\n",
      "2022-05-07 02:16:46,834 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:16:47,282 epoch 81 - iter 5/52 - loss 0.08794239 - samples/sec: 44.74 - lr: 0.003125\n",
      "2022-05-07 02:16:48,184 epoch 81 - iter 10/52 - loss 0.09152081 - samples/sec: 49.08 - lr: 0.003125\n",
      "2022-05-07 02:16:49,110 epoch 81 - iter 15/52 - loss 0.08940868 - samples/sec: 45.56 - lr: 0.003125\n",
      "2022-05-07 02:16:50,034 epoch 81 - iter 20/52 - loss 0.09058127 - samples/sec: 45.82 - lr: 0.003125\n",
      "2022-05-07 02:16:50,940 epoch 81 - iter 25/52 - loss 0.10319590 - samples/sec: 48.43 - lr: 0.003125\n",
      "2022-05-07 02:16:51,844 epoch 81 - iter 30/52 - loss 0.10363728 - samples/sec: 47.96 - lr: 0.003125\n",
      "2022-05-07 02:16:52,767 epoch 81 - iter 35/52 - loss 0.09615652 - samples/sec: 48.36 - lr: 0.003125\n",
      "2022-05-07 02:16:53,678 epoch 81 - iter 40/52 - loss 0.08982978 - samples/sec: 49.08 - lr: 0.003125\n",
      "2022-05-07 02:16:54,628 epoch 81 - iter 45/52 - loss 0.09226419 - samples/sec: 44.54 - lr: 0.003125\n",
      "2022-05-07 02:16:55,558 epoch 81 - iter 50/52 - loss 0.09556592 - samples/sec: 47.17 - lr: 0.003125\n",
      "2022-05-07 02:16:56,225 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:16:56,226 EPOCH 81 done: loss 0.0944 - lr 0.003125\n",
      "2022-05-07 02:16:56,227 Epoch    81: reducing learning rate of group 0 to 1.5625e-03.\n",
      "2022-05-07 02:16:56,227 BAD EPOCHS (no improvement): 4\n",
      "2022-05-07 02:16:57,673 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:16:58,169 epoch 82 - iter 5/52 - loss 0.06714834 - samples/sec: 40.53 - lr: 0.001563\n",
      "2022-05-07 02:16:59,104 epoch 82 - iter 10/52 - loss 0.06096173 - samples/sec: 45.45 - lr: 0.001563\n",
      "2022-05-07 02:17:00,004 epoch 82 - iter 15/52 - loss 0.06226238 - samples/sec: 49.63 - lr: 0.001563\n",
      "2022-05-07 02:17:00,908 epoch 82 - iter 20/52 - loss 0.06213417 - samples/sec: 49.08 - lr: 0.001563\n",
      "2022-05-07 02:17:01,816 epoch 82 - iter 25/52 - loss 0.07509116 - samples/sec: 46.78 - lr: 0.001563\n",
      "2022-05-07 02:17:02,738 epoch 82 - iter 30/52 - loss 0.07847292 - samples/sec: 48.19 - lr: 0.001563\n",
      "2022-05-07 02:17:03,654 epoch 82 - iter 35/52 - loss 0.07756090 - samples/sec: 48.60 - lr: 0.001563\n",
      "2022-05-07 02:17:04,570 epoch 82 - iter 40/52 - loss 0.07798404 - samples/sec: 48.02 - lr: 0.001563\n",
      "2022-05-07 02:17:05,515 epoch 82 - iter 45/52 - loss 0.07551468 - samples/sec: 48.96 - lr: 0.001563\n",
      "2022-05-07 02:17:06,410 epoch 82 - iter 50/52 - loss 0.07779157 - samples/sec: 48.25 - lr: 0.001563\n",
      "2022-05-07 02:17:07,088 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:17:07,088 EPOCH 82 done: loss 0.0798 - lr 0.001563\n",
      "2022-05-07 02:17:07,089 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 02:17:08,530 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:17:08,973 epoch 83 - iter 5/52 - loss 0.08569353 - samples/sec: 45.30 - lr: 0.001563\n",
      "2022-05-07 02:17:09,909 epoch 83 - iter 10/52 - loss 0.08255678 - samples/sec: 45.82 - lr: 0.001563\n",
      "2022-05-07 02:17:10,820 epoch 83 - iter 15/52 - loss 0.08479056 - samples/sec: 48.31 - lr: 0.001563\n",
      "2022-05-07 02:17:11,737 epoch 83 - iter 20/52 - loss 0.08542614 - samples/sec: 46.46 - lr: 0.001563\n",
      "2022-05-07 02:17:12,682 epoch 83 - iter 25/52 - loss 0.07777075 - samples/sec: 46.56 - lr: 0.001563\n",
      "2022-05-07 02:17:13,599 epoch 83 - iter 30/52 - loss 0.07619019 - samples/sec: 48.36 - lr: 0.001563\n",
      "2022-05-07 02:17:14,501 epoch 83 - iter 35/52 - loss 0.07416875 - samples/sec: 49.14 - lr: 0.001563\n",
      "2022-05-07 02:17:15,430 epoch 83 - iter 40/52 - loss 0.07652697 - samples/sec: 47.62 - lr: 0.001563\n",
      "2022-05-07 02:17:16,343 epoch 83 - iter 45/52 - loss 0.08110841 - samples/sec: 47.34 - lr: 0.001563\n",
      "2022-05-07 02:17:17,261 epoch 83 - iter 50/52 - loss 0.07681665 - samples/sec: 49.20 - lr: 0.001563\n",
      "2022-05-07 02:17:17,918 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:17:17,919 EPOCH 83 done: loss 0.0774 - lr 0.001563\n",
      "2022-05-07 02:17:17,919 BAD EPOCHS (no improvement): 2\n",
      "2022-05-07 02:17:19,349 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:17:19,800 epoch 84 - iter 5/52 - loss 0.11738078 - samples/sec: 44.44 - lr: 0.001563\n",
      "2022-05-07 02:17:20,712 epoch 84 - iter 10/52 - loss 0.08750942 - samples/sec: 48.90 - lr: 0.001563\n",
      "2022-05-07 02:17:21,635 epoch 84 - iter 15/52 - loss 0.07241373 - samples/sec: 47.68 - lr: 0.001563\n",
      "2022-05-07 02:17:22,550 epoch 84 - iter 20/52 - loss 0.07197928 - samples/sec: 47.33 - lr: 0.001563\n",
      "2022-05-07 02:17:23,475 epoch 84 - iter 25/52 - loss 0.07165964 - samples/sec: 45.66 - lr: 0.001563\n",
      "2022-05-07 02:17:24,389 epoch 84 - iter 30/52 - loss 0.07115741 - samples/sec: 48.36 - lr: 0.001563\n",
      "2022-05-07 02:17:25,341 epoch 84 - iter 35/52 - loss 0.07313087 - samples/sec: 44.10 - lr: 0.001563\n",
      "2022-05-07 02:17:26,240 epoch 84 - iter 40/52 - loss 0.07597641 - samples/sec: 48.02 - lr: 0.001563\n",
      "2022-05-07 02:17:27,129 epoch 84 - iter 45/52 - loss 0.07588653 - samples/sec: 49.88 - lr: 0.001563\n",
      "2022-05-07 02:17:28,032 epoch 84 - iter 50/52 - loss 0.07819173 - samples/sec: 47.79 - lr: 0.001563\n",
      "2022-05-07 02:17:28,687 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:17:28,688 EPOCH 84 done: loss 0.0773 - lr 0.001563\n",
      "2022-05-07 02:17:28,688 BAD EPOCHS (no improvement): 3\n",
      "2022-05-07 02:17:30,124 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:17:30,558 epoch 85 - iter 5/52 - loss 0.07074402 - samples/sec: 46.35 - lr: 0.001563\n",
      "2022-05-07 02:17:31,473 epoch 85 - iter 10/52 - loss 0.07396385 - samples/sec: 48.48 - lr: 0.001563\n",
      "2022-05-07 02:17:32,378 epoch 85 - iter 15/52 - loss 0.07129558 - samples/sec: 49.69 - lr: 0.001563\n",
      "2022-05-07 02:17:33,323 epoch 85 - iter 20/52 - loss 0.07643485 - samples/sec: 44.49 - lr: 0.001563\n",
      "2022-05-07 02:17:34,251 epoch 85 - iter 25/52 - loss 0.08069623 - samples/sec: 46.03 - lr: 0.001563\n",
      "2022-05-07 02:17:35,147 epoch 85 - iter 30/52 - loss 0.09412303 - samples/sec: 49.26 - lr: 0.001563\n",
      "2022-05-07 02:17:36,050 epoch 85 - iter 35/52 - loss 0.08907550 - samples/sec: 49.32 - lr: 0.001563\n",
      "2022-05-07 02:17:36,958 epoch 85 - iter 40/52 - loss 0.08873321 - samples/sec: 49.56 - lr: 0.001563\n",
      "2022-05-07 02:17:37,874 epoch 85 - iter 45/52 - loss 0.09132874 - samples/sec: 47.00 - lr: 0.001563\n",
      "2022-05-07 02:17:38,822 epoch 85 - iter 50/52 - loss 0.08638003 - samples/sec: 44.49 - lr: 0.001563\n",
      "2022-05-07 02:17:39,488 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:17:39,489 EPOCH 85 done: loss 0.0869 - lr 0.001563\n",
      "2022-05-07 02:17:39,490 Epoch    85: reducing learning rate of group 0 to 7.8125e-04.\n",
      "2022-05-07 02:17:39,490 BAD EPOCHS (no improvement): 4\n",
      "2022-05-07 02:17:40,931 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:17:41,416 epoch 86 - iter 5/52 - loss 0.08555639 - samples/sec: 41.36 - lr: 0.000781\n",
      "2022-05-07 02:17:42,326 epoch 86 - iter 10/52 - loss 0.07810143 - samples/sec: 49.38 - lr: 0.000781\n",
      "2022-05-07 02:17:43,263 epoch 86 - iter 15/52 - loss 0.06779296 - samples/sec: 45.30 - lr: 0.000781\n",
      "2022-05-07 02:17:44,174 epoch 86 - iter 20/52 - loss 0.07242654 - samples/sec: 48.25 - lr: 0.000781\n",
      "2022-05-07 02:17:45,124 epoch 86 - iter 25/52 - loss 0.07094700 - samples/sec: 45.40 - lr: 0.000781\n",
      "2022-05-07 02:17:46,037 epoch 86 - iter 30/52 - loss 0.07464020 - samples/sec: 47.62 - lr: 0.000781\n",
      "2022-05-07 02:17:46,964 epoch 86 - iter 35/52 - loss 0.08557287 - samples/sec: 46.46 - lr: 0.000781\n",
      "2022-05-07 02:17:47,890 epoch 86 - iter 40/52 - loss 0.08613525 - samples/sec: 47.06 - lr: 0.000781\n",
      "2022-05-07 02:17:48,790 epoch 86 - iter 45/52 - loss 0.08222479 - samples/sec: 50.06 - lr: 0.000781\n",
      "2022-05-07 02:17:49,707 epoch 86 - iter 50/52 - loss 0.08065349 - samples/sec: 46.95 - lr: 0.000781\n",
      "2022-05-07 02:17:50,359 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:17:50,360 EPOCH 86 done: loss 0.0799 - lr 0.000781\n",
      "2022-05-07 02:17:50,361 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 02:17:51,823 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:17:52,283 epoch 87 - iter 5/52 - loss 0.09890967 - samples/sec: 43.67 - lr: 0.000781\n",
      "2022-05-07 02:17:53,180 epoch 87 - iter 10/52 - loss 0.11031025 - samples/sec: 49.69 - lr: 0.000781\n",
      "2022-05-07 02:17:54,097 epoch 87 - iter 15/52 - loss 0.09892841 - samples/sec: 47.62 - lr: 0.000781\n",
      "2022-05-07 02:17:55,013 epoch 87 - iter 20/52 - loss 0.08898101 - samples/sec: 47.45 - lr: 0.000781\n",
      "2022-05-07 02:17:55,992 epoch 87 - iter 25/52 - loss 0.08157371 - samples/sec: 46.56 - lr: 0.000781\n",
      "2022-05-07 02:17:56,933 epoch 87 - iter 30/52 - loss 0.07694286 - samples/sec: 44.74 - lr: 0.000781\n",
      "2022-05-07 02:17:57,846 epoch 87 - iter 35/52 - loss 0.07397139 - samples/sec: 47.62 - lr: 0.000781\n",
      "2022-05-07 02:17:58,757 epoch 87 - iter 40/52 - loss 0.07499046 - samples/sec: 47.39 - lr: 0.000781\n",
      "2022-05-07 02:17:59,660 epoch 87 - iter 45/52 - loss 0.07580621 - samples/sec: 46.72 - lr: 0.000781\n",
      "2022-05-07 02:18:00,545 epoch 87 - iter 50/52 - loss 0.07533297 - samples/sec: 48.90 - lr: 0.000781\n",
      "2022-05-07 02:18:01,175 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:18:01,175 EPOCH 87 done: loss 0.0755 - lr 0.000781\n",
      "2022-05-07 02:18:01,176 BAD EPOCHS (no improvement): 2\n",
      "2022-05-07 02:18:02,601 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:18:03,047 epoch 88 - iter 5/52 - loss 0.08747578 - samples/sec: 45.02 - lr: 0.000781\n",
      "2022-05-07 02:18:03,930 epoch 88 - iter 10/52 - loss 0.09636916 - samples/sec: 51.09 - lr: 0.000781\n",
      "2022-05-07 02:18:04,829 epoch 88 - iter 15/52 - loss 0.09307861 - samples/sec: 48.54 - lr: 0.000781\n",
      "2022-05-07 02:18:05,746 epoch 88 - iter 20/52 - loss 0.08230706 - samples/sec: 47.34 - lr: 0.000781\n",
      "2022-05-07 02:18:06,693 epoch 88 - iter 25/52 - loss 0.07652868 - samples/sec: 45.92 - lr: 0.000781\n",
      "2022-05-07 02:18:07,608 epoch 88 - iter 30/52 - loss 0.08512310 - samples/sec: 48.43 - lr: 0.000781\n",
      "2022-05-07 02:18:08,524 epoch 88 - iter 35/52 - loss 0.09036084 - samples/sec: 44.97 - lr: 0.000781\n",
      "2022-05-07 02:18:09,416 epoch 88 - iter 40/52 - loss 0.08611400 - samples/sec: 48.60 - lr: 0.000781\n",
      "2022-05-07 02:18:10,309 epoch 88 - iter 45/52 - loss 0.08542556 - samples/sec: 47.18 - lr: 0.000781\n",
      "2022-05-07 02:18:11,208 epoch 88 - iter 50/52 - loss 0.08340385 - samples/sec: 46.76 - lr: 0.000781\n",
      "2022-05-07 02:18:11,849 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:18:11,850 EPOCH 88 done: loss 0.0832 - lr 0.000781\n",
      "2022-05-07 02:18:11,850 BAD EPOCHS (no improvement): 3\n",
      "2022-05-07 02:18:13,215 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:18:13,660 epoch 89 - iter 5/52 - loss 0.04860148 - samples/sec: 45.05 - lr: 0.000781\n",
      "2022-05-07 02:18:14,538 epoch 89 - iter 10/52 - loss 0.04519400 - samples/sec: 51.41 - lr: 0.000781\n",
      "2022-05-07 02:18:15,413 epoch 89 - iter 15/52 - loss 0.05189202 - samples/sec: 49.88 - lr: 0.000781\n",
      "2022-05-07 02:18:16,307 epoch 89 - iter 20/52 - loss 0.07323431 - samples/sec: 47.81 - lr: 0.000781\n",
      "2022-05-07 02:18:17,201 epoch 89 - iter 25/52 - loss 0.08710440 - samples/sec: 49.02 - lr: 0.000781\n",
      "2022-05-07 02:18:18,099 epoch 89 - iter 30/52 - loss 0.08684534 - samples/sec: 49.08 - lr: 0.000781\n",
      "2022-05-07 02:18:18,996 epoch 89 - iter 35/52 - loss 0.08696077 - samples/sec: 49.32 - lr: 0.000781\n",
      "2022-05-07 02:18:19,903 epoch 89 - iter 40/52 - loss 0.08456883 - samples/sec: 48.88 - lr: 0.000781\n",
      "2022-05-07 02:18:20,810 epoch 89 - iter 45/52 - loss 0.08410397 - samples/sec: 46.35 - lr: 0.000781\n",
      "2022-05-07 02:18:21,694 epoch 89 - iter 50/52 - loss 0.08244408 - samples/sec: 49.38 - lr: 0.000781\n",
      "2022-05-07 02:18:22,332 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:18:22,333 EPOCH 89 done: loss 0.0834 - lr 0.000781\n",
      "2022-05-07 02:18:22,334 Epoch    89: reducing learning rate of group 0 to 3.9063e-04.\n",
      "2022-05-07 02:18:22,334 BAD EPOCHS (no improvement): 4\n",
      "2022-05-07 02:18:23,746 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:18:24,145 epoch 90 - iter 5/52 - loss 0.06274536 - samples/sec: 50.29 - lr: 0.000391\n",
      "2022-05-07 02:18:25,033 epoch 90 - iter 10/52 - loss 0.06669439 - samples/sec: 50.12 - lr: 0.000391\n",
      "2022-05-07 02:18:25,936 epoch 90 - iter 15/52 - loss 0.07013120 - samples/sec: 47.68 - lr: 0.000391\n",
      "2022-05-07 02:18:26,851 epoch 90 - iter 20/52 - loss 0.07543407 - samples/sec: 46.78 - lr: 0.000391\n",
      "2022-05-07 02:18:27,779 epoch 90 - iter 25/52 - loss 0.07117248 - samples/sec: 46.35 - lr: 0.000391\n",
      "2022-05-07 02:18:28,706 epoch 90 - iter 30/52 - loss 0.07017374 - samples/sec: 47.28 - lr: 0.000391\n",
      "2022-05-07 02:18:29,622 epoch 90 - iter 35/52 - loss 0.06935553 - samples/sec: 46.95 - lr: 0.000391\n",
      "2022-05-07 02:18:30,534 epoch 90 - iter 40/52 - loss 0.06890690 - samples/sec: 46.14 - lr: 0.000391\n",
      "2022-05-07 02:18:31,469 epoch 90 - iter 45/52 - loss 0.07499172 - samples/sec: 46.67 - lr: 0.000391\n",
      "2022-05-07 02:18:32,403 epoch 90 - iter 50/52 - loss 0.07424482 - samples/sec: 45.45 - lr: 0.000391\n",
      "2022-05-07 02:18:33,077 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:18:33,078 EPOCH 90 done: loss 0.0750 - lr 0.000391\n",
      "2022-05-07 02:18:33,078 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 02:18:34,530 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:18:34,987 epoch 91 - iter 5/52 - loss 0.05320805 - samples/sec: 43.91 - lr: 0.000391\n",
      "2022-05-07 02:18:35,901 epoch 91 - iter 10/52 - loss 0.05708315 - samples/sec: 47.96 - lr: 0.000391\n",
      "2022-05-07 02:18:36,807 epoch 91 - iter 15/52 - loss 0.06107819 - samples/sec: 48.90 - lr: 0.000391\n",
      "2022-05-07 02:18:37,733 epoch 91 - iter 20/52 - loss 0.06915721 - samples/sec: 46.57 - lr: 0.000391\n",
      "2022-05-07 02:18:38,650 epoch 91 - iter 25/52 - loss 0.06739374 - samples/sec: 48.31 - lr: 0.000391\n",
      "2022-05-07 02:18:39,563 epoch 91 - iter 30/52 - loss 0.06849878 - samples/sec: 47.45 - lr: 0.000391\n",
      "2022-05-07 02:18:40,463 epoch 91 - iter 35/52 - loss 0.07330149 - samples/sec: 47.56 - lr: 0.000391\n",
      "2022-05-07 02:18:41,396 epoch 91 - iter 40/52 - loss 0.07238642 - samples/sec: 46.84 - lr: 0.000391\n",
      "2022-05-07 02:18:42,305 epoch 91 - iter 45/52 - loss 0.06958643 - samples/sec: 48.96 - lr: 0.000391\n",
      "2022-05-07 02:18:43,190 epoch 91 - iter 50/52 - loss 0.07400343 - samples/sec: 49.32 - lr: 0.000391\n",
      "2022-05-07 02:18:43,858 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:18:43,858 EPOCH 91 done: loss 0.0721 - lr 0.000391\n",
      "2022-05-07 02:18:43,859 BAD EPOCHS (no improvement): 0\n",
      "2022-05-07 02:18:45,302 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:18:45,733 epoch 92 - iter 5/52 - loss 0.10387514 - samples/sec: 46.57 - lr: 0.000391\n",
      "2022-05-07 02:18:46,625 epoch 92 - iter 10/52 - loss 0.09849815 - samples/sec: 50.13 - lr: 0.000391\n",
      "2022-05-07 02:18:47,529 epoch 92 - iter 15/52 - loss 0.11159095 - samples/sec: 47.84 - lr: 0.000391\n",
      "2022-05-07 02:18:48,449 epoch 92 - iter 20/52 - loss 0.10097117 - samples/sec: 46.57 - lr: 0.000391\n",
      "2022-05-07 02:18:49,390 epoch 92 - iter 25/52 - loss 0.09723778 - samples/sec: 45.15 - lr: 0.000391\n",
      "2022-05-07 02:18:50,300 epoch 92 - iter 30/52 - loss 0.09124990 - samples/sec: 47.85 - lr: 0.000391\n",
      "2022-05-07 02:18:51,237 epoch 92 - iter 35/52 - loss 0.08483450 - samples/sec: 45.20 - lr: 0.000391\n",
      "2022-05-07 02:18:52,174 epoch 92 - iter 40/52 - loss 0.08531924 - samples/sec: 46.84 - lr: 0.000391\n",
      "2022-05-07 02:18:53,085 epoch 92 - iter 45/52 - loss 0.08439639 - samples/sec: 48.02 - lr: 0.000391\n",
      "2022-05-07 02:18:54,008 epoch 92 - iter 50/52 - loss 0.08699299 - samples/sec: 48.31 - lr: 0.000391\n",
      "2022-05-07 02:18:54,671 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:18:54,671 EPOCH 92 done: loss 0.0861 - lr 0.000391\n",
      "2022-05-07 02:18:54,672 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 02:18:56,106 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:18:56,546 epoch 93 - iter 5/52 - loss 0.07334046 - samples/sec: 45.66 - lr: 0.000391\n",
      "2022-05-07 02:18:57,480 epoch 93 - iter 10/52 - loss 0.07884650 - samples/sec: 46.40 - lr: 0.000391\n",
      "2022-05-07 02:18:58,382 epoch 93 - iter 15/52 - loss 0.08185293 - samples/sec: 48.02 - lr: 0.000391\n",
      "2022-05-07 02:18:59,297 epoch 93 - iter 20/52 - loss 0.09204983 - samples/sec: 46.62 - lr: 0.000391\n",
      "2022-05-07 02:19:00,231 epoch 93 - iter 25/52 - loss 0.08593771 - samples/sec: 47.17 - lr: 0.000391\n",
      "2022-05-07 02:19:01,152 epoch 93 - iter 30/52 - loss 0.09093654 - samples/sec: 47.73 - lr: 0.000391\n",
      "2022-05-07 02:19:02,070 epoch 93 - iter 35/52 - loss 0.08692525 - samples/sec: 47.62 - lr: 0.000391\n",
      "2022-05-07 02:19:03,020 epoch 93 - iter 40/52 - loss 0.08638233 - samples/sec: 43.96 - lr: 0.000391\n",
      "2022-05-07 02:19:03,946 epoch 93 - iter 45/52 - loss 0.08942739 - samples/sec: 45.20 - lr: 0.000391\n",
      "2022-05-07 02:19:04,889 epoch 93 - iter 50/52 - loss 0.08634419 - samples/sec: 44.64 - lr: 0.000391\n",
      "2022-05-07 02:19:05,537 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:19:05,538 EPOCH 93 done: loss 0.0885 - lr 0.000391\n",
      "2022-05-07 02:19:05,539 BAD EPOCHS (no improvement): 2\n",
      "2022-05-07 02:19:06,970 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:19:07,421 epoch 94 - iter 5/52 - loss 0.05951986 - samples/sec: 44.49 - lr: 0.000391\n",
      "2022-05-07 02:19:08,352 epoch 94 - iter 10/52 - loss 0.06900380 - samples/sec: 45.51 - lr: 0.000391\n",
      "2022-05-07 02:19:09,288 epoch 94 - iter 15/52 - loss 0.05947054 - samples/sec: 45.40 - lr: 0.000391\n",
      "2022-05-07 02:19:10,228 epoch 94 - iter 20/52 - loss 0.06892992 - samples/sec: 45.30 - lr: 0.000391\n",
      "2022-05-07 02:19:11,129 epoch 94 - iter 25/52 - loss 0.06608545 - samples/sec: 48.66 - lr: 0.000391\n",
      "2022-05-07 02:19:12,040 epoch 94 - iter 30/52 - loss 0.07254876 - samples/sec: 48.02 - lr: 0.000391\n",
      "2022-05-07 02:19:12,947 epoch 94 - iter 35/52 - loss 0.07580520 - samples/sec: 48.19 - lr: 0.000391\n",
      "2022-05-07 02:19:13,844 epoch 94 - iter 40/52 - loss 0.07596009 - samples/sec: 48.78 - lr: 0.000391\n",
      "2022-05-07 02:19:14,743 epoch 94 - iter 45/52 - loss 0.07573052 - samples/sec: 49.20 - lr: 0.000391\n",
      "2022-05-07 02:19:15,648 epoch 94 - iter 50/52 - loss 0.07931876 - samples/sec: 47.84 - lr: 0.000391\n",
      "2022-05-07 02:19:16,331 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:19:16,332 EPOCH 94 done: loss 0.0791 - lr 0.000391\n",
      "2022-05-07 02:19:16,333 BAD EPOCHS (no improvement): 3\n",
      "2022-05-07 02:19:17,780 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:19:18,218 epoch 95 - iter 5/52 - loss 0.10361070 - samples/sec: 45.77 - lr: 0.000391\n",
      "2022-05-07 02:19:19,140 epoch 95 - iter 10/52 - loss 0.09602822 - samples/sec: 46.51 - lr: 0.000391\n",
      "2022-05-07 02:19:20,052 epoch 95 - iter 15/52 - loss 0.08272232 - samples/sec: 47.17 - lr: 0.000391\n",
      "2022-05-07 02:19:20,964 epoch 95 - iter 20/52 - loss 0.09119025 - samples/sec: 46.84 - lr: 0.000391\n",
      "2022-05-07 02:19:21,890 epoch 95 - iter 25/52 - loss 0.08363631 - samples/sec: 46.89 - lr: 0.000391\n",
      "2022-05-07 02:19:22,794 epoch 95 - iter 30/52 - loss 0.08715129 - samples/sec: 47.56 - lr: 0.000391\n",
      "2022-05-07 02:19:23,714 epoch 95 - iter 35/52 - loss 0.08448828 - samples/sec: 47.45 - lr: 0.000391\n",
      "2022-05-07 02:19:24,640 epoch 95 - iter 40/52 - loss 0.08421318 - samples/sec: 47.79 - lr: 0.000391\n",
      "2022-05-07 02:19:25,558 epoch 95 - iter 45/52 - loss 0.08790084 - samples/sec: 46.35 - lr: 0.000391\n",
      "2022-05-07 02:19:26,444 epoch 95 - iter 50/52 - loss 0.08731228 - samples/sec: 50.25 - lr: 0.000391\n",
      "2022-05-07 02:19:27,110 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:19:27,111 EPOCH 95 done: loss 0.0870 - lr 0.000391\n",
      "2022-05-07 02:19:27,111 Epoch    95: reducing learning rate of group 0 to 1.9531e-04.\n",
      "2022-05-07 02:19:27,112 BAD EPOCHS (no improvement): 4\n",
      "2022-05-07 02:19:28,575 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:19:28,993 epoch 96 - iter 5/52 - loss 0.06890808 - samples/sec: 48.13 - lr: 0.000195\n",
      "2022-05-07 02:19:29,901 epoch 96 - iter 10/52 - loss 0.08442213 - samples/sec: 49.08 - lr: 0.000195\n",
      "2022-05-07 02:19:30,824 epoch 96 - iter 15/52 - loss 0.08927586 - samples/sec: 48.48 - lr: 0.000195\n",
      "2022-05-07 02:19:31,736 epoch 96 - iter 20/52 - loss 0.09254849 - samples/sec: 47.33 - lr: 0.000195\n",
      "2022-05-07 02:19:32,638 epoch 96 - iter 25/52 - loss 0.09235026 - samples/sec: 48.48 - lr: 0.000195\n",
      "2022-05-07 02:19:33,563 epoch 96 - iter 30/52 - loss 0.08736313 - samples/sec: 46.67 - lr: 0.000195\n",
      "2022-05-07 02:19:34,488 epoch 96 - iter 35/52 - loss 0.08031052 - samples/sec: 45.56 - lr: 0.000195\n",
      "2022-05-07 02:19:35,393 epoch 96 - iter 40/52 - loss 0.07923166 - samples/sec: 47.56 - lr: 0.000195\n",
      "2022-05-07 02:19:36,325 epoch 96 - iter 45/52 - loss 0.07927680 - samples/sec: 45.15 - lr: 0.000195\n",
      "2022-05-07 02:19:37,315 epoch 96 - iter 50/52 - loss 0.07995062 - samples/sec: 44.00 - lr: 0.000195\n",
      "2022-05-07 02:19:37,991 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:19:37,992 EPOCH 96 done: loss 0.0779 - lr 0.000195\n",
      "2022-05-07 02:19:37,992 BAD EPOCHS (no improvement): 1\n",
      "2022-05-07 02:19:39,432 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:19:39,873 epoch 97 - iter 5/52 - loss 0.06871294 - samples/sec: 45.51 - lr: 0.000195\n",
      "2022-05-07 02:19:40,781 epoch 97 - iter 10/52 - loss 0.07565840 - samples/sec: 50.37 - lr: 0.000195\n",
      "2022-05-07 02:19:41,720 epoch 97 - iter 15/52 - loss 0.07705219 - samples/sec: 44.84 - lr: 0.000195\n",
      "2022-05-07 02:19:42,625 epoch 97 - iter 20/52 - loss 0.08257258 - samples/sec: 47.40 - lr: 0.000195\n",
      "2022-05-07 02:19:43,523 epoch 97 - iter 25/52 - loss 0.07895098 - samples/sec: 49.08 - lr: 0.000195\n",
      "2022-05-07 02:19:44,420 epoch 97 - iter 30/52 - loss 0.07661749 - samples/sec: 48.37 - lr: 0.000195\n",
      "2022-05-07 02:19:45,319 epoch 97 - iter 35/52 - loss 0.07613249 - samples/sec: 48.54 - lr: 0.000195\n",
      "2022-05-07 02:19:46,208 epoch 97 - iter 40/52 - loss 0.07958827 - samples/sec: 50.31 - lr: 0.000195\n",
      "2022-05-07 02:19:47,104 epoch 97 - iter 45/52 - loss 0.07764205 - samples/sec: 50.06 - lr: 0.000195\n",
      "2022-05-07 02:19:48,036 epoch 97 - iter 50/52 - loss 0.07971827 - samples/sec: 46.13 - lr: 0.000195\n",
      "2022-05-07 02:19:48,693 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:19:48,693 EPOCH 97 done: loss 0.0800 - lr 0.000195\n",
      "2022-05-07 02:19:48,694 BAD EPOCHS (no improvement): 2\n",
      "2022-05-07 02:19:50,075 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:19:50,511 epoch 98 - iter 5/52 - loss 0.06116274 - samples/sec: 46.03 - lr: 0.000195\n",
      "2022-05-07 02:19:51,388 epoch 98 - iter 10/52 - loss 0.06797908 - samples/sec: 50.13 - lr: 0.000195\n",
      "2022-05-07 02:19:52,278 epoch 98 - iter 15/52 - loss 0.06240722 - samples/sec: 48.31 - lr: 0.000195\n",
      "2022-05-07 02:19:53,153 epoch 98 - iter 20/52 - loss 0.06976456 - samples/sec: 51.75 - lr: 0.000195\n",
      "2022-05-07 02:19:54,047 epoch 98 - iter 25/52 - loss 0.07249827 - samples/sec: 48.19 - lr: 0.000195\n",
      "2022-05-07 02:19:54,919 epoch 98 - iter 30/52 - loss 0.08344374 - samples/sec: 50.32 - lr: 0.000195\n",
      "2022-05-07 02:19:55,828 epoch 98 - iter 35/52 - loss 0.07986702 - samples/sec: 46.62 - lr: 0.000195\n",
      "2022-05-07 02:19:56,729 epoch 98 - iter 40/52 - loss 0.07421804 - samples/sec: 47.45 - lr: 0.000195\n",
      "2022-05-07 02:19:57,629 epoch 98 - iter 45/52 - loss 0.07723845 - samples/sec: 47.79 - lr: 0.000195\n",
      "2022-05-07 02:19:58,505 epoch 98 - iter 50/52 - loss 0.07905923 - samples/sec: 49.24 - lr: 0.000195\n",
      "2022-05-07 02:19:59,167 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:19:59,167 EPOCH 98 done: loss 0.0795 - lr 0.000195\n",
      "2022-05-07 02:19:59,168 BAD EPOCHS (no improvement): 3\n",
      "2022-05-07 02:20:00,580 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:20:01,040 epoch 99 - iter 5/52 - loss 0.09834620 - samples/sec: 43.76 - lr: 0.000195\n",
      "2022-05-07 02:20:01,931 epoch 99 - iter 10/52 - loss 0.08860211 - samples/sec: 49.44 - lr: 0.000195\n",
      "2022-05-07 02:20:02,817 epoch 99 - iter 15/52 - loss 0.08504004 - samples/sec: 47.92 - lr: 0.000195\n",
      "2022-05-07 02:20:03,706 epoch 99 - iter 20/52 - loss 0.08833149 - samples/sec: 48.01 - lr: 0.000195\n",
      "2022-05-07 02:20:04,579 epoch 99 - iter 25/52 - loss 0.08301786 - samples/sec: 50.13 - lr: 0.000195\n",
      "2022-05-07 02:20:05,505 epoch 99 - iter 30/52 - loss 0.07916507 - samples/sec: 47.17 - lr: 0.000195\n",
      "2022-05-07 02:20:06,392 epoch 99 - iter 35/52 - loss 0.08129501 - samples/sec: 47.85 - lr: 0.000195\n",
      "2022-05-07 02:20:07,289 epoch 99 - iter 40/52 - loss 0.08211587 - samples/sec: 47.90 - lr: 0.000195\n",
      "2022-05-07 02:20:08,220 epoch 99 - iter 45/52 - loss 0.08335894 - samples/sec: 47.90 - lr: 0.000195\n",
      "2022-05-07 02:20:09,161 epoch 99 - iter 50/52 - loss 0.08190629 - samples/sec: 47.23 - lr: 0.000195\n",
      "2022-05-07 02:20:09,844 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:20:09,844 EPOCH 99 done: loss 0.0808 - lr 0.000195\n",
      "2022-05-07 02:20:09,845 Epoch    99: reducing learning rate of group 0 to 9.7656e-05.\n",
      "2022-05-07 02:20:09,846 BAD EPOCHS (no improvement): 4\n",
      "2022-05-07 02:20:11,274 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:20:11,274 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:20:11,275 learning rate too small - quitting training!\n",
      "2022-05-07 02:20:11,276 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:20:12,826 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:20:12,828 Testing using last state of model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 17/17 [00:00<00:00, 18.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-07 02:20:13,766 Evaluating as a multi-label problem: False\n",
      "2022-05-07 02:20:13,773 0.6947\t0.7674\t0.7293\t0.5739\n",
      "2022-05-07 02:20:13,774 \n",
      "Results:\n",
      "- F-score (micro) 0.7293\n",
      "- F-score (macro) 0.7301\n",
      "- Accuracy 0.5739\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         DIS     0.6711    0.7969    0.7286        64\n",
      "        DRUG     0.7895    0.6818    0.7317        22\n",
      "\n",
      "   micro avg     0.6947    0.7674    0.7293        86\n",
      "   macro avg     0.7303    0.7393    0.7301        86\n",
      "weighted avg     0.7013    0.7674    0.7294        86\n",
      "\n",
      "2022-05-07 02:20:13,774 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:20:13,777 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:20:13,777 WARNING: No LOSS found for test split in this data.\n",
      "2022-05-07 02:20:13,777 Are you sure you want to plot LOSS and not another value?\n",
      "2022-05-07 02:20:13,778 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:20:13,793 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-07 02:20:13,794 WARNING: No F1 found for test split in this data.\n",
      "2022-05-07 02:20:13,795 Are you sure you want to plot F1 and not another value?\n",
      "2022-05-07 02:20:13,795 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-07 02:20:14,399 Loss and F1 plots are saved in ..\\resources\\taggers\\FA_Micromed_glove_roberta\\training.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = 'Micromed'\n",
    "selected_embeddings = defaultdict(lambda: 0, {'glove':1, 'roberta':1})\n",
    "train(model, selected_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c6b58fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-08 01:54:21,145 Reading data from ..\\data\\cadec\n",
      "2022-05-08 01:54:21,146 Train: ..\\data\\cadec\\NER_CADEC_labels_train.csv\n",
      "2022-05-08 01:54:21,146 Dev: ..\\data\\cadec\\NER_CADEC_labels_dev.csv\n",
      "2022-05-08 01:54:21,147 Test: ..\\data\\cadec\\NER_CADEC_labels_test.csv\n",
      "2022-05-08 01:54:22,301 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "434it [00:00, 39370.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-08 01:54:22,314 Dictionary created for label 'ner' with 3 values: DRUG (seen 866 times), DIS (seen 189 times)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-08 01:54:31,542 SequenceTagger predicts: Dictionary with 9 tags: O, S-DRUG, B-DRUG, E-DRUG, I-DRUG, S-DIS, B-DIS, E-DIS, I-DIS\n",
      "2022-05-08 01:54:31,566 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 01:54:31,568 Model: \"SequenceTagger(\n",
      "  (embeddings): StackedEmbeddings(\n",
      "    (list_embedding_0): WordEmbeddings(\n",
      "      'glove'\n",
      "      (embedding): Embedding(400001, 100)\n",
      "    )\n",
      "    (list_embedding_1): TransformerWordEmbeddings(\n",
      "      (model): RobertaModel(\n",
      "        (embeddings): RobertaEmbeddings(\n",
      "          (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "          (token_type_embeddings): Embedding(1, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): RobertaEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (3): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (4): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (5): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (6): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (7): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (8): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (9): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (10): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (11): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (pooler): RobertaPooler(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (activation): Tanh()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=868, out_features=868, bias=True)\n",
      "  (rnn): LSTM(868, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=11, bias=True)\n",
      "  (loss_function): ViterbiLoss()\n",
      "  (crf): CRF()\n",
      ")\"\n",
      "2022-05-08 01:54:31,569 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 01:54:31,570 Corpus: \"Corpus: 434 train + 204 dev + 205 test sentences\"\n",
      "2022-05-08 01:54:31,572 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 01:54:31,572 Parameters:\n",
      "2022-05-08 01:54:31,573  - learning_rate: \"0.100000\"\n",
      "2022-05-08 01:54:31,573  - mini_batch_size: \"4\"\n",
      "2022-05-08 01:54:31,573  - patience: \"3\"\n",
      "2022-05-08 01:54:31,574  - anneal_factor: \"0.5\"\n",
      "2022-05-08 01:54:31,575  - max_epochs: \"200\"\n",
      "2022-05-08 01:54:31,575  - shuffle: \"True\"\n",
      "2022-05-08 01:54:31,576  - train_with_dev: \"True\"\n",
      "2022-05-08 01:54:31,577  - batch_growth_annealing: \"False\"\n",
      "2022-05-08 01:54:31,578 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 01:54:31,579 Model training base path: \"..\\resources\\taggers\\FA_CADEC_glove_roberta\"\n",
      "2022-05-08 01:54:31,580 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 01:54:31,582 Device: cuda:0\n",
      "2022-05-08 01:54:31,582 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 01:54:31,583 Embeddings storage mode: cpu\n",
      "2022-05-08 01:54:31,584 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 01:54:36,468 epoch 1 - iter 16/160 - loss 0.74812359 - samples/sec: 13.11 - lr: 0.100000\n",
      "2022-05-08 01:54:40,891 epoch 1 - iter 32/160 - loss 0.55078641 - samples/sec: 16.27 - lr: 0.100000\n",
      "2022-05-08 01:54:47,472 epoch 1 - iter 48/160 - loss 0.39127810 - samples/sec: 10.55 - lr: 0.100000\n",
      "2022-05-08 01:54:53,785 epoch 1 - iter 64/160 - loss 0.30532425 - samples/sec: 11.02 - lr: 0.100000\n",
      "2022-05-08 01:55:00,179 epoch 1 - iter 80/160 - loss 0.25135137 - samples/sec: 10.88 - lr: 0.100000\n",
      "2022-05-08 01:55:06,174 epoch 1 - iter 96/160 - loss 0.22010625 - samples/sec: 11.62 - lr: 0.100000\n",
      "2022-05-08 01:55:11,781 epoch 1 - iter 112/160 - loss 0.19935411 - samples/sec: 12.51 - lr: 0.100000\n",
      "2022-05-08 01:55:17,876 epoch 1 - iter 128/160 - loss 0.18197674 - samples/sec: 11.39 - lr: 0.100000\n",
      "2022-05-08 01:55:23,716 epoch 1 - iter 144/160 - loss 0.16957248 - samples/sec: 11.93 - lr: 0.100000\n",
      "2022-05-08 01:55:30,116 epoch 1 - iter 160/160 - loss 0.15679634 - samples/sec: 10.85 - lr: 0.100000\n",
      "2022-05-08 01:55:30,613 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 01:55:30,614 EPOCH 1 done: loss 0.1568 - lr 0.100000\n",
      "2022-05-08 01:55:30,614 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 01:55:31,931 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 01:55:36,507 epoch 2 - iter 16/160 - loss 0.10800699 - samples/sec: 13.99 - lr: 0.100000\n",
      "2022-05-08 01:55:41,588 epoch 2 - iter 32/160 - loss 0.10950221 - samples/sec: 13.93 - lr: 0.100000\n",
      "2022-05-08 01:55:47,643 epoch 2 - iter 48/160 - loss 0.10539311 - samples/sec: 11.48 - lr: 0.100000\n",
      "2022-05-08 01:55:53,283 epoch 2 - iter 64/160 - loss 0.10112012 - samples/sec: 12.40 - lr: 0.100000\n",
      "2022-05-08 01:55:58,500 epoch 2 - iter 80/160 - loss 0.10689462 - samples/sec: 13.59 - lr: 0.100000\n",
      "2022-05-08 01:56:03,921 epoch 2 - iter 96/160 - loss 0.10212241 - samples/sec: 12.99 - lr: 0.100000\n",
      "2022-05-08 01:56:09,194 epoch 2 - iter 112/160 - loss 0.09767642 - samples/sec: 13.45 - lr: 0.100000\n",
      "2022-05-08 01:56:15,064 epoch 2 - iter 128/160 - loss 0.09314248 - samples/sec: 11.88 - lr: 0.100000\n",
      "2022-05-08 01:56:20,346 epoch 2 - iter 144/160 - loss 0.09447569 - samples/sec: 13.35 - lr: 0.100000\n",
      "2022-05-08 01:56:25,840 epoch 2 - iter 160/160 - loss 0.09462313 - samples/sec: 12.78 - lr: 0.100000\n",
      "2022-05-08 01:56:26,343 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 01:56:26,344 EPOCH 2 done: loss 0.0946 - lr 0.100000\n",
      "2022-05-08 01:56:26,345 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 01:56:27,676 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 01:56:33,157 epoch 3 - iter 16/160 - loss 0.09565672 - samples/sec: 11.68 - lr: 0.100000\n",
      "2022-05-08 01:56:38,499 epoch 3 - iter 32/160 - loss 0.09453153 - samples/sec: 13.18 - lr: 0.100000\n",
      "2022-05-08 01:56:43,384 epoch 3 - iter 48/160 - loss 0.08927735 - samples/sec: 14.54 - lr: 0.100000\n",
      "2022-05-08 01:56:48,942 epoch 3 - iter 64/160 - loss 0.08687701 - samples/sec: 12.67 - lr: 0.100000\n",
      "2022-05-08 01:56:54,188 epoch 3 - iter 80/160 - loss 0.08457536 - samples/sec: 13.56 - lr: 0.100000\n",
      "2022-05-08 01:56:59,415 epoch 3 - iter 96/160 - loss 0.08311706 - samples/sec: 13.49 - lr: 0.100000\n",
      "2022-05-08 01:57:04,783 epoch 3 - iter 112/160 - loss 0.08238675 - samples/sec: 13.10 - lr: 0.100000\n",
      "2022-05-08 01:57:10,582 epoch 3 - iter 128/160 - loss 0.07908146 - samples/sec: 12.07 - lr: 0.100000\n",
      "2022-05-08 01:57:15,741 epoch 3 - iter 144/160 - loss 0.07831457 - samples/sec: 13.70 - lr: 0.100000\n",
      "2022-05-08 01:57:21,430 epoch 3 - iter 160/160 - loss 0.07372873 - samples/sec: 12.36 - lr: 0.100000\n",
      "2022-05-08 01:57:21,915 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 01:57:21,915 EPOCH 3 done: loss 0.0737 - lr 0.100000\n",
      "2022-05-08 01:57:21,916 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 01:57:23,240 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 01:57:28,729 epoch 4 - iter 16/160 - loss 0.07800322 - samples/sec: 11.66 - lr: 0.100000\n",
      "2022-05-08 01:57:34,564 epoch 4 - iter 32/160 - loss 0.06899416 - samples/sec: 12.00 - lr: 0.100000\n",
      "2022-05-08 01:57:40,455 epoch 4 - iter 48/160 - loss 0.06452879 - samples/sec: 11.86 - lr: 0.100000\n",
      "2022-05-08 01:57:46,002 epoch 4 - iter 64/160 - loss 0.07009896 - samples/sec: 12.69 - lr: 0.100000\n",
      "2022-05-08 01:57:51,060 epoch 4 - iter 80/160 - loss 0.06803784 - samples/sec: 14.13 - lr: 0.100000\n",
      "2022-05-08 01:57:56,447 epoch 4 - iter 96/160 - loss 0.06761878 - samples/sec: 13.04 - lr: 0.100000\n",
      "2022-05-08 01:58:01,981 epoch 4 - iter 112/160 - loss 0.06615687 - samples/sec: 12.67 - lr: 0.100000\n",
      "2022-05-08 01:58:07,299 epoch 4 - iter 128/160 - loss 0.06630540 - samples/sec: 13.33 - lr: 0.100000\n",
      "2022-05-08 01:58:12,495 epoch 4 - iter 144/160 - loss 0.06832804 - samples/sec: 13.58 - lr: 0.100000\n",
      "2022-05-08 01:58:17,326 epoch 4 - iter 160/160 - loss 0.06684745 - samples/sec: 14.72 - lr: 0.100000\n",
      "2022-05-08 01:58:17,814 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 01:58:17,815 EPOCH 4 done: loss 0.0668 - lr 0.100000\n",
      "2022-05-08 01:58:17,815 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 01:58:19,127 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 01:58:23,996 epoch 5 - iter 16/160 - loss 0.06491852 - samples/sec: 13.15 - lr: 0.100000\n",
      "2022-05-08 01:58:29,543 epoch 5 - iter 32/160 - loss 0.05658438 - samples/sec: 12.65 - lr: 0.100000\n",
      "2022-05-08 01:58:35,654 epoch 5 - iter 48/160 - loss 0.06196264 - samples/sec: 11.42 - lr: 0.100000\n",
      "2022-05-08 01:58:41,108 epoch 5 - iter 64/160 - loss 0.06243277 - samples/sec: 12.88 - lr: 0.100000\n",
      "2022-05-08 01:58:46,676 epoch 5 - iter 80/160 - loss 0.06687256 - samples/sec: 12.67 - lr: 0.100000\n",
      "2022-05-08 01:58:52,124 epoch 5 - iter 96/160 - loss 0.06479098 - samples/sec: 12.91 - lr: 0.100000\n",
      "2022-05-08 01:58:57,235 epoch 5 - iter 112/160 - loss 0.06376251 - samples/sec: 13.87 - lr: 0.100000\n",
      "2022-05-08 01:59:02,293 epoch 5 - iter 128/160 - loss 0.06454392 - samples/sec: 14.02 - lr: 0.100000\n",
      "2022-05-08 01:59:07,562 epoch 5 - iter 144/160 - loss 0.06410624 - samples/sec: 13.41 - lr: 0.100000\n",
      "2022-05-08 01:59:13,080 epoch 5 - iter 160/160 - loss 0.06291357 - samples/sec: 12.73 - lr: 0.100000\n",
      "2022-05-08 01:59:13,575 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 01:59:13,575 EPOCH 5 done: loss 0.0629 - lr 0.100000\n",
      "2022-05-08 01:59:13,576 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 01:59:14,898 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 01:59:20,316 epoch 6 - iter 16/160 - loss 0.03580060 - samples/sec: 11.82 - lr: 0.100000\n",
      "2022-05-08 01:59:26,107 epoch 6 - iter 32/160 - loss 0.03908332 - samples/sec: 12.06 - lr: 0.100000\n",
      "2022-05-08 01:59:31,750 epoch 6 - iter 48/160 - loss 0.05273754 - samples/sec: 12.46 - lr: 0.100000\n",
      "2022-05-08 01:59:36,828 epoch 6 - iter 64/160 - loss 0.05322105 - samples/sec: 13.97 - lr: 0.100000\n",
      "2022-05-08 01:59:42,102 epoch 6 - iter 80/160 - loss 0.05636836 - samples/sec: 13.40 - lr: 0.100000\n",
      "2022-05-08 01:59:47,543 epoch 6 - iter 96/160 - loss 0.05483608 - samples/sec: 12.91 - lr: 0.100000\n",
      "2022-05-08 01:59:53,189 epoch 6 - iter 112/160 - loss 0.05486664 - samples/sec: 12.43 - lr: 0.100000\n",
      "2022-05-08 01:59:58,442 epoch 6 - iter 128/160 - loss 0.05786815 - samples/sec: 13.44 - lr: 0.100000\n",
      "2022-05-08 02:00:03,848 epoch 6 - iter 144/160 - loss 0.05646413 - samples/sec: 13.00 - lr: 0.100000\n",
      "2022-05-08 02:00:08,313 epoch 6 - iter 160/160 - loss 0.05704833 - samples/sec: 16.15 - lr: 0.100000\n",
      "2022-05-08 02:00:08,792 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:00:08,792 EPOCH 6 done: loss 0.0570 - lr 0.100000\n",
      "2022-05-08 02:00:08,793 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 02:00:10,642 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:00:15,176 epoch 7 - iter 16/160 - loss 0.07361525 - samples/sec: 14.12 - lr: 0.100000\n",
      "2022-05-08 02:00:20,452 epoch 7 - iter 32/160 - loss 0.06068246 - samples/sec: 13.36 - lr: 0.100000\n",
      "2022-05-08 02:00:25,769 epoch 7 - iter 48/160 - loss 0.06104291 - samples/sec: 13.25 - lr: 0.100000\n",
      "2022-05-08 02:00:31,489 epoch 7 - iter 64/160 - loss 0.05785526 - samples/sec: 12.39 - lr: 0.100000\n",
      "2022-05-08 02:00:37,071 epoch 7 - iter 80/160 - loss 0.05654645 - samples/sec: 12.61 - lr: 0.100000\n",
      "2022-05-08 02:00:42,287 epoch 7 - iter 96/160 - loss 0.05733722 - samples/sec: 13.52 - lr: 0.100000\n",
      "2022-05-08 02:00:47,175 epoch 7 - iter 112/160 - loss 0.05617332 - samples/sec: 14.66 - lr: 0.100000\n",
      "2022-05-08 02:00:52,380 epoch 7 - iter 128/160 - loss 0.05422391 - samples/sec: 13.61 - lr: 0.100000\n",
      "2022-05-08 02:00:58,064 epoch 7 - iter 144/160 - loss 0.05353646 - samples/sec: 12.33 - lr: 0.100000\n",
      "2022-05-08 02:01:03,915 epoch 7 - iter 160/160 - loss 0.05321522 - samples/sec: 11.99 - lr: 0.100000\n",
      "2022-05-08 02:01:04,421 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:01:04,421 EPOCH 7 done: loss 0.0532 - lr 0.100000\n",
      "2022-05-08 02:01:04,422 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 02:01:05,745 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:01:11,251 epoch 8 - iter 16/160 - loss 0.05108628 - samples/sec: 11.63 - lr: 0.100000\n",
      "2022-05-08 02:01:17,137 epoch 8 - iter 32/160 - loss 0.04697079 - samples/sec: 11.84 - lr: 0.100000\n",
      "2022-05-08 02:01:22,959 epoch 8 - iter 48/160 - loss 0.04484574 - samples/sec: 12.02 - lr: 0.100000\n",
      "2022-05-08 02:01:28,109 epoch 8 - iter 64/160 - loss 0.04279296 - samples/sec: 13.73 - lr: 0.100000\n",
      "2022-05-08 02:01:32,955 epoch 8 - iter 80/160 - loss 0.04648044 - samples/sec: 14.71 - lr: 0.100000\n",
      "2022-05-08 02:01:37,998 epoch 8 - iter 96/160 - loss 0.04609155 - samples/sec: 14.03 - lr: 0.100000\n",
      "2022-05-08 02:01:43,318 epoch 8 - iter 112/160 - loss 0.04716005 - samples/sec: 13.23 - lr: 0.100000\n",
      "2022-05-08 02:01:48,300 epoch 8 - iter 128/160 - loss 0.04823954 - samples/sec: 14.28 - lr: 0.100000\n",
      "2022-05-08 02:01:54,010 epoch 8 - iter 144/160 - loss 0.04741088 - samples/sec: 12.26 - lr: 0.100000\n",
      "2022-05-08 02:01:59,162 epoch 8 - iter 160/160 - loss 0.04936746 - samples/sec: 13.73 - lr: 0.100000\n",
      "2022-05-08 02:01:59,643 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:01:59,644 EPOCH 8 done: loss 0.0494 - lr 0.100000\n",
      "2022-05-08 02:01:59,644 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 02:02:00,996 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:02:05,768 epoch 9 - iter 16/160 - loss 0.03710478 - samples/sec: 13.42 - lr: 0.100000\n",
      "2022-05-08 02:02:11,654 epoch 9 - iter 32/160 - loss 0.04267591 - samples/sec: 11.95 - lr: 0.100000\n",
      "2022-05-08 02:02:17,291 epoch 9 - iter 48/160 - loss 0.04231018 - samples/sec: 12.46 - lr: 0.100000\n",
      "2022-05-08 02:02:22,911 epoch 9 - iter 64/160 - loss 0.04459398 - samples/sec: 12.53 - lr: 0.100000\n",
      "2022-05-08 02:02:28,383 epoch 9 - iter 80/160 - loss 0.04394904 - samples/sec: 12.84 - lr: 0.100000\n",
      "2022-05-08 02:02:33,949 epoch 9 - iter 96/160 - loss 0.05389135 - samples/sec: 12.63 - lr: 0.100000\n",
      "2022-05-08 02:02:39,235 epoch 9 - iter 112/160 - loss 0.05153431 - samples/sec: 13.36 - lr: 0.100000\n",
      "2022-05-08 02:02:44,264 epoch 9 - iter 128/160 - loss 0.05316755 - samples/sec: 14.10 - lr: 0.100000\n",
      "2022-05-08 02:02:49,373 epoch 9 - iter 144/160 - loss 0.05376281 - samples/sec: 13.90 - lr: 0.100000\n",
      "2022-05-08 02:02:54,611 epoch 9 - iter 160/160 - loss 0.05205020 - samples/sec: 13.46 - lr: 0.100000\n",
      "2022-05-08 02:02:55,107 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:02:55,108 EPOCH 9 done: loss 0.0521 - lr 0.100000\n",
      "2022-05-08 02:02:55,108 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 02:02:56,400 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:03:01,606 epoch 10 - iter 16/160 - loss 0.04767315 - samples/sec: 12.30 - lr: 0.100000\n",
      "2022-05-08 02:03:07,177 epoch 10 - iter 32/160 - loss 0.04781341 - samples/sec: 12.57 - lr: 0.100000\n",
      "2022-05-08 02:03:12,639 epoch 10 - iter 48/160 - loss 0.04452485 - samples/sec: 12.90 - lr: 0.100000\n",
      "2022-05-08 02:03:18,389 epoch 10 - iter 64/160 - loss 0.04347628 - samples/sec: 12.15 - lr: 0.100000\n",
      "2022-05-08 02:03:24,092 epoch 10 - iter 80/160 - loss 0.04611677 - samples/sec: 12.30 - lr: 0.100000\n",
      "2022-05-08 02:03:28,924 epoch 10 - iter 96/160 - loss 0.04525203 - samples/sec: 14.73 - lr: 0.100000\n",
      "2022-05-08 02:03:33,742 epoch 10 - iter 112/160 - loss 0.04596330 - samples/sec: 14.78 - lr: 0.100000\n",
      "2022-05-08 02:03:39,123 epoch 10 - iter 128/160 - loss 0.04440063 - samples/sec: 13.16 - lr: 0.100000\n",
      "2022-05-08 02:03:44,971 epoch 10 - iter 144/160 - loss 0.04463228 - samples/sec: 11.97 - lr: 0.100000\n",
      "2022-05-08 02:03:50,340 epoch 10 - iter 160/160 - loss 0.04526163 - samples/sec: 13.15 - lr: 0.100000\n",
      "2022-05-08 02:03:50,837 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:03:50,838 EPOCH 10 done: loss 0.0453 - lr 0.100000\n",
      "2022-05-08 02:03:50,839 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 02:03:52,392 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:03:57,969 epoch 11 - iter 16/160 - loss 0.05208998 - samples/sec: 11.48 - lr: 0.100000\n",
      "2022-05-08 02:04:03,457 epoch 11 - iter 32/160 - loss 0.05347693 - samples/sec: 12.81 - lr: 0.100000\n",
      "2022-05-08 02:04:09,204 epoch 11 - iter 48/160 - loss 0.04603736 - samples/sec: 12.20 - lr: 0.100000\n",
      "2022-05-08 02:04:14,682 epoch 11 - iter 64/160 - loss 0.04491920 - samples/sec: 12.84 - lr: 0.100000\n",
      "2022-05-08 02:04:20,316 epoch 11 - iter 80/160 - loss 0.04227285 - samples/sec: 12.44 - lr: 0.100000\n",
      "2022-05-08 02:04:24,602 epoch 11 - iter 96/160 - loss 0.04218602 - samples/sec: 16.85 - lr: 0.100000\n",
      "2022-05-08 02:04:29,912 epoch 11 - iter 112/160 - loss 0.04268482 - samples/sec: 13.27 - lr: 0.100000\n",
      "2022-05-08 02:04:35,348 epoch 11 - iter 128/160 - loss 0.04430762 - samples/sec: 12.96 - lr: 0.100000\n",
      "2022-05-08 02:04:40,730 epoch 11 - iter 144/160 - loss 0.04519118 - samples/sec: 13.05 - lr: 0.100000\n",
      "2022-05-08 02:04:46,212 epoch 11 - iter 160/160 - loss 0.04514270 - samples/sec: 12.83 - lr: 0.100000\n",
      "2022-05-08 02:04:46,712 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:04:46,712 EPOCH 11 done: loss 0.0451 - lr 0.100000\n",
      "2022-05-08 02:04:46,713 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 02:04:48,013 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:04:52,423 epoch 12 - iter 16/160 - loss 0.03762243 - samples/sec: 14.52 - lr: 0.100000\n",
      "2022-05-08 02:04:57,591 epoch 12 - iter 32/160 - loss 0.04094022 - samples/sec: 13.78 - lr: 0.100000\n",
      "2022-05-08 02:05:02,748 epoch 12 - iter 48/160 - loss 0.04506292 - samples/sec: 13.71 - lr: 0.100000\n",
      "2022-05-08 02:05:08,286 epoch 12 - iter 64/160 - loss 0.04159971 - samples/sec: 12.68 - lr: 0.100000\n",
      "2022-05-08 02:05:13,342 epoch 12 - iter 80/160 - loss 0.04550004 - samples/sec: 13.99 - lr: 0.100000\n",
      "2022-05-08 02:05:19,410 epoch 12 - iter 96/160 - loss 0.04492956 - samples/sec: 11.52 - lr: 0.100000\n",
      "2022-05-08 02:05:24,522 epoch 12 - iter 112/160 - loss 0.04364274 - samples/sec: 13.87 - lr: 0.100000\n",
      "2022-05-08 02:05:30,145 epoch 12 - iter 128/160 - loss 0.04319438 - samples/sec: 12.48 - lr: 0.100000\n",
      "2022-05-08 02:05:35,850 epoch 12 - iter 144/160 - loss 0.04321274 - samples/sec: 12.25 - lr: 0.100000\n",
      "2022-05-08 02:05:41,843 epoch 12 - iter 160/160 - loss 0.04252003 - samples/sec: 11.62 - lr: 0.100000\n",
      "2022-05-08 02:05:42,328 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:05:42,328 EPOCH 12 done: loss 0.0425 - lr 0.100000\n",
      "2022-05-08 02:05:42,329 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 02:05:43,646 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:05:48,475 epoch 13 - iter 16/160 - loss 0.04157507 - samples/sec: 13.26 - lr: 0.100000\n",
      "2022-05-08 02:05:53,645 epoch 13 - iter 32/160 - loss 0.04842910 - samples/sec: 13.66 - lr: 0.100000\n",
      "2022-05-08 02:05:58,888 epoch 13 - iter 48/160 - loss 0.04864684 - samples/sec: 13.50 - lr: 0.100000\n",
      "2022-05-08 02:06:04,829 epoch 13 - iter 64/160 - loss 0.04694319 - samples/sec: 11.79 - lr: 0.100000\n",
      "2022-05-08 02:06:09,785 epoch 13 - iter 80/160 - loss 0.04437010 - samples/sec: 14.33 - lr: 0.100000\n",
      "2022-05-08 02:06:14,923 epoch 13 - iter 96/160 - loss 0.04505597 - samples/sec: 13.86 - lr: 0.100000\n",
      "2022-05-08 02:06:20,260 epoch 13 - iter 112/160 - loss 0.04293911 - samples/sec: 13.23 - lr: 0.100000\n",
      "2022-05-08 02:06:26,117 epoch 13 - iter 128/160 - loss 0.04352153 - samples/sec: 11.95 - lr: 0.100000\n",
      "2022-05-08 02:06:31,962 epoch 13 - iter 144/160 - loss 0.04392514 - samples/sec: 11.96 - lr: 0.100000\n",
      "2022-05-08 02:06:37,159 epoch 13 - iter 160/160 - loss 0.04299293 - samples/sec: 13.61 - lr: 0.100000\n",
      "2022-05-08 02:06:37,654 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:06:37,655 EPOCH 13 done: loss 0.0430 - lr 0.100000\n",
      "2022-05-08 02:06:37,655 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 02:06:38,958 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:06:43,613 epoch 14 - iter 16/160 - loss 0.04194569 - samples/sec: 13.75 - lr: 0.100000\n",
      "2022-05-08 02:06:48,795 epoch 14 - iter 32/160 - loss 0.04004007 - samples/sec: 13.64 - lr: 0.100000\n",
      "2022-05-08 02:06:53,873 epoch 14 - iter 48/160 - loss 0.04264041 - samples/sec: 13.98 - lr: 0.100000\n",
      "2022-05-08 02:06:59,041 epoch 14 - iter 64/160 - loss 0.04125812 - samples/sec: 13.71 - lr: 0.100000\n",
      "2022-05-08 02:07:04,472 epoch 14 - iter 80/160 - loss 0.04312635 - samples/sec: 12.95 - lr: 0.100000\n",
      "2022-05-08 02:07:10,176 epoch 14 - iter 96/160 - loss 0.04316767 - samples/sec: 12.29 - lr: 0.100000\n",
      "2022-05-08 02:07:15,795 epoch 14 - iter 112/160 - loss 0.04214333 - samples/sec: 12.46 - lr: 0.100000\n",
      "2022-05-08 02:07:21,365 epoch 14 - iter 128/160 - loss 0.04226760 - samples/sec: 12.61 - lr: 0.100000\n",
      "2022-05-08 02:07:27,312 epoch 14 - iter 144/160 - loss 0.04346847 - samples/sec: 11.74 - lr: 0.100000\n",
      "2022-05-08 02:07:32,753 epoch 14 - iter 160/160 - loss 0.04363077 - samples/sec: 12.93 - lr: 0.100000\n",
      "2022-05-08 02:07:33,244 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:07:33,245 EPOCH 14 done: loss 0.0436 - lr 0.100000\n",
      "2022-05-08 02:07:33,245 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 02:07:34,552 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:07:39,828 epoch 15 - iter 16/160 - loss 0.03862174 - samples/sec: 12.13 - lr: 0.100000\n",
      "2022-05-08 02:07:45,423 epoch 15 - iter 32/160 - loss 0.03866430 - samples/sec: 12.55 - lr: 0.100000\n",
      "2022-05-08 02:07:50,399 epoch 15 - iter 48/160 - loss 0.03970757 - samples/sec: 14.30 - lr: 0.100000\n",
      "2022-05-08 02:07:55,791 epoch 15 - iter 64/160 - loss 0.03898122 - samples/sec: 13.05 - lr: 0.100000\n",
      "2022-05-08 02:08:01,644 epoch 15 - iter 80/160 - loss 0.04053970 - samples/sec: 11.92 - lr: 0.100000\n",
      "2022-05-08 02:08:07,083 epoch 15 - iter 96/160 - loss 0.04165503 - samples/sec: 12.94 - lr: 0.100000\n",
      "2022-05-08 02:08:11,734 epoch 15 - iter 112/160 - loss 0.04082539 - samples/sec: 15.41 - lr: 0.100000\n",
      "2022-05-08 02:08:16,707 epoch 15 - iter 128/160 - loss 0.04176420 - samples/sec: 14.28 - lr: 0.100000\n",
      "2022-05-08 02:08:22,108 epoch 15 - iter 144/160 - loss 0.04216781 - samples/sec: 13.01 - lr: 0.100000\n",
      "2022-05-08 02:08:28,587 epoch 15 - iter 160/160 - loss 0.03983731 - samples/sec: 10.72 - lr: 0.100000\n",
      "2022-05-08 02:08:29,168 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:08:29,168 EPOCH 15 done: loss 0.0398 - lr 0.100000\n",
      "2022-05-08 02:08:29,169 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 02:08:30,549 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:08:36,239 epoch 16 - iter 16/160 - loss 0.04580509 - samples/sec: 11.25 - lr: 0.100000\n",
      "2022-05-08 02:08:41,158 epoch 16 - iter 32/160 - loss 0.03871316 - samples/sec: 14.54 - lr: 0.100000\n",
      "2022-05-08 02:08:46,598 epoch 16 - iter 48/160 - loss 0.04522605 - samples/sec: 12.92 - lr: 0.100000\n",
      "2022-05-08 02:08:52,041 epoch 16 - iter 64/160 - loss 0.04146252 - samples/sec: 12.99 - lr: 0.100000\n",
      "2022-05-08 02:08:57,387 epoch 16 - iter 80/160 - loss 0.03997001 - samples/sec: 13.17 - lr: 0.100000\n",
      "2022-05-08 02:09:02,492 epoch 16 - iter 96/160 - loss 0.04219226 - samples/sec: 13.84 - lr: 0.100000\n",
      "2022-05-08 02:09:07,983 epoch 16 - iter 112/160 - loss 0.04121021 - samples/sec: 12.86 - lr: 0.100000\n",
      "2022-05-08 02:09:13,267 epoch 16 - iter 128/160 - loss 0.04211855 - samples/sec: 13.34 - lr: 0.100000\n",
      "2022-05-08 02:09:19,219 epoch 16 - iter 144/160 - loss 0.04182931 - samples/sec: 11.68 - lr: 0.100000\n",
      "2022-05-08 02:09:24,768 epoch 16 - iter 160/160 - loss 0.04128470 - samples/sec: 12.63 - lr: 0.100000\n",
      "2022-05-08 02:09:25,258 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:09:25,258 EPOCH 16 done: loss 0.0413 - lr 0.100000\n",
      "2022-05-08 02:09:25,258 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 02:09:26,572 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:09:31,432 epoch 17 - iter 16/160 - loss 0.04082522 - samples/sec: 13.17 - lr: 0.100000\n",
      "2022-05-08 02:09:36,380 epoch 17 - iter 32/160 - loss 0.03657241 - samples/sec: 14.41 - lr: 0.100000\n",
      "2022-05-08 02:09:41,436 epoch 17 - iter 48/160 - loss 0.04661899 - samples/sec: 14.08 - lr: 0.100000\n",
      "2022-05-08 02:09:46,787 epoch 17 - iter 64/160 - loss 0.04432877 - samples/sec: 13.20 - lr: 0.100000\n",
      "2022-05-08 02:09:52,165 epoch 17 - iter 80/160 - loss 0.04377438 - samples/sec: 13.12 - lr: 0.100000\n",
      "2022-05-08 02:09:57,560 epoch 17 - iter 96/160 - loss 0.04679721 - samples/sec: 13.04 - lr: 0.100000\n",
      "2022-05-08 02:10:03,025 epoch 17 - iter 112/160 - loss 0.04497191 - samples/sec: 12.85 - lr: 0.100000\n",
      "2022-05-08 02:10:08,969 epoch 17 - iter 128/160 - loss 0.04519418 - samples/sec: 11.76 - lr: 0.100000\n",
      "2022-05-08 02:10:15,152 epoch 17 - iter 144/160 - loss 0.04332182 - samples/sec: 11.24 - lr: 0.100000\n",
      "2022-05-08 02:10:20,656 epoch 17 - iter 160/160 - loss 0.04176824 - samples/sec: 12.78 - lr: 0.100000\n",
      "2022-05-08 02:10:21,161 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:10:21,161 EPOCH 17 done: loss 0.0418 - lr 0.100000\n",
      "2022-05-08 02:10:21,162 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 02:10:22,496 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:10:27,566 epoch 18 - iter 16/160 - loss 0.04536233 - samples/sec: 12.63 - lr: 0.100000\n",
      "2022-05-08 02:10:33,089 epoch 18 - iter 32/160 - loss 0.03768877 - samples/sec: 12.73 - lr: 0.100000\n",
      "2022-05-08 02:10:38,866 epoch 18 - iter 48/160 - loss 0.03608927 - samples/sec: 12.12 - lr: 0.100000\n",
      "2022-05-08 02:10:44,811 epoch 18 - iter 64/160 - loss 0.04095228 - samples/sec: 11.74 - lr: 0.100000\n",
      "2022-05-08 02:10:50,041 epoch 18 - iter 80/160 - loss 0.04165820 - samples/sec: 13.50 - lr: 0.100000\n",
      "2022-05-08 02:10:55,474 epoch 18 - iter 96/160 - loss 0.03993130 - samples/sec: 12.95 - lr: 0.100000\n",
      "2022-05-08 02:11:00,572 epoch 18 - iter 112/160 - loss 0.03893238 - samples/sec: 13.87 - lr: 0.100000\n",
      "2022-05-08 02:11:05,479 epoch 18 - iter 128/160 - loss 0.03773688 - samples/sec: 14.50 - lr: 0.100000\n",
      "2022-05-08 02:11:10,853 epoch 18 - iter 144/160 - loss 0.03736048 - samples/sec: 13.09 - lr: 0.100000\n",
      "2022-05-08 02:11:16,435 epoch 18 - iter 160/160 - loss 0.03639835 - samples/sec: 12.57 - lr: 0.100000\n",
      "2022-05-08 02:11:16,919 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:11:16,920 EPOCH 18 done: loss 0.0364 - lr 0.100000\n",
      "2022-05-08 02:11:16,920 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 02:11:18,276 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:11:23,084 epoch 19 - iter 16/160 - loss 0.05493204 - samples/sec: 13.32 - lr: 0.100000\n",
      "2022-05-08 02:11:28,702 epoch 19 - iter 32/160 - loss 0.04614411 - samples/sec: 12.49 - lr: 0.100000\n",
      "2022-05-08 02:11:33,620 epoch 19 - iter 48/160 - loss 0.04405677 - samples/sec: 14.48 - lr: 0.100000\n",
      "2022-05-08 02:11:39,029 epoch 19 - iter 64/160 - loss 0.04233257 - samples/sec: 12.96 - lr: 0.100000\n",
      "2022-05-08 02:11:44,819 epoch 19 - iter 80/160 - loss 0.04016030 - samples/sec: 12.08 - lr: 0.100000\n",
      "2022-05-08 02:11:49,940 epoch 19 - iter 96/160 - loss 0.04036867 - samples/sec: 13.85 - lr: 0.100000\n",
      "2022-05-08 02:11:55,496 epoch 19 - iter 112/160 - loss 0.04129651 - samples/sec: 12.63 - lr: 0.100000\n",
      "2022-05-08 02:12:00,906 epoch 19 - iter 128/160 - loss 0.04132072 - samples/sec: 12.99 - lr: 0.100000\n",
      "2022-05-08 02:12:06,703 epoch 19 - iter 144/160 - loss 0.03933833 - samples/sec: 12.05 - lr: 0.100000\n",
      "2022-05-08 02:12:12,001 epoch 19 - iter 160/160 - loss 0.03872291 - samples/sec: 13.34 - lr: 0.100000\n",
      "2022-05-08 02:12:12,490 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:12:12,491 EPOCH 19 done: loss 0.0387 - lr 0.100000\n",
      "2022-05-08 02:12:12,491 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 02:12:13,852 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:12:19,482 epoch 20 - iter 16/160 - loss 0.04570581 - samples/sec: 11.37 - lr: 0.100000\n",
      "2022-05-08 02:12:24,755 epoch 20 - iter 32/160 - loss 0.04164601 - samples/sec: 13.37 - lr: 0.100000\n",
      "2022-05-08 02:12:30,365 epoch 20 - iter 48/160 - loss 0.03735567 - samples/sec: 12.52 - lr: 0.100000\n",
      "2022-05-08 02:12:36,129 epoch 20 - iter 64/160 - loss 0.03465733 - samples/sec: 12.15 - lr: 0.100000\n",
      "2022-05-08 02:12:41,666 epoch 20 - iter 80/160 - loss 0.03526906 - samples/sec: 12.67 - lr: 0.100000\n",
      "2022-05-08 02:12:47,318 epoch 20 - iter 96/160 - loss 0.03537799 - samples/sec: 12.41 - lr: 0.100000\n",
      "2022-05-08 02:12:51,920 epoch 20 - iter 112/160 - loss 0.03500066 - samples/sec: 15.57 - lr: 0.100000\n",
      "2022-05-08 02:12:57,474 epoch 20 - iter 128/160 - loss 0.03453012 - samples/sec: 12.71 - lr: 0.100000\n",
      "2022-05-08 02:13:03,269 epoch 20 - iter 144/160 - loss 0.03551448 - samples/sec: 12.09 - lr: 0.100000\n",
      "2022-05-08 02:13:08,081 epoch 20 - iter 160/160 - loss 0.03531798 - samples/sec: 14.81 - lr: 0.100000\n",
      "2022-05-08 02:13:08,581 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:13:08,581 EPOCH 20 done: loss 0.0353 - lr 0.100000\n",
      "2022-05-08 02:13:08,582 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 02:13:09,967 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:13:15,130 epoch 21 - iter 16/160 - loss 0.03576601 - samples/sec: 12.40 - lr: 0.100000\n",
      "2022-05-08 02:13:20,035 epoch 21 - iter 32/160 - loss 0.03523785 - samples/sec: 14.48 - lr: 0.100000\n",
      "2022-05-08 02:13:25,727 epoch 21 - iter 48/160 - loss 0.03348767 - samples/sec: 12.32 - lr: 0.100000\n",
      "2022-05-08 02:13:31,185 epoch 21 - iter 64/160 - loss 0.03514110 - samples/sec: 12.89 - lr: 0.100000\n",
      "2022-05-08 02:13:35,962 epoch 21 - iter 80/160 - loss 0.03576373 - samples/sec: 14.89 - lr: 0.100000\n",
      "2022-05-08 02:13:41,520 epoch 21 - iter 96/160 - loss 0.03805234 - samples/sec: 12.58 - lr: 0.100000\n",
      "2022-05-08 02:13:47,260 epoch 21 - iter 112/160 - loss 0.03779864 - samples/sec: 12.20 - lr: 0.100000\n",
      "2022-05-08 02:13:52,537 epoch 21 - iter 128/160 - loss 0.03838506 - samples/sec: 13.38 - lr: 0.100000\n",
      "2022-05-08 02:13:58,262 epoch 21 - iter 144/160 - loss 0.03922823 - samples/sec: 12.23 - lr: 0.100000\n",
      "2022-05-08 02:14:03,189 epoch 21 - iter 160/160 - loss 0.03770615 - samples/sec: 14.48 - lr: 0.100000\n",
      "2022-05-08 02:14:03,689 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:14:03,690 EPOCH 21 done: loss 0.0377 - lr 0.100000\n",
      "2022-05-08 02:14:03,690 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 02:14:05,062 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:14:09,491 epoch 22 - iter 16/160 - loss 0.04024819 - samples/sec: 14.46 - lr: 0.100000\n",
      "2022-05-08 02:14:14,579 epoch 22 - iter 32/160 - loss 0.03984805 - samples/sec: 13.89 - lr: 0.100000\n",
      "2022-05-08 02:14:20,082 epoch 22 - iter 48/160 - loss 0.03749650 - samples/sec: 12.74 - lr: 0.100000\n",
      "2022-05-08 02:14:25,194 epoch 22 - iter 64/160 - loss 0.03671836 - samples/sec: 13.87 - lr: 0.100000\n",
      "2022-05-08 02:14:30,384 epoch 22 - iter 80/160 - loss 0.03782820 - samples/sec: 13.63 - lr: 0.100000\n",
      "2022-05-08 02:14:35,877 epoch 22 - iter 96/160 - loss 0.03583203 - samples/sec: 12.75 - lr: 0.100000\n",
      "2022-05-08 02:14:41,439 epoch 22 - iter 112/160 - loss 0.03634618 - samples/sec: 12.59 - lr: 0.100000\n",
      "2022-05-08 02:14:46,928 epoch 22 - iter 128/160 - loss 0.03552935 - samples/sec: 12.82 - lr: 0.100000\n",
      "2022-05-08 02:14:52,602 epoch 22 - iter 144/160 - loss 0.03602533 - samples/sec: 12.33 - lr: 0.100000\n",
      "2022-05-08 02:14:58,036 epoch 22 - iter 160/160 - loss 0.03521192 - samples/sec: 12.93 - lr: 0.100000\n",
      "2022-05-08 02:14:58,521 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:14:58,521 EPOCH 22 done: loss 0.0352 - lr 0.100000\n",
      "2022-05-08 02:14:58,522 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 02:14:59,882 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:15:05,089 epoch 23 - iter 16/160 - loss 0.03690236 - samples/sec: 12.30 - lr: 0.100000\n",
      "2022-05-08 02:15:11,445 epoch 23 - iter 32/160 - loss 0.03773666 - samples/sec: 10.90 - lr: 0.100000\n",
      "2022-05-08 02:15:16,694 epoch 23 - iter 48/160 - loss 0.03686808 - samples/sec: 13.45 - lr: 0.100000\n",
      "2022-05-08 02:15:21,897 epoch 23 - iter 64/160 - loss 0.03586467 - samples/sec: 13.63 - lr: 0.100000\n",
      "2022-05-08 02:15:27,454 epoch 23 - iter 80/160 - loss 0.03441059 - samples/sec: 12.63 - lr: 0.100000\n",
      "2022-05-08 02:15:33,084 epoch 23 - iter 96/160 - loss 0.03347800 - samples/sec: 12.47 - lr: 0.100000\n",
      "2022-05-08 02:15:38,662 epoch 23 - iter 112/160 - loss 0.03461298 - samples/sec: 12.60 - lr: 0.100000\n",
      "2022-05-08 02:15:43,758 epoch 23 - iter 128/160 - loss 0.03555367 - samples/sec: 13.93 - lr: 0.100000\n",
      "2022-05-08 02:15:49,229 epoch 23 - iter 144/160 - loss 0.03632665 - samples/sec: 12.91 - lr: 0.100000\n",
      "2022-05-08 02:15:54,382 epoch 23 - iter 160/160 - loss 0.03607223 - samples/sec: 13.72 - lr: 0.100000\n",
      "2022-05-08 02:15:54,866 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:15:54,866 EPOCH 23 done: loss 0.0361 - lr 0.100000\n",
      "2022-05-08 02:15:54,867 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 02:15:56,194 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:16:01,506 epoch 24 - iter 16/160 - loss 0.04072610 - samples/sec: 12.05 - lr: 0.100000\n",
      "2022-05-08 02:16:06,654 epoch 24 - iter 32/160 - loss 0.03649202 - samples/sec: 13.79 - lr: 0.100000\n",
      "2022-05-08 02:16:12,265 epoch 24 - iter 48/160 - loss 0.03639823 - samples/sec: 12.49 - lr: 0.100000\n",
      "2022-05-08 02:16:17,565 epoch 24 - iter 64/160 - loss 0.03426427 - samples/sec: 13.29 - lr: 0.100000\n",
      "2022-05-08 02:16:22,607 epoch 24 - iter 80/160 - loss 0.03447734 - samples/sec: 14.03 - lr: 0.100000\n",
      "2022-05-08 02:16:28,171 epoch 24 - iter 96/160 - loss 0.03321853 - samples/sec: 12.58 - lr: 0.100000\n",
      "2022-05-08 02:16:32,864 epoch 24 - iter 112/160 - loss 0.03521144 - samples/sec: 15.26 - lr: 0.100000\n",
      "2022-05-08 02:16:38,426 epoch 24 - iter 128/160 - loss 0.03464738 - samples/sec: 12.59 - lr: 0.100000\n",
      "2022-05-08 02:16:43,764 epoch 24 - iter 144/160 - loss 0.03392324 - samples/sec: 13.18 - lr: 0.100000\n",
      "2022-05-08 02:16:48,849 epoch 24 - iter 160/160 - loss 0.03466995 - samples/sec: 13.92 - lr: 0.100000\n",
      "2022-05-08 02:16:49,324 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:16:49,325 EPOCH 24 done: loss 0.0347 - lr 0.100000\n",
      "2022-05-08 02:16:49,325 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 02:16:50,608 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:16:55,624 epoch 25 - iter 16/160 - loss 0.04053690 - samples/sec: 12.76 - lr: 0.100000\n",
      "2022-05-08 02:17:00,475 epoch 25 - iter 32/160 - loss 0.03846966 - samples/sec: 14.72 - lr: 0.100000\n",
      "2022-05-08 02:17:06,093 epoch 25 - iter 48/160 - loss 0.03574603 - samples/sec: 12.53 - lr: 0.100000\n",
      "2022-05-08 02:17:11,597 epoch 25 - iter 64/160 - loss 0.03693443 - samples/sec: 12.75 - lr: 0.100000\n",
      "2022-05-08 02:17:16,654 epoch 25 - iter 80/160 - loss 0.03746061 - samples/sec: 14.03 - lr: 0.100000\n",
      "2022-05-08 02:17:22,060 epoch 25 - iter 96/160 - loss 0.03656190 - samples/sec: 13.00 - lr: 0.100000\n",
      "2022-05-08 02:17:27,676 epoch 25 - iter 112/160 - loss 0.03533519 - samples/sec: 12.47 - lr: 0.100000\n",
      "2022-05-08 02:17:33,119 epoch 25 - iter 128/160 - loss 0.03543111 - samples/sec: 12.91 - lr: 0.100000\n",
      "2022-05-08 02:17:37,918 epoch 25 - iter 144/160 - loss 0.03504027 - samples/sec: 14.81 - lr: 0.100000\n",
      "2022-05-08 02:17:43,440 epoch 25 - iter 160/160 - loss 0.03506848 - samples/sec: 12.71 - lr: 0.100000\n",
      "2022-05-08 02:17:43,934 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:17:43,935 EPOCH 25 done: loss 0.0351 - lr 0.100000\n",
      "2022-05-08 02:17:43,936 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 02:17:45,259 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:17:50,199 epoch 26 - iter 16/160 - loss 0.03149532 - samples/sec: 12.96 - lr: 0.100000\n",
      "2022-05-08 02:17:55,823 epoch 26 - iter 32/160 - loss 0.02939241 - samples/sec: 12.51 - lr: 0.100000\n",
      "2022-05-08 02:18:01,169 epoch 26 - iter 48/160 - loss 0.03172670 - samples/sec: 13.14 - lr: 0.100000\n",
      "2022-05-08 02:18:05,931 epoch 26 - iter 64/160 - loss 0.03249589 - samples/sec: 14.96 - lr: 0.100000\n",
      "2022-05-08 02:18:11,279 epoch 26 - iter 80/160 - loss 0.03052559 - samples/sec: 13.12 - lr: 0.100000\n",
      "2022-05-08 02:18:16,860 epoch 26 - iter 96/160 - loss 0.03056315 - samples/sec: 12.53 - lr: 0.100000\n",
      "2022-05-08 02:18:22,327 epoch 26 - iter 112/160 - loss 0.03217479 - samples/sec: 12.91 - lr: 0.100000\n",
      "2022-05-08 02:18:27,940 epoch 26 - iter 128/160 - loss 0.03187091 - samples/sec: 12.52 - lr: 0.100000\n",
      "2022-05-08 02:18:33,435 epoch 26 - iter 144/160 - loss 0.03206986 - samples/sec: 12.83 - lr: 0.100000\n",
      "2022-05-08 02:18:38,616 epoch 26 - iter 160/160 - loss 0.03304847 - samples/sec: 13.65 - lr: 0.100000\n",
      "2022-05-08 02:18:39,102 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:18:39,102 EPOCH 26 done: loss 0.0330 - lr 0.100000\n",
      "2022-05-08 02:18:39,103 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 02:18:40,447 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:18:45,800 epoch 27 - iter 16/160 - loss 0.04401217 - samples/sec: 11.96 - lr: 0.100000\n",
      "2022-05-08 02:18:50,438 epoch 27 - iter 32/160 - loss 0.04269043 - samples/sec: 15.38 - lr: 0.100000\n",
      "2022-05-08 02:18:55,443 epoch 27 - iter 48/160 - loss 0.03546077 - samples/sec: 14.12 - lr: 0.100000\n",
      "2022-05-08 02:19:00,318 epoch 27 - iter 64/160 - loss 0.03486833 - samples/sec: 14.56 - lr: 0.100000\n",
      "2022-05-08 02:19:05,582 epoch 27 - iter 80/160 - loss 0.03403006 - samples/sec: 13.37 - lr: 0.100000\n",
      "2022-05-08 02:19:10,857 epoch 27 - iter 96/160 - loss 0.03404585 - samples/sec: 13.32 - lr: 0.100000\n",
      "2022-05-08 02:19:16,139 epoch 27 - iter 112/160 - loss 0.03434519 - samples/sec: 13.32 - lr: 0.100000\n",
      "2022-05-08 02:19:22,213 epoch 27 - iter 128/160 - loss 0.03347916 - samples/sec: 11.45 - lr: 0.100000\n",
      "2022-05-08 02:19:27,666 epoch 27 - iter 144/160 - loss 0.03350548 - samples/sec: 12.86 - lr: 0.100000\n",
      "2022-05-08 02:19:33,047 epoch 27 - iter 160/160 - loss 0.03272864 - samples/sec: 13.05 - lr: 0.100000\n",
      "2022-05-08 02:19:33,519 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:19:33,521 EPOCH 27 done: loss 0.0327 - lr 0.100000\n",
      "2022-05-08 02:19:33,521 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 02:19:34,839 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:19:40,198 epoch 28 - iter 16/160 - loss 0.03355158 - samples/sec: 11.94 - lr: 0.100000\n",
      "2022-05-08 02:19:45,512 epoch 28 - iter 32/160 - loss 0.03137687 - samples/sec: 13.32 - lr: 0.100000\n",
      "2022-05-08 02:19:50,412 epoch 28 - iter 48/160 - loss 0.03171328 - samples/sec: 14.49 - lr: 0.100000\n",
      "2022-05-08 02:19:55,584 epoch 28 - iter 64/160 - loss 0.03259966 - samples/sec: 13.63 - lr: 0.100000\n",
      "2022-05-08 02:20:00,758 epoch 28 - iter 80/160 - loss 0.03238633 - samples/sec: 13.61 - lr: 0.100000\n",
      "2022-05-08 02:20:06,274 epoch 28 - iter 96/160 - loss 0.03326216 - samples/sec: 12.73 - lr: 0.100000\n",
      "2022-05-08 02:20:11,230 epoch 28 - iter 112/160 - loss 0.03258846 - samples/sec: 14.38 - lr: 0.100000\n",
      "2022-05-08 02:20:16,292 epoch 28 - iter 128/160 - loss 0.03232243 - samples/sec: 13.94 - lr: 0.100000\n",
      "2022-05-08 02:20:21,791 epoch 28 - iter 144/160 - loss 0.03389118 - samples/sec: 12.76 - lr: 0.100000\n",
      "2022-05-08 02:20:27,335 epoch 28 - iter 160/160 - loss 0.03390877 - samples/sec: 12.69 - lr: 0.100000\n",
      "2022-05-08 02:20:27,802 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:20:27,802 EPOCH 28 done: loss 0.0339 - lr 0.100000\n",
      "2022-05-08 02:20:27,803 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 02:20:29,138 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:20:33,807 epoch 29 - iter 16/160 - loss 0.04013763 - samples/sec: 13.71 - lr: 0.100000\n",
      "2022-05-08 02:20:38,705 epoch 29 - iter 32/160 - loss 0.03909146 - samples/sec: 14.61 - lr: 0.100000\n",
      "2022-05-08 02:20:43,705 epoch 29 - iter 48/160 - loss 0.03856701 - samples/sec: 14.16 - lr: 0.100000\n",
      "2022-05-08 02:20:48,518 epoch 29 - iter 64/160 - loss 0.03749388 - samples/sec: 14.80 - lr: 0.100000\n",
      "2022-05-08 02:20:54,124 epoch 29 - iter 80/160 - loss 0.03649357 - samples/sec: 12.46 - lr: 0.100000\n",
      "2022-05-08 02:21:00,014 epoch 29 - iter 96/160 - loss 0.03493931 - samples/sec: 11.82 - lr: 0.100000\n",
      "2022-05-08 02:21:05,218 epoch 29 - iter 112/160 - loss 0.03483896 - samples/sec: 13.63 - lr: 0.100000\n",
      "2022-05-08 02:21:11,156 epoch 29 - iter 128/160 - loss 0.03754182 - samples/sec: 11.74 - lr: 0.100000\n",
      "2022-05-08 02:21:17,067 epoch 29 - iter 144/160 - loss 0.03693984 - samples/sec: 11.81 - lr: 0.100000\n",
      "2022-05-08 02:21:21,898 epoch 29 - iter 160/160 - loss 0.03722472 - samples/sec: 14.73 - lr: 0.100000\n",
      "2022-05-08 02:21:22,380 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:21:22,380 EPOCH 29 done: loss 0.0372 - lr 0.100000\n",
      "2022-05-08 02:21:22,381 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 02:21:23,692 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:21:28,271 epoch 30 - iter 16/160 - loss 0.03364174 - samples/sec: 13.98 - lr: 0.100000\n",
      "2022-05-08 02:21:33,908 epoch 30 - iter 32/160 - loss 0.02928133 - samples/sec: 12.63 - lr: 0.100000\n",
      "2022-05-08 02:21:39,386 epoch 30 - iter 48/160 - loss 0.02791540 - samples/sec: 12.81 - lr: 0.100000\n",
      "2022-05-08 02:21:44,637 epoch 30 - iter 64/160 - loss 0.02776767 - samples/sec: 13.52 - lr: 0.100000\n",
      "2022-05-08 02:21:50,040 epoch 30 - iter 80/160 - loss 0.02929941 - samples/sec: 13.17 - lr: 0.100000\n",
      "2022-05-08 02:21:55,569 epoch 30 - iter 96/160 - loss 0.03138679 - samples/sec: 12.71 - lr: 0.100000\n",
      "2022-05-08 02:22:00,565 epoch 30 - iter 112/160 - loss 0.03254548 - samples/sec: 14.16 - lr: 0.100000\n",
      "2022-05-08 02:22:05,625 epoch 30 - iter 128/160 - loss 0.03299046 - samples/sec: 13.98 - lr: 0.100000\n",
      "2022-05-08 02:22:11,105 epoch 30 - iter 144/160 - loss 0.03291250 - samples/sec: 12.81 - lr: 0.100000\n",
      "2022-05-08 02:22:16,165 epoch 30 - iter 160/160 - loss 0.03294716 - samples/sec: 14.01 - lr: 0.100000\n",
      "2022-05-08 02:22:16,653 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:22:16,654 EPOCH 30 done: loss 0.0329 - lr 0.100000\n",
      "2022-05-08 02:22:16,654 BAD EPOCHS (no improvement): 3\n",
      "2022-05-08 02:22:17,937 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:22:22,497 epoch 31 - iter 16/160 - loss 0.04136354 - samples/sec: 14.04 - lr: 0.100000\n",
      "2022-05-08 02:22:27,868 epoch 31 - iter 32/160 - loss 0.03639451 - samples/sec: 13.16 - lr: 0.100000\n",
      "2022-05-08 02:22:32,903 epoch 31 - iter 48/160 - loss 0.03390238 - samples/sec: 14.02 - lr: 0.100000\n",
      "2022-05-08 02:22:38,511 epoch 31 - iter 64/160 - loss 0.03345630 - samples/sec: 12.54 - lr: 0.100000\n",
      "2022-05-08 02:22:43,671 epoch 31 - iter 80/160 - loss 0.03268508 - samples/sec: 13.69 - lr: 0.100000\n",
      "2022-05-08 02:22:49,564 epoch 31 - iter 96/160 - loss 0.03165691 - samples/sec: 11.83 - lr: 0.100000\n",
      "2022-05-08 02:22:54,943 epoch 31 - iter 112/160 - loss 0.03129295 - samples/sec: 13.13 - lr: 0.100000\n",
      "2022-05-08 02:23:00,353 epoch 31 - iter 128/160 - loss 0.03098118 - samples/sec: 12.99 - lr: 0.100000\n",
      "2022-05-08 02:23:05,734 epoch 31 - iter 144/160 - loss 0.03093814 - samples/sec: 13.03 - lr: 0.100000\n",
      "2022-05-08 02:23:10,993 epoch 31 - iter 160/160 - loss 0.03305742 - samples/sec: 13.39 - lr: 0.100000\n",
      "2022-05-08 02:23:11,492 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:23:11,492 EPOCH 31 done: loss 0.0331 - lr 0.100000\n",
      "2022-05-08 02:23:11,493 Epoch    31: reducing learning rate of group 0 to 5.0000e-02.\n",
      "2022-05-08 02:23:11,493 BAD EPOCHS (no improvement): 4\n",
      "2022-05-08 02:23:12,800 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:23:17,935 epoch 32 - iter 16/160 - loss 0.03880808 - samples/sec: 12.47 - lr: 0.050000\n",
      "2022-05-08 02:23:23,192 epoch 32 - iter 32/160 - loss 0.03550677 - samples/sec: 13.38 - lr: 0.050000\n",
      "2022-05-08 02:23:27,950 epoch 32 - iter 48/160 - loss 0.03467548 - samples/sec: 14.94 - lr: 0.050000\n",
      "2022-05-08 02:23:32,796 epoch 32 - iter 64/160 - loss 0.03400782 - samples/sec: 14.67 - lr: 0.050000\n",
      "2022-05-08 02:23:38,650 epoch 32 - iter 80/160 - loss 0.03305201 - samples/sec: 11.96 - lr: 0.050000\n",
      "2022-05-08 02:23:44,217 epoch 32 - iter 96/160 - loss 0.03107700 - samples/sec: 12.63 - lr: 0.050000\n",
      "2022-05-08 02:23:49,202 epoch 32 - iter 112/160 - loss 0.03101322 - samples/sec: 14.21 - lr: 0.050000\n",
      "2022-05-08 02:23:54,552 epoch 32 - iter 128/160 - loss 0.03030774 - samples/sec: 13.17 - lr: 0.050000\n",
      "2022-05-08 02:23:59,970 epoch 32 - iter 144/160 - loss 0.03013524 - samples/sec: 12.98 - lr: 0.050000\n",
      "2022-05-08 02:24:05,135 epoch 32 - iter 160/160 - loss 0.02994860 - samples/sec: 13.69 - lr: 0.050000\n",
      "2022-05-08 02:24:05,624 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:24:05,625 EPOCH 32 done: loss 0.0299 - lr 0.050000\n",
      "2022-05-08 02:24:05,625 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 02:24:06,920 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:24:11,697 epoch 33 - iter 16/160 - loss 0.02283668 - samples/sec: 13.40 - lr: 0.050000\n",
      "2022-05-08 02:24:17,097 epoch 33 - iter 32/160 - loss 0.02479356 - samples/sec: 13.13 - lr: 0.050000\n",
      "2022-05-08 02:24:22,474 epoch 33 - iter 48/160 - loss 0.02682988 - samples/sec: 13.11 - lr: 0.050000\n",
      "2022-05-08 02:24:27,589 epoch 33 - iter 64/160 - loss 0.02735030 - samples/sec: 13.85 - lr: 0.050000\n",
      "2022-05-08 02:24:33,402 epoch 33 - iter 80/160 - loss 0.02628901 - samples/sec: 12.02 - lr: 0.050000\n",
      "2022-05-08 02:24:38,297 epoch 33 - iter 96/160 - loss 0.02740940 - samples/sec: 14.51 - lr: 0.050000\n",
      "2022-05-08 02:24:43,794 epoch 33 - iter 112/160 - loss 0.02772421 - samples/sec: 12.75 - lr: 0.050000\n",
      "2022-05-08 02:24:49,420 epoch 33 - iter 128/160 - loss 0.02834904 - samples/sec: 12.43 - lr: 0.050000\n",
      "2022-05-08 02:24:54,604 epoch 33 - iter 144/160 - loss 0.02781133 - samples/sec: 13.61 - lr: 0.050000\n",
      "2022-05-08 02:24:59,543 epoch 33 - iter 160/160 - loss 0.02756851 - samples/sec: 14.34 - lr: 0.050000\n",
      "2022-05-08 02:25:00,029 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:25:00,029 EPOCH 33 done: loss 0.0276 - lr 0.050000\n",
      "2022-05-08 02:25:00,030 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 02:25:01,317 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:25:06,352 epoch 34 - iter 16/160 - loss 0.02287916 - samples/sec: 12.72 - lr: 0.050000\n",
      "2022-05-08 02:25:12,161 epoch 34 - iter 32/160 - loss 0.02830430 - samples/sec: 11.97 - lr: 0.050000\n",
      "2022-05-08 02:25:17,392 epoch 34 - iter 48/160 - loss 0.02707467 - samples/sec: 13.48 - lr: 0.050000\n",
      "2022-05-08 02:25:22,987 epoch 34 - iter 64/160 - loss 0.02525649 - samples/sec: 12.52 - lr: 0.050000\n",
      "2022-05-08 02:25:28,300 epoch 34 - iter 80/160 - loss 0.02657052 - samples/sec: 13.26 - lr: 0.050000\n",
      "2022-05-08 02:25:33,411 epoch 34 - iter 96/160 - loss 0.02659286 - samples/sec: 13.80 - lr: 0.050000\n",
      "2022-05-08 02:25:38,610 epoch 34 - iter 112/160 - loss 0.02613778 - samples/sec: 13.54 - lr: 0.050000\n",
      "2022-05-08 02:25:43,564 epoch 34 - iter 128/160 - loss 0.02637013 - samples/sec: 14.31 - lr: 0.050000\n",
      "2022-05-08 02:25:48,829 epoch 34 - iter 144/160 - loss 0.02637103 - samples/sec: 13.36 - lr: 0.050000\n",
      "2022-05-08 02:25:53,845 epoch 34 - iter 160/160 - loss 0.02695855 - samples/sec: 14.12 - lr: 0.050000\n",
      "2022-05-08 02:25:54,320 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:25:54,321 EPOCH 34 done: loss 0.0270 - lr 0.050000\n",
      "2022-05-08 02:25:54,321 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 02:25:55,626 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:26:00,462 epoch 35 - iter 16/160 - loss 0.02832328 - samples/sec: 13.24 - lr: 0.050000\n",
      "2022-05-08 02:26:05,576 epoch 35 - iter 32/160 - loss 0.02877089 - samples/sec: 13.86 - lr: 0.050000\n",
      "2022-05-08 02:26:10,867 epoch 35 - iter 48/160 - loss 0.02668146 - samples/sec: 13.34 - lr: 0.050000\n",
      "2022-05-08 02:26:16,055 epoch 35 - iter 64/160 - loss 0.02794020 - samples/sec: 13.59 - lr: 0.050000\n",
      "2022-05-08 02:26:21,874 epoch 35 - iter 80/160 - loss 0.02653256 - samples/sec: 12.00 - lr: 0.050000\n",
      "2022-05-08 02:26:27,766 epoch 35 - iter 96/160 - loss 0.02691386 - samples/sec: 11.82 - lr: 0.050000\n",
      "2022-05-08 02:26:32,976 epoch 35 - iter 112/160 - loss 0.02733869 - samples/sec: 13.55 - lr: 0.050000\n",
      "2022-05-08 02:26:38,297 epoch 35 - iter 128/160 - loss 0.02766898 - samples/sec: 13.22 - lr: 0.050000\n",
      "2022-05-08 02:26:43,191 epoch 35 - iter 144/160 - loss 0.02772127 - samples/sec: 14.63 - lr: 0.050000\n",
      "2022-05-08 02:26:48,854 epoch 35 - iter 160/160 - loss 0.02744054 - samples/sec: 12.36 - lr: 0.050000\n",
      "2022-05-08 02:26:49,339 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:26:49,340 EPOCH 35 done: loss 0.0274 - lr 0.050000\n",
      "2022-05-08 02:26:49,340 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 02:26:50,673 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:26:55,723 epoch 36 - iter 16/160 - loss 0.02059611 - samples/sec: 12.68 - lr: 0.050000\n",
      "2022-05-08 02:27:00,864 epoch 36 - iter 32/160 - loss 0.02481503 - samples/sec: 13.71 - lr: 0.050000\n",
      "2022-05-08 02:27:06,237 epoch 36 - iter 48/160 - loss 0.02830860 - samples/sec: 13.04 - lr: 0.050000\n",
      "2022-05-08 02:27:11,563 epoch 36 - iter 64/160 - loss 0.02605341 - samples/sec: 13.24 - lr: 0.050000\n",
      "2022-05-08 02:27:17,280 epoch 36 - iter 80/160 - loss 0.02565647 - samples/sec: 12.22 - lr: 0.050000\n",
      "2022-05-08 02:27:22,622 epoch 36 - iter 96/160 - loss 0.02503601 - samples/sec: 13.16 - lr: 0.050000\n",
      "2022-05-08 02:27:27,364 epoch 36 - iter 112/160 - loss 0.02514777 - samples/sec: 15.01 - lr: 0.050000\n",
      "2022-05-08 02:27:33,279 epoch 36 - iter 128/160 - loss 0.02619540 - samples/sec: 11.77 - lr: 0.050000\n",
      "2022-05-08 02:27:38,573 epoch 36 - iter 144/160 - loss 0.02605449 - samples/sec: 13.37 - lr: 0.050000\n",
      "2022-05-08 02:27:43,443 epoch 36 - iter 160/160 - loss 0.02641882 - samples/sec: 14.55 - lr: 0.050000\n",
      "2022-05-08 02:27:43,913 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:27:43,913 EPOCH 36 done: loss 0.0264 - lr 0.050000\n",
      "2022-05-08 02:27:43,914 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 02:27:45,222 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:27:49,999 epoch 37 - iter 16/160 - loss 0.02217192 - samples/sec: 13.41 - lr: 0.050000\n",
      "2022-05-08 02:27:56,009 epoch 37 - iter 32/160 - loss 0.02401145 - samples/sec: 11.59 - lr: 0.050000\n",
      "2022-05-08 02:28:01,304 epoch 37 - iter 48/160 - loss 0.02542308 - samples/sec: 13.33 - lr: 0.050000\n",
      "2022-05-08 02:28:06,741 epoch 37 - iter 64/160 - loss 0.02644779 - samples/sec: 12.94 - lr: 0.050000\n",
      "2022-05-08 02:28:11,530 epoch 37 - iter 80/160 - loss 0.02718898 - samples/sec: 14.85 - lr: 0.050000\n",
      "2022-05-08 02:28:16,609 epoch 37 - iter 96/160 - loss 0.02690119 - samples/sec: 14.04 - lr: 0.050000\n",
      "2022-05-08 02:28:21,648 epoch 37 - iter 112/160 - loss 0.02695298 - samples/sec: 14.09 - lr: 0.050000\n",
      "2022-05-08 02:28:27,226 epoch 37 - iter 128/160 - loss 0.02636708 - samples/sec: 12.53 - lr: 0.050000\n",
      "2022-05-08 02:28:32,251 epoch 37 - iter 144/160 - loss 0.02642254 - samples/sec: 14.09 - lr: 0.050000\n",
      "2022-05-08 02:28:37,467 epoch 37 - iter 160/160 - loss 0.02643684 - samples/sec: 13.57 - lr: 0.050000\n",
      "2022-05-08 02:28:37,944 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:28:37,945 EPOCH 37 done: loss 0.0264 - lr 0.050000\n",
      "2022-05-08 02:28:37,946 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 02:28:39,264 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:28:44,441 epoch 38 - iter 16/160 - loss 0.02881532 - samples/sec: 12.37 - lr: 0.050000\n",
      "2022-05-08 02:28:50,154 epoch 38 - iter 32/160 - loss 0.02798098 - samples/sec: 12.20 - lr: 0.050000\n",
      "2022-05-08 02:28:55,881 epoch 38 - iter 48/160 - loss 0.02464225 - samples/sec: 12.21 - lr: 0.050000\n",
      "2022-05-08 02:29:01,473 epoch 38 - iter 64/160 - loss 0.02630323 - samples/sec: 12.50 - lr: 0.050000\n",
      "2022-05-08 02:29:06,119 epoch 38 - iter 80/160 - loss 0.02689371 - samples/sec: 15.35 - lr: 0.050000\n",
      "2022-05-08 02:29:11,194 epoch 38 - iter 96/160 - loss 0.02810662 - samples/sec: 13.95 - lr: 0.050000\n",
      "2022-05-08 02:29:16,493 epoch 38 - iter 112/160 - loss 0.02774166 - samples/sec: 13.27 - lr: 0.050000\n",
      "2022-05-08 02:29:21,788 epoch 38 - iter 128/160 - loss 0.02713090 - samples/sec: 13.40 - lr: 0.050000\n",
      "2022-05-08 02:29:27,049 epoch 38 - iter 144/160 - loss 0.02699673 - samples/sec: 13.43 - lr: 0.050000\n",
      "2022-05-08 02:29:32,080 epoch 38 - iter 160/160 - loss 0.02652972 - samples/sec: 14.07 - lr: 0.050000\n",
      "2022-05-08 02:29:32,583 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:29:32,583 EPOCH 38 done: loss 0.0265 - lr 0.050000\n",
      "2022-05-08 02:29:32,584 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 02:29:34,034 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:29:39,578 epoch 39 - iter 16/160 - loss 0.02504859 - samples/sec: 11.55 - lr: 0.050000\n",
      "2022-05-08 02:29:44,553 epoch 39 - iter 32/160 - loss 0.02382277 - samples/sec: 14.27 - lr: 0.050000\n",
      "2022-05-08 02:29:49,726 epoch 39 - iter 48/160 - loss 0.02374912 - samples/sec: 13.63 - lr: 0.050000\n",
      "2022-05-08 02:29:54,598 epoch 39 - iter 64/160 - loss 0.02409820 - samples/sec: 14.60 - lr: 0.050000\n",
      "2022-05-08 02:29:59,755 epoch 39 - iter 80/160 - loss 0.02486192 - samples/sec: 13.71 - lr: 0.050000\n",
      "2022-05-08 02:30:05,095 epoch 39 - iter 96/160 - loss 0.02622363 - samples/sec: 13.24 - lr: 0.050000\n",
      "2022-05-08 02:30:10,555 epoch 39 - iter 112/160 - loss 0.02491160 - samples/sec: 12.85 - lr: 0.050000\n",
      "2022-05-08 02:30:16,087 epoch 39 - iter 128/160 - loss 0.02590334 - samples/sec: 12.70 - lr: 0.050000\n",
      "2022-05-08 02:30:20,879 epoch 39 - iter 144/160 - loss 0.02579975 - samples/sec: 14.86 - lr: 0.050000\n",
      "2022-05-08 02:30:26,076 epoch 39 - iter 160/160 - loss 0.02531308 - samples/sec: 13.57 - lr: 0.050000\n",
      "2022-05-08 02:30:26,576 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:30:26,577 EPOCH 39 done: loss 0.0253 - lr 0.050000\n",
      "2022-05-08 02:30:26,578 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 02:30:27,882 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:30:32,679 epoch 40 - iter 16/160 - loss 0.02532145 - samples/sec: 13.35 - lr: 0.050000\n",
      "2022-05-08 02:30:37,656 epoch 40 - iter 32/160 - loss 0.02512978 - samples/sec: 14.27 - lr: 0.050000\n",
      "2022-05-08 02:30:42,713 epoch 40 - iter 48/160 - loss 0.02691397 - samples/sec: 13.98 - lr: 0.050000\n",
      "2022-05-08 02:30:47,972 epoch 40 - iter 64/160 - loss 0.02861969 - samples/sec: 13.39 - lr: 0.050000\n",
      "2022-05-08 02:30:53,788 epoch 40 - iter 80/160 - loss 0.02801148 - samples/sec: 11.98 - lr: 0.050000\n",
      "2022-05-08 02:30:58,561 epoch 40 - iter 96/160 - loss 0.02759052 - samples/sec: 14.94 - lr: 0.050000\n",
      "2022-05-08 02:31:03,813 epoch 40 - iter 112/160 - loss 0.02717219 - samples/sec: 13.44 - lr: 0.050000\n",
      "2022-05-08 02:31:09,190 epoch 40 - iter 128/160 - loss 0.02662961 - samples/sec: 13.05 - lr: 0.050000\n",
      "2022-05-08 02:31:14,717 epoch 40 - iter 144/160 - loss 0.02643170 - samples/sec: 12.69 - lr: 0.050000\n",
      "2022-05-08 02:31:20,397 epoch 40 - iter 160/160 - loss 0.02555849 - samples/sec: 12.29 - lr: 0.050000\n",
      "2022-05-08 02:31:20,868 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:31:20,869 EPOCH 40 done: loss 0.0256 - lr 0.050000\n",
      "2022-05-08 02:31:20,869 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 02:31:22,179 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:31:27,213 epoch 41 - iter 16/160 - loss 0.02493136 - samples/sec: 12.72 - lr: 0.050000\n",
      "2022-05-08 02:31:32,972 epoch 41 - iter 32/160 - loss 0.02395010 - samples/sec: 12.13 - lr: 0.050000\n",
      "2022-05-08 02:31:38,358 epoch 41 - iter 48/160 - loss 0.02862915 - samples/sec: 13.05 - lr: 0.050000\n",
      "2022-05-08 02:31:43,748 epoch 41 - iter 64/160 - loss 0.02687777 - samples/sec: 13.05 - lr: 0.050000\n",
      "2022-05-08 02:31:49,065 epoch 41 - iter 80/160 - loss 0.02798422 - samples/sec: 13.23 - lr: 0.050000\n",
      "2022-05-08 02:31:54,284 epoch 41 - iter 96/160 - loss 0.02835474 - samples/sec: 13.54 - lr: 0.050000\n",
      "2022-05-08 02:31:59,596 epoch 41 - iter 112/160 - loss 0.02672374 - samples/sec: 13.24 - lr: 0.050000\n",
      "2022-05-08 02:32:04,395 epoch 41 - iter 128/160 - loss 0.02732183 - samples/sec: 14.89 - lr: 0.050000\n",
      "2022-05-08 02:32:09,921 epoch 41 - iter 144/160 - loss 0.02645011 - samples/sec: 12.71 - lr: 0.050000\n",
      "2022-05-08 02:32:15,426 epoch 41 - iter 160/160 - loss 0.02656132 - samples/sec: 12.79 - lr: 0.050000\n",
      "2022-05-08 02:32:15,910 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:32:15,911 EPOCH 41 done: loss 0.0266 - lr 0.050000\n",
      "2022-05-08 02:32:15,911 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 02:32:17,240 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:32:22,194 epoch 42 - iter 16/160 - loss 0.02097761 - samples/sec: 12.93 - lr: 0.050000\n",
      "2022-05-08 02:32:27,047 epoch 42 - iter 32/160 - loss 0.02022379 - samples/sec: 14.64 - lr: 0.050000\n",
      "2022-05-08 02:32:32,347 epoch 42 - iter 48/160 - loss 0.02084598 - samples/sec: 13.37 - lr: 0.050000\n",
      "2022-05-08 02:32:37,660 epoch 42 - iter 64/160 - loss 0.02141347 - samples/sec: 13.20 - lr: 0.050000\n",
      "2022-05-08 02:32:43,005 epoch 42 - iter 80/160 - loss 0.02458620 - samples/sec: 13.18 - lr: 0.050000\n",
      "2022-05-08 02:32:48,881 epoch 42 - iter 96/160 - loss 0.02453622 - samples/sec: 11.85 - lr: 0.050000\n",
      "2022-05-08 02:32:54,194 epoch 42 - iter 112/160 - loss 0.02454588 - samples/sec: 13.27 - lr: 0.050000\n",
      "2022-05-08 02:32:59,184 epoch 42 - iter 128/160 - loss 0.02425656 - samples/sec: 14.20 - lr: 0.050000\n",
      "2022-05-08 02:33:04,924 epoch 42 - iter 144/160 - loss 0.02400950 - samples/sec: 12.24 - lr: 0.050000\n",
      "2022-05-08 02:33:10,014 epoch 42 - iter 160/160 - loss 0.02375415 - samples/sec: 13.88 - lr: 0.050000\n",
      "2022-05-08 02:33:10,511 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:33:10,511 EPOCH 42 done: loss 0.0238 - lr 0.050000\n",
      "2022-05-08 02:33:10,512 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 02:33:11,817 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:33:16,392 epoch 43 - iter 16/160 - loss 0.02444555 - samples/sec: 14.00 - lr: 0.050000\n",
      "2022-05-08 02:33:21,035 epoch 43 - iter 32/160 - loss 0.02422803 - samples/sec: 15.49 - lr: 0.050000\n",
      "2022-05-08 02:33:26,642 epoch 43 - iter 48/160 - loss 0.02265545 - samples/sec: 12.52 - lr: 0.050000\n",
      "2022-05-08 02:33:32,273 epoch 43 - iter 64/160 - loss 0.02280187 - samples/sec: 12.44 - lr: 0.050000\n",
      "2022-05-08 02:33:37,463 epoch 43 - iter 80/160 - loss 0.02203522 - samples/sec: 13.60 - lr: 0.050000\n",
      "2022-05-08 02:33:42,261 epoch 43 - iter 96/160 - loss 0.02281124 - samples/sec: 14.81 - lr: 0.050000\n",
      "2022-05-08 02:33:47,738 epoch 43 - iter 112/160 - loss 0.02493609 - samples/sec: 12.86 - lr: 0.050000\n",
      "2022-05-08 02:33:53,164 epoch 43 - iter 128/160 - loss 0.02528546 - samples/sec: 12.92 - lr: 0.050000\n",
      "2022-05-08 02:33:59,223 epoch 43 - iter 144/160 - loss 0.02447896 - samples/sec: 11.54 - lr: 0.050000\n",
      "2022-05-08 02:34:04,220 epoch 43 - iter 160/160 - loss 0.02513750 - samples/sec: 14.15 - lr: 0.050000\n",
      "2022-05-08 02:34:04,707 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:34:04,708 EPOCH 43 done: loss 0.0251 - lr 0.050000\n",
      "2022-05-08 02:34:04,708 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 02:34:06,021 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:34:10,803 epoch 44 - iter 16/160 - loss 0.01876259 - samples/sec: 13.39 - lr: 0.050000\n",
      "2022-05-08 02:34:16,082 epoch 44 - iter 32/160 - loss 0.01952609 - samples/sec: 13.35 - lr: 0.050000\n",
      "2022-05-08 02:34:21,616 epoch 44 - iter 48/160 - loss 0.02252542 - samples/sec: 12.73 - lr: 0.050000\n",
      "2022-05-08 02:34:27,497 epoch 44 - iter 64/160 - loss 0.02421430 - samples/sec: 11.86 - lr: 0.050000\n",
      "2022-05-08 02:34:32,641 epoch 44 - iter 80/160 - loss 0.02322541 - samples/sec: 13.75 - lr: 0.050000\n",
      "2022-05-08 02:34:37,416 epoch 44 - iter 96/160 - loss 0.02460948 - samples/sec: 14.91 - lr: 0.050000\n",
      "2022-05-08 02:34:42,791 epoch 44 - iter 112/160 - loss 0.02478408 - samples/sec: 13.10 - lr: 0.050000\n",
      "2022-05-08 02:34:48,332 epoch 44 - iter 128/160 - loss 0.02449310 - samples/sec: 12.63 - lr: 0.050000\n",
      "2022-05-08 02:34:53,351 epoch 44 - iter 144/160 - loss 0.02411022 - samples/sec: 14.14 - lr: 0.050000\n",
      "2022-05-08 02:34:58,650 epoch 44 - iter 160/160 - loss 0.02494735 - samples/sec: 13.35 - lr: 0.050000\n",
      "2022-05-08 02:34:59,124 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:34:59,124 EPOCH 44 done: loss 0.0249 - lr 0.050000\n",
      "2022-05-08 02:34:59,125 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 02:35:00,438 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:35:05,724 epoch 45 - iter 16/160 - loss 0.02713388 - samples/sec: 12.11 - lr: 0.050000\n",
      "2022-05-08 02:35:10,297 epoch 45 - iter 32/160 - loss 0.02356107 - samples/sec: 15.63 - lr: 0.050000\n",
      "2022-05-08 02:35:15,536 epoch 45 - iter 48/160 - loss 0.02460478 - samples/sec: 13.68 - lr: 0.050000\n",
      "2022-05-08 02:35:20,604 epoch 45 - iter 64/160 - loss 0.02485088 - samples/sec: 14.01 - lr: 0.050000\n",
      "2022-05-08 02:35:26,289 epoch 45 - iter 80/160 - loss 0.02533283 - samples/sec: 12.31 - lr: 0.050000\n",
      "2022-05-08 02:35:31,661 epoch 45 - iter 96/160 - loss 0.02596119 - samples/sec: 13.17 - lr: 0.050000\n",
      "2022-05-08 02:35:37,016 epoch 45 - iter 112/160 - loss 0.02591212 - samples/sec: 13.16 - lr: 0.050000\n",
      "2022-05-08 02:35:42,649 epoch 45 - iter 128/160 - loss 0.02514822 - samples/sec: 12.46 - lr: 0.050000\n",
      "2022-05-08 02:35:48,033 epoch 45 - iter 144/160 - loss 0.02594929 - samples/sec: 13.02 - lr: 0.050000\n",
      "2022-05-08 02:35:53,104 epoch 45 - iter 160/160 - loss 0.02567818 - samples/sec: 13.96 - lr: 0.050000\n",
      "2022-05-08 02:35:53,611 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:35:53,612 EPOCH 45 done: loss 0.0257 - lr 0.050000\n",
      "2022-05-08 02:35:53,612 BAD EPOCHS (no improvement): 3\n",
      "2022-05-08 02:35:54,904 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:35:59,690 epoch 46 - iter 16/160 - loss 0.02059581 - samples/sec: 13.38 - lr: 0.050000\n",
      "2022-05-08 02:36:04,218 epoch 46 - iter 32/160 - loss 0.02145484 - samples/sec: 15.83 - lr: 0.050000\n",
      "2022-05-08 02:36:09,761 epoch 46 - iter 48/160 - loss 0.02330740 - samples/sec: 12.65 - lr: 0.050000\n",
      "2022-05-08 02:36:15,115 epoch 46 - iter 64/160 - loss 0.02283992 - samples/sec: 13.12 - lr: 0.050000\n",
      "2022-05-08 02:36:20,534 epoch 46 - iter 80/160 - loss 0.02343873 - samples/sec: 12.97 - lr: 0.050000\n",
      "2022-05-08 02:36:25,988 epoch 46 - iter 96/160 - loss 0.02305056 - samples/sec: 12.98 - lr: 0.050000\n",
      "2022-05-08 02:36:30,955 epoch 46 - iter 112/160 - loss 0.02310737 - samples/sec: 14.25 - lr: 0.050000\n",
      "2022-05-08 02:36:36,351 epoch 46 - iter 128/160 - loss 0.02354476 - samples/sec: 13.02 - lr: 0.050000\n",
      "2022-05-08 02:36:42,094 epoch 46 - iter 144/160 - loss 0.02357658 - samples/sec: 12.17 - lr: 0.050000\n",
      "2022-05-08 02:36:47,726 epoch 46 - iter 160/160 - loss 0.02336397 - samples/sec: 12.44 - lr: 0.050000\n",
      "2022-05-08 02:36:48,203 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:36:48,204 EPOCH 46 done: loss 0.0234 - lr 0.050000\n",
      "2022-05-08 02:36:48,204 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 02:36:49,492 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:36:54,466 epoch 47 - iter 16/160 - loss 0.02446028 - samples/sec: 12.87 - lr: 0.050000\n",
      "2022-05-08 02:37:00,033 epoch 47 - iter 32/160 - loss 0.02206361 - samples/sec: 12.57 - lr: 0.050000\n",
      "2022-05-08 02:37:04,953 epoch 47 - iter 48/160 - loss 0.02359131 - samples/sec: 14.44 - lr: 0.050000\n",
      "2022-05-08 02:37:10,420 epoch 47 - iter 64/160 - loss 0.02371818 - samples/sec: 12.85 - lr: 0.050000\n",
      "2022-05-08 02:37:15,709 epoch 47 - iter 80/160 - loss 0.02245227 - samples/sec: 13.35 - lr: 0.050000\n",
      "2022-05-08 02:37:21,014 epoch 47 - iter 96/160 - loss 0.02176838 - samples/sec: 13.27 - lr: 0.050000\n",
      "2022-05-08 02:37:26,479 epoch 47 - iter 112/160 - loss 0.02169204 - samples/sec: 12.84 - lr: 0.050000\n",
      "2022-05-08 02:37:31,642 epoch 47 - iter 128/160 - loss 0.02151700 - samples/sec: 13.68 - lr: 0.050000\n",
      "2022-05-08 02:37:37,172 epoch 47 - iter 144/160 - loss 0.02306674 - samples/sec: 12.67 - lr: 0.050000\n",
      "2022-05-08 02:37:42,507 epoch 47 - iter 160/160 - loss 0.02311186 - samples/sec: 13.19 - lr: 0.050000\n",
      "2022-05-08 02:37:42,985 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:37:42,985 EPOCH 47 done: loss 0.0231 - lr 0.050000\n",
      "2022-05-08 02:37:42,986 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 02:37:44,301 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:37:49,379 epoch 48 - iter 16/160 - loss 0.02014193 - samples/sec: 12.61 - lr: 0.050000\n",
      "2022-05-08 02:37:54,800 epoch 48 - iter 32/160 - loss 0.02321125 - samples/sec: 12.94 - lr: 0.050000\n",
      "2022-05-08 02:38:00,433 epoch 48 - iter 48/160 - loss 0.02525430 - samples/sec: 12.45 - lr: 0.050000\n",
      "2022-05-08 02:38:05,192 epoch 48 - iter 64/160 - loss 0.02469298 - samples/sec: 14.92 - lr: 0.050000\n",
      "2022-05-08 02:38:10,321 epoch 48 - iter 80/160 - loss 0.02527203 - samples/sec: 13.78 - lr: 0.050000\n",
      "2022-05-08 02:38:15,686 epoch 48 - iter 96/160 - loss 0.02425370 - samples/sec: 13.14 - lr: 0.050000\n",
      "2022-05-08 02:38:21,338 epoch 48 - iter 112/160 - loss 0.02523743 - samples/sec: 12.40 - lr: 0.050000\n",
      "2022-05-08 02:38:25,917 epoch 48 - iter 128/160 - loss 0.02456519 - samples/sec: 15.63 - lr: 0.050000\n",
      "2022-05-08 02:38:31,511 epoch 48 - iter 144/160 - loss 0.02442713 - samples/sec: 12.49 - lr: 0.050000\n",
      "2022-05-08 02:38:37,098 epoch 48 - iter 160/160 - loss 0.02462925 - samples/sec: 12.56 - lr: 0.050000\n",
      "2022-05-08 02:38:37,597 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:38:37,597 EPOCH 48 done: loss 0.0246 - lr 0.050000\n",
      "2022-05-08 02:38:37,598 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 02:38:38,995 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:38:43,999 epoch 49 - iter 16/160 - loss 0.02091300 - samples/sec: 12.79 - lr: 0.050000\n",
      "2022-05-08 02:38:49,070 epoch 49 - iter 32/160 - loss 0.02291054 - samples/sec: 13.93 - lr: 0.050000\n",
      "2022-05-08 02:38:54,206 epoch 49 - iter 48/160 - loss 0.02182318 - samples/sec: 13.74 - lr: 0.050000\n",
      "2022-05-08 02:38:59,819 epoch 49 - iter 64/160 - loss 0.02438465 - samples/sec: 12.49 - lr: 0.050000\n",
      "2022-05-08 02:39:04,465 epoch 49 - iter 80/160 - loss 0.02523453 - samples/sec: 15.45 - lr: 0.050000\n",
      "2022-05-08 02:39:10,187 epoch 49 - iter 96/160 - loss 0.02481850 - samples/sec: 12.21 - lr: 0.050000\n",
      "2022-05-08 02:39:15,937 epoch 49 - iter 112/160 - loss 0.02488023 - samples/sec: 12.17 - lr: 0.050000\n",
      "2022-05-08 02:39:21,262 epoch 49 - iter 128/160 - loss 0.02590297 - samples/sec: 13.23 - lr: 0.050000\n",
      "2022-05-08 02:39:26,343 epoch 49 - iter 144/160 - loss 0.02477211 - samples/sec: 13.91 - lr: 0.050000\n",
      "2022-05-08 02:39:31,483 epoch 49 - iter 160/160 - loss 0.02437134 - samples/sec: 13.79 - lr: 0.050000\n",
      "2022-05-08 02:39:31,964 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:39:31,965 EPOCH 49 done: loss 0.0244 - lr 0.050000\n",
      "2022-05-08 02:39:31,966 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 02:39:33,268 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:39:37,892 epoch 50 - iter 16/160 - loss 0.02771980 - samples/sec: 13.85 - lr: 0.050000\n",
      "2022-05-08 02:39:43,153 epoch 50 - iter 32/160 - loss 0.02609106 - samples/sec: 13.41 - lr: 0.050000\n",
      "2022-05-08 02:39:49,001 epoch 50 - iter 48/160 - loss 0.02408118 - samples/sec: 11.94 - lr: 0.050000\n",
      "2022-05-08 02:39:55,130 epoch 50 - iter 64/160 - loss 0.02453738 - samples/sec: 11.34 - lr: 0.050000\n",
      "2022-05-08 02:40:00,193 epoch 50 - iter 80/160 - loss 0.02459575 - samples/sec: 14.04 - lr: 0.050000\n",
      "2022-05-08 02:40:04,980 epoch 50 - iter 96/160 - loss 0.02336521 - samples/sec: 14.86 - lr: 0.050000\n",
      "2022-05-08 02:40:10,145 epoch 50 - iter 112/160 - loss 0.02365663 - samples/sec: 13.63 - lr: 0.050000\n",
      "2022-05-08 02:40:15,219 epoch 50 - iter 128/160 - loss 0.02334363 - samples/sec: 13.91 - lr: 0.050000\n",
      "2022-05-08 02:40:20,345 epoch 50 - iter 144/160 - loss 0.02450500 - samples/sec: 13.80 - lr: 0.050000\n",
      "2022-05-08 02:40:25,563 epoch 50 - iter 160/160 - loss 0.02448989 - samples/sec: 13.54 - lr: 0.050000\n",
      "2022-05-08 02:40:26,042 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:40:26,042 EPOCH 50 done: loss 0.0245 - lr 0.050000\n",
      "2022-05-08 02:40:26,043 BAD EPOCHS (no improvement): 3\n",
      "2022-05-08 02:40:27,348 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:40:32,082 epoch 51 - iter 16/160 - loss 0.02518579 - samples/sec: 13.53 - lr: 0.050000\n",
      "2022-05-08 02:40:37,576 epoch 51 - iter 32/160 - loss 0.02363917 - samples/sec: 12.78 - lr: 0.050000\n",
      "2022-05-08 02:40:42,936 epoch 51 - iter 48/160 - loss 0.02210828 - samples/sec: 13.17 - lr: 0.050000\n",
      "2022-05-08 02:40:48,229 epoch 51 - iter 64/160 - loss 0.02102167 - samples/sec: 13.30 - lr: 0.050000\n",
      "2022-05-08 02:40:53,602 epoch 51 - iter 80/160 - loss 0.02282035 - samples/sec: 13.07 - lr: 0.050000\n",
      "2022-05-08 02:40:58,500 epoch 51 - iter 96/160 - loss 0.02300287 - samples/sec: 14.49 - lr: 0.050000\n",
      "2022-05-08 02:41:03,550 epoch 51 - iter 112/160 - loss 0.02304780 - samples/sec: 14.08 - lr: 0.050000\n",
      "2022-05-08 02:41:09,011 epoch 51 - iter 128/160 - loss 0.02318605 - samples/sec: 12.82 - lr: 0.050000\n",
      "2022-05-08 02:41:14,129 epoch 51 - iter 144/160 - loss 0.02364169 - samples/sec: 13.79 - lr: 0.050000\n",
      "2022-05-08 02:41:19,673 epoch 51 - iter 160/160 - loss 0.02378913 - samples/sec: 12.64 - lr: 0.050000\n",
      "2022-05-08 02:41:20,148 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:41:20,148 EPOCH 51 done: loss 0.0238 - lr 0.050000\n",
      "2022-05-08 02:41:20,149 Epoch    51: reducing learning rate of group 0 to 2.5000e-02.\n",
      "2022-05-08 02:41:20,149 BAD EPOCHS (no improvement): 4\n",
      "2022-05-08 02:41:21,469 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:41:26,111 epoch 52 - iter 16/160 - loss 0.02175721 - samples/sec: 13.80 - lr: 0.025000\n",
      "2022-05-08 02:41:31,268 epoch 52 - iter 32/160 - loss 0.02413517 - samples/sec: 13.70 - lr: 0.025000\n",
      "2022-05-08 02:41:36,635 epoch 52 - iter 48/160 - loss 0.02506318 - samples/sec: 13.09 - lr: 0.025000\n",
      "2022-05-08 02:41:41,739 epoch 52 - iter 64/160 - loss 0.02369471 - samples/sec: 13.85 - lr: 0.025000\n",
      "2022-05-08 02:41:46,955 epoch 52 - iter 80/160 - loss 0.02348178 - samples/sec: 13.51 - lr: 0.025000\n",
      "2022-05-08 02:41:52,342 epoch 52 - iter 96/160 - loss 0.02290222 - samples/sec: 13.04 - lr: 0.025000\n",
      "2022-05-08 02:41:57,730 epoch 52 - iter 112/160 - loss 0.02328788 - samples/sec: 13.05 - lr: 0.025000\n",
      "2022-05-08 02:42:03,258 epoch 52 - iter 128/160 - loss 0.02309222 - samples/sec: 12.71 - lr: 0.025000\n",
      "2022-05-08 02:42:08,429 epoch 52 - iter 144/160 - loss 0.02251460 - samples/sec: 13.62 - lr: 0.025000\n",
      "2022-05-08 02:42:13,762 epoch 52 - iter 160/160 - loss 0.02322068 - samples/sec: 13.18 - lr: 0.025000\n",
      "2022-05-08 02:42:14,244 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:42:14,244 EPOCH 52 done: loss 0.0232 - lr 0.025000\n",
      "2022-05-08 02:42:14,245 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 02:42:15,545 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:42:20,193 epoch 53 - iter 16/160 - loss 0.01498966 - samples/sec: 13.77 - lr: 0.025000\n",
      "2022-05-08 02:42:25,462 epoch 53 - iter 32/160 - loss 0.01708776 - samples/sec: 13.40 - lr: 0.025000\n",
      "2022-05-08 02:42:30,729 epoch 53 - iter 48/160 - loss 0.02009076 - samples/sec: 13.39 - lr: 0.025000\n",
      "2022-05-08 02:42:36,072 epoch 53 - iter 64/160 - loss 0.02129148 - samples/sec: 13.18 - lr: 0.025000\n",
      "2022-05-08 02:42:41,086 epoch 53 - iter 80/160 - loss 0.02021148 - samples/sec: 14.12 - lr: 0.025000\n",
      "2022-05-08 02:42:47,043 epoch 53 - iter 96/160 - loss 0.02000842 - samples/sec: 11.68 - lr: 0.025000\n",
      "2022-05-08 02:42:52,477 epoch 53 - iter 112/160 - loss 0.02077853 - samples/sec: 12.90 - lr: 0.025000\n",
      "2022-05-08 02:42:57,298 epoch 53 - iter 128/160 - loss 0.02119006 - samples/sec: 14.80 - lr: 0.025000\n",
      "2022-05-08 02:43:02,893 epoch 53 - iter 144/160 - loss 0.02130164 - samples/sec: 12.51 - lr: 0.025000\n",
      "2022-05-08 02:43:08,603 epoch 53 - iter 160/160 - loss 0.02099458 - samples/sec: 12.27 - lr: 0.025000\n",
      "2022-05-08 02:43:09,091 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:43:09,091 EPOCH 53 done: loss 0.0210 - lr 0.025000\n",
      "2022-05-08 02:43:09,092 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 02:43:10,387 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:43:14,620 epoch 54 - iter 16/160 - loss 0.02008713 - samples/sec: 15.13 - lr: 0.025000\n",
      "2022-05-08 02:43:20,417 epoch 54 - iter 32/160 - loss 0.02123644 - samples/sec: 12.13 - lr: 0.025000\n",
      "2022-05-08 02:43:25,771 epoch 54 - iter 48/160 - loss 0.02011900 - samples/sec: 13.19 - lr: 0.025000\n",
      "2022-05-08 02:43:31,545 epoch 54 - iter 64/160 - loss 0.01985169 - samples/sec: 12.08 - lr: 0.025000\n",
      "2022-05-08 02:43:36,238 epoch 54 - iter 80/160 - loss 0.01997068 - samples/sec: 15.28 - lr: 0.025000\n",
      "2022-05-08 02:43:41,601 epoch 54 - iter 96/160 - loss 0.02003491 - samples/sec: 13.12 - lr: 0.025000\n",
      "2022-05-08 02:43:46,981 epoch 54 - iter 112/160 - loss 0.02023097 - samples/sec: 13.13 - lr: 0.025000\n",
      "2022-05-08 02:43:52,688 epoch 54 - iter 128/160 - loss 0.02082465 - samples/sec: 12.24 - lr: 0.025000\n",
      "2022-05-08 02:43:58,686 epoch 54 - iter 144/160 - loss 0.02124088 - samples/sec: 11.59 - lr: 0.025000\n",
      "2022-05-08 02:44:03,567 epoch 54 - iter 160/160 - loss 0.02156115 - samples/sec: 14.55 - lr: 0.025000\n",
      "2022-05-08 02:44:04,038 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:44:04,039 EPOCH 54 done: loss 0.0216 - lr 0.025000\n",
      "2022-05-08 02:44:04,039 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 02:44:05,318 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:44:10,056 epoch 55 - iter 16/160 - loss 0.02181772 - samples/sec: 13.51 - lr: 0.025000\n",
      "2022-05-08 02:44:15,372 epoch 55 - iter 32/160 - loss 0.01925803 - samples/sec: 13.29 - lr: 0.025000\n",
      "2022-05-08 02:44:20,494 epoch 55 - iter 48/160 - loss 0.01899941 - samples/sec: 13.81 - lr: 0.025000\n",
      "2022-05-08 02:44:26,067 epoch 55 - iter 64/160 - loss 0.01985931 - samples/sec: 12.57 - lr: 0.025000\n",
      "2022-05-08 02:44:31,502 epoch 55 - iter 80/160 - loss 0.02066377 - samples/sec: 12.96 - lr: 0.025000\n",
      "2022-05-08 02:44:36,273 epoch 55 - iter 96/160 - loss 0.02201223 - samples/sec: 14.93 - lr: 0.025000\n",
      "2022-05-08 02:44:41,497 epoch 55 - iter 112/160 - loss 0.02214459 - samples/sec: 13.56 - lr: 0.025000\n",
      "2022-05-08 02:44:46,791 epoch 55 - iter 128/160 - loss 0.02176741 - samples/sec: 13.29 - lr: 0.025000\n",
      "2022-05-08 02:44:52,415 epoch 55 - iter 144/160 - loss 0.02161723 - samples/sec: 12.45 - lr: 0.025000\n",
      "2022-05-08 02:44:58,347 epoch 55 - iter 160/160 - loss 0.02242297 - samples/sec: 11.76 - lr: 0.025000\n",
      "2022-05-08 02:44:58,821 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:44:58,821 EPOCH 55 done: loss 0.0224 - lr 0.025000\n",
      "2022-05-08 02:44:58,822 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 02:45:00,113 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:45:05,410 epoch 56 - iter 16/160 - loss 0.01649101 - samples/sec: 12.09 - lr: 0.025000\n",
      "2022-05-08 02:45:10,375 epoch 56 - iter 32/160 - loss 0.01945275 - samples/sec: 14.34 - lr: 0.025000\n",
      "2022-05-08 02:45:15,760 epoch 56 - iter 48/160 - loss 0.02159623 - samples/sec: 13.06 - lr: 0.025000\n",
      "2022-05-08 02:45:20,736 epoch 56 - iter 64/160 - loss 0.02113702 - samples/sec: 14.21 - lr: 0.025000\n",
      "2022-05-08 02:45:25,943 epoch 56 - iter 80/160 - loss 0.02135161 - samples/sec: 13.51 - lr: 0.025000\n",
      "2022-05-08 02:45:31,392 epoch 56 - iter 96/160 - loss 0.02180880 - samples/sec: 12.88 - lr: 0.025000\n",
      "2022-05-08 02:45:36,709 epoch 56 - iter 112/160 - loss 0.02184524 - samples/sec: 13.25 - lr: 0.025000\n",
      "2022-05-08 02:45:41,761 epoch 56 - iter 128/160 - loss 0.02173114 - samples/sec: 13.99 - lr: 0.025000\n",
      "2022-05-08 02:45:47,441 epoch 56 - iter 144/160 - loss 0.02159502 - samples/sec: 12.33 - lr: 0.025000\n",
      "2022-05-08 02:45:52,624 epoch 56 - iter 160/160 - loss 0.02171407 - samples/sec: 13.65 - lr: 0.025000\n",
      "2022-05-08 02:45:53,125 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:45:53,125 EPOCH 56 done: loss 0.0217 - lr 0.025000\n",
      "2022-05-08 02:45:53,126 BAD EPOCHS (no improvement): 3\n",
      "2022-05-08 02:45:54,404 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:45:59,104 epoch 57 - iter 16/160 - loss 0.01888775 - samples/sec: 13.62 - lr: 0.025000\n",
      "2022-05-08 02:46:04,288 epoch 57 - iter 32/160 - loss 0.01801959 - samples/sec: 13.63 - lr: 0.025000\n",
      "2022-05-08 02:46:09,677 epoch 57 - iter 48/160 - loss 0.01886500 - samples/sec: 13.09 - lr: 0.025000\n",
      "2022-05-08 02:46:14,878 epoch 57 - iter 64/160 - loss 0.02045640 - samples/sec: 13.53 - lr: 0.025000\n",
      "2022-05-08 02:46:19,832 epoch 57 - iter 80/160 - loss 0.02048063 - samples/sec: 14.29 - lr: 0.025000\n",
      "2022-05-08 02:46:25,413 epoch 57 - iter 96/160 - loss 0.02012971 - samples/sec: 12.54 - lr: 0.025000\n",
      "2022-05-08 02:46:30,851 epoch 57 - iter 112/160 - loss 0.02006245 - samples/sec: 12.95 - lr: 0.025000\n",
      "2022-05-08 02:46:36,073 epoch 57 - iter 128/160 - loss 0.02013466 - samples/sec: 13.55 - lr: 0.025000\n",
      "2022-05-08 02:46:41,365 epoch 57 - iter 144/160 - loss 0.02051421 - samples/sec: 13.34 - lr: 0.025000\n",
      "2022-05-08 02:46:47,229 epoch 57 - iter 160/160 - loss 0.02121772 - samples/sec: 11.88 - lr: 0.025000\n",
      "2022-05-08 02:46:47,739 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:46:47,740 EPOCH 57 done: loss 0.0212 - lr 0.025000\n",
      "2022-05-08 02:46:47,740 Epoch    57: reducing learning rate of group 0 to 1.2500e-02.\n",
      "2022-05-08 02:46:47,741 BAD EPOCHS (no improvement): 4\n",
      "2022-05-08 02:46:49,042 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:46:54,197 epoch 58 - iter 16/160 - loss 0.02509594 - samples/sec: 12.42 - lr: 0.012500\n",
      "2022-05-08 02:46:59,518 epoch 58 - iter 32/160 - loss 0.02285368 - samples/sec: 13.30 - lr: 0.012500\n",
      "2022-05-08 02:47:04,526 epoch 58 - iter 48/160 - loss 0.02078232 - samples/sec: 14.12 - lr: 0.012500\n",
      "2022-05-08 02:47:10,184 epoch 58 - iter 64/160 - loss 0.02103594 - samples/sec: 12.38 - lr: 0.012500\n",
      "2022-05-08 02:47:15,219 epoch 58 - iter 80/160 - loss 0.02064449 - samples/sec: 14.22 - lr: 0.012500\n",
      "2022-05-08 02:47:21,329 epoch 58 - iter 96/160 - loss 0.02107462 - samples/sec: 11.46 - lr: 0.012500\n",
      "2022-05-08 02:47:26,911 epoch 58 - iter 112/160 - loss 0.02175289 - samples/sec: 12.61 - lr: 0.012500\n",
      "2022-05-08 02:47:32,165 epoch 58 - iter 128/160 - loss 0.02109234 - samples/sec: 13.40 - lr: 0.012500\n",
      "2022-05-08 02:47:37,330 epoch 58 - iter 144/160 - loss 0.02117314 - samples/sec: 13.67 - lr: 0.012500\n",
      "2022-05-08 02:47:42,097 epoch 58 - iter 160/160 - loss 0.02149216 - samples/sec: 14.96 - lr: 0.012500\n",
      "2022-05-08 02:47:42,605 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:47:42,605 EPOCH 58 done: loss 0.0215 - lr 0.012500\n",
      "2022-05-08 02:47:42,606 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 02:47:43,897 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:47:48,391 epoch 59 - iter 16/160 - loss 0.03125698 - samples/sec: 14.25 - lr: 0.012500\n",
      "2022-05-08 02:47:53,500 epoch 59 - iter 32/160 - loss 0.02649751 - samples/sec: 13.88 - lr: 0.012500\n",
      "2022-05-08 02:47:58,452 epoch 59 - iter 48/160 - loss 0.02750996 - samples/sec: 14.34 - lr: 0.012500\n",
      "2022-05-08 02:48:04,322 epoch 59 - iter 64/160 - loss 0.02678977 - samples/sec: 11.98 - lr: 0.012500\n",
      "2022-05-08 02:48:09,683 epoch 59 - iter 80/160 - loss 0.02467644 - samples/sec: 13.18 - lr: 0.012500\n",
      "2022-05-08 02:48:15,200 epoch 59 - iter 96/160 - loss 0.02358684 - samples/sec: 12.80 - lr: 0.012500\n",
      "2022-05-08 02:48:21,465 epoch 59 - iter 112/160 - loss 0.02296961 - samples/sec: 11.09 - lr: 0.012500\n",
      "2022-05-08 02:48:26,233 epoch 59 - iter 128/160 - loss 0.02346359 - samples/sec: 14.92 - lr: 0.012500\n",
      "2022-05-08 02:48:32,082 epoch 59 - iter 144/160 - loss 0.02289633 - samples/sec: 11.95 - lr: 0.012500\n",
      "2022-05-08 02:48:36,832 epoch 59 - iter 160/160 - loss 0.02244293 - samples/sec: 15.00 - lr: 0.012500\n",
      "2022-05-08 02:48:37,347 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:48:37,347 EPOCH 59 done: loss 0.0224 - lr 0.012500\n",
      "2022-05-08 02:48:37,348 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 02:48:38,624 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:48:43,879 epoch 60 - iter 16/160 - loss 0.01794740 - samples/sec: 12.18 - lr: 0.012500\n",
      "2022-05-08 02:48:49,460 epoch 60 - iter 32/160 - loss 0.01968694 - samples/sec: 12.58 - lr: 0.012500\n",
      "2022-05-08 02:48:54,556 epoch 60 - iter 48/160 - loss 0.02055904 - samples/sec: 13.88 - lr: 0.012500\n",
      "2022-05-08 02:48:59,875 epoch 60 - iter 64/160 - loss 0.01911467 - samples/sec: 13.21 - lr: 0.012500\n",
      "2022-05-08 02:49:05,007 epoch 60 - iter 80/160 - loss 0.01994182 - samples/sec: 13.77 - lr: 0.012500\n",
      "2022-05-08 02:49:10,299 epoch 60 - iter 96/160 - loss 0.02040340 - samples/sec: 13.36 - lr: 0.012500\n",
      "2022-05-08 02:49:15,376 epoch 60 - iter 112/160 - loss 0.02063455 - samples/sec: 13.91 - lr: 0.012500\n",
      "2022-05-08 02:49:20,931 epoch 60 - iter 128/160 - loss 0.02075190 - samples/sec: 12.63 - lr: 0.012500\n",
      "2022-05-08 02:49:26,158 epoch 60 - iter 144/160 - loss 0.02022148 - samples/sec: 13.52 - lr: 0.012500\n",
      "2022-05-08 02:49:31,249 epoch 60 - iter 160/160 - loss 0.01970199 - samples/sec: 13.90 - lr: 0.012500\n",
      "2022-05-08 02:49:31,757 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:49:31,757 EPOCH 60 done: loss 0.0197 - lr 0.012500\n",
      "2022-05-08 02:49:31,758 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 02:49:33,034 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:49:37,965 epoch 61 - iter 16/160 - loss 0.01607802 - samples/sec: 12.98 - lr: 0.012500\n",
      "2022-05-08 02:49:42,793 epoch 61 - iter 32/160 - loss 0.01828266 - samples/sec: 14.74 - lr: 0.012500\n",
      "2022-05-08 02:49:48,411 epoch 61 - iter 48/160 - loss 0.01922882 - samples/sec: 12.46 - lr: 0.012500\n",
      "2022-05-08 02:49:53,410 epoch 61 - iter 64/160 - loss 0.02115487 - samples/sec: 14.19 - lr: 0.012500\n",
      "2022-05-08 02:49:59,123 epoch 61 - iter 80/160 - loss 0.02062533 - samples/sec: 12.27 - lr: 0.012500\n",
      "2022-05-08 02:50:04,306 epoch 61 - iter 96/160 - loss 0.02031573 - samples/sec: 13.66 - lr: 0.012500\n",
      "2022-05-08 02:50:09,733 epoch 61 - iter 112/160 - loss 0.01961689 - samples/sec: 13.01 - lr: 0.012500\n",
      "2022-05-08 02:50:15,307 epoch 61 - iter 128/160 - loss 0.01974609 - samples/sec: 12.60 - lr: 0.012500\n",
      "2022-05-08 02:50:20,735 epoch 61 - iter 144/160 - loss 0.02127025 - samples/sec: 12.96 - lr: 0.012500\n",
      "2022-05-08 02:50:25,794 epoch 61 - iter 160/160 - loss 0.02109599 - samples/sec: 14.00 - lr: 0.012500\n",
      "2022-05-08 02:50:26,294 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:50:26,295 EPOCH 61 done: loss 0.0211 - lr 0.012500\n",
      "2022-05-08 02:50:26,296 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 02:50:27,590 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:50:32,868 epoch 62 - iter 16/160 - loss 0.02253938 - samples/sec: 12.13 - lr: 0.012500\n",
      "2022-05-08 02:50:38,033 epoch 62 - iter 32/160 - loss 0.02180813 - samples/sec: 13.66 - lr: 0.012500\n",
      "2022-05-08 02:50:42,966 epoch 62 - iter 48/160 - loss 0.02170005 - samples/sec: 14.45 - lr: 0.012500\n",
      "2022-05-08 02:50:47,960 epoch 62 - iter 64/160 - loss 0.02253035 - samples/sec: 14.24 - lr: 0.012500\n",
      "2022-05-08 02:50:53,025 epoch 62 - iter 80/160 - loss 0.02231700 - samples/sec: 13.94 - lr: 0.012500\n",
      "2022-05-08 02:50:58,544 epoch 62 - iter 96/160 - loss 0.02205528 - samples/sec: 12.72 - lr: 0.012500\n",
      "2022-05-08 02:51:04,127 epoch 62 - iter 112/160 - loss 0.02110088 - samples/sec: 12.59 - lr: 0.012500\n",
      "2022-05-08 02:51:09,928 epoch 62 - iter 128/160 - loss 0.02089485 - samples/sec: 12.04 - lr: 0.012500\n",
      "2022-05-08 02:51:15,042 epoch 62 - iter 144/160 - loss 0.01993450 - samples/sec: 13.86 - lr: 0.012500\n",
      "2022-05-08 02:51:20,334 epoch 62 - iter 160/160 - loss 0.01994496 - samples/sec: 13.41 - lr: 0.012500\n",
      "2022-05-08 02:51:20,822 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:51:20,822 EPOCH 62 done: loss 0.0199 - lr 0.012500\n",
      "2022-05-08 02:51:20,823 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 02:51:22,131 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:51:27,704 epoch 63 - iter 16/160 - loss 0.01648424 - samples/sec: 11.49 - lr: 0.012500\n",
      "2022-05-08 02:51:32,307 epoch 63 - iter 32/160 - loss 0.02093502 - samples/sec: 15.50 - lr: 0.012500\n",
      "2022-05-08 02:51:37,226 epoch 63 - iter 48/160 - loss 0.02319961 - samples/sec: 14.42 - lr: 0.012500\n",
      "2022-05-08 02:51:42,534 epoch 63 - iter 64/160 - loss 0.02214644 - samples/sec: 13.29 - lr: 0.012500\n",
      "2022-05-08 02:51:48,197 epoch 63 - iter 80/160 - loss 0.02141943 - samples/sec: 12.39 - lr: 0.012500\n",
      "2022-05-08 02:51:53,251 epoch 63 - iter 96/160 - loss 0.02174871 - samples/sec: 14.00 - lr: 0.012500\n",
      "2022-05-08 02:51:58,539 epoch 63 - iter 112/160 - loss 0.02214487 - samples/sec: 13.38 - lr: 0.012500\n",
      "2022-05-08 02:52:03,663 epoch 63 - iter 128/160 - loss 0.02186884 - samples/sec: 13.80 - lr: 0.012500\n",
      "2022-05-08 02:52:08,981 epoch 63 - iter 144/160 - loss 0.02172950 - samples/sec: 13.28 - lr: 0.012500\n",
      "2022-05-08 02:52:14,517 epoch 63 - iter 160/160 - loss 0.02123378 - samples/sec: 12.67 - lr: 0.012500\n",
      "2022-05-08 02:52:14,998 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:52:14,998 EPOCH 63 done: loss 0.0212 - lr 0.012500\n",
      "2022-05-08 02:52:14,999 BAD EPOCHS (no improvement): 3\n",
      "2022-05-08 02:52:16,315 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:52:21,285 epoch 64 - iter 16/160 - loss 0.02448713 - samples/sec: 12.88 - lr: 0.012500\n",
      "2022-05-08 02:52:26,805 epoch 64 - iter 32/160 - loss 0.02346661 - samples/sec: 12.72 - lr: 0.012500\n",
      "2022-05-08 02:52:32,464 epoch 64 - iter 48/160 - loss 0.02335834 - samples/sec: 12.39 - lr: 0.012500\n",
      "2022-05-08 02:52:37,842 epoch 64 - iter 64/160 - loss 0.02225711 - samples/sec: 13.20 - lr: 0.012500\n",
      "2022-05-08 02:52:43,363 epoch 64 - iter 80/160 - loss 0.02112526 - samples/sec: 12.78 - lr: 0.012500\n",
      "2022-05-08 02:52:48,893 epoch 64 - iter 96/160 - loss 0.02083752 - samples/sec: 12.67 - lr: 0.012500\n",
      "2022-05-08 02:52:53,948 epoch 64 - iter 112/160 - loss 0.01996168 - samples/sec: 13.98 - lr: 0.012500\n",
      "2022-05-08 02:52:59,315 epoch 64 - iter 128/160 - loss 0.01945146 - samples/sec: 13.11 - lr: 0.012500\n",
      "2022-05-08 02:53:04,693 epoch 64 - iter 144/160 - loss 0.01957997 - samples/sec: 13.13 - lr: 0.012500\n",
      "2022-05-08 02:53:09,469 epoch 64 - iter 160/160 - loss 0.01967362 - samples/sec: 14.89 - lr: 0.012500\n",
      "2022-05-08 02:53:09,953 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:53:09,954 EPOCH 64 done: loss 0.0197 - lr 0.012500\n",
      "2022-05-08 02:53:09,955 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 02:53:11,275 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:53:16,118 epoch 65 - iter 16/160 - loss 0.02411569 - samples/sec: 13.22 - lr: 0.012500\n",
      "2022-05-08 02:53:21,138 epoch 65 - iter 32/160 - loss 0.02483912 - samples/sec: 14.13 - lr: 0.012500\n",
      "2022-05-08 02:53:26,508 epoch 65 - iter 48/160 - loss 0.02306786 - samples/sec: 13.11 - lr: 0.012500\n",
      "2022-05-08 02:53:31,562 epoch 65 - iter 64/160 - loss 0.02159756 - samples/sec: 14.02 - lr: 0.012500\n",
      "2022-05-08 02:53:36,787 epoch 65 - iter 80/160 - loss 0.02082228 - samples/sec: 13.56 - lr: 0.012500\n",
      "2022-05-08 02:53:42,519 epoch 65 - iter 96/160 - loss 0.02105274 - samples/sec: 12.20 - lr: 0.012500\n",
      "2022-05-08 02:53:47,956 epoch 65 - iter 112/160 - loss 0.02123506 - samples/sec: 12.94 - lr: 0.012500\n",
      "2022-05-08 02:53:53,690 epoch 65 - iter 128/160 - loss 0.02168860 - samples/sec: 12.23 - lr: 0.012500\n",
      "2022-05-08 02:53:58,997 epoch 65 - iter 144/160 - loss 0.02150175 - samples/sec: 13.34 - lr: 0.012500\n",
      "2022-05-08 02:54:04,264 epoch 65 - iter 160/160 - loss 0.02131716 - samples/sec: 13.38 - lr: 0.012500\n",
      "2022-05-08 02:54:04,764 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:54:04,765 EPOCH 65 done: loss 0.0213 - lr 0.012500\n",
      "2022-05-08 02:54:04,765 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 02:54:06,054 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:54:11,217 epoch 66 - iter 16/160 - loss 0.02144038 - samples/sec: 12.40 - lr: 0.012500\n",
      "2022-05-08 02:54:16,692 epoch 66 - iter 32/160 - loss 0.01840238 - samples/sec: 12.89 - lr: 0.012500\n",
      "2022-05-08 02:54:21,924 epoch 66 - iter 48/160 - loss 0.01756346 - samples/sec: 13.46 - lr: 0.012500\n",
      "2022-05-08 02:54:26,931 epoch 66 - iter 64/160 - loss 0.01865096 - samples/sec: 14.21 - lr: 0.012500\n",
      "2022-05-08 02:54:31,365 epoch 66 - iter 80/160 - loss 0.01918278 - samples/sec: 16.15 - lr: 0.012500\n",
      "2022-05-08 02:54:36,716 epoch 66 - iter 96/160 - loss 0.01965616 - samples/sec: 13.13 - lr: 0.012500\n",
      "2022-05-08 02:54:42,135 epoch 66 - iter 112/160 - loss 0.01999074 - samples/sec: 12.97 - lr: 0.012500\n",
      "2022-05-08 02:54:47,471 epoch 66 - iter 128/160 - loss 0.02061729 - samples/sec: 13.26 - lr: 0.012500\n",
      "2022-05-08 02:54:53,172 epoch 66 - iter 144/160 - loss 0.02016544 - samples/sec: 12.30 - lr: 0.012500\n",
      "2022-05-08 02:54:59,017 epoch 66 - iter 160/160 - loss 0.02000268 - samples/sec: 11.95 - lr: 0.012500\n",
      "2022-05-08 02:54:59,497 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:54:59,497 EPOCH 66 done: loss 0.0200 - lr 0.012500\n",
      "2022-05-08 02:54:59,497 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 02:55:00,862 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:55:05,517 epoch 67 - iter 16/160 - loss 0.01752514 - samples/sec: 13.76 - lr: 0.012500\n",
      "2022-05-08 02:55:11,087 epoch 67 - iter 32/160 - loss 0.01842182 - samples/sec: 12.67 - lr: 0.012500\n",
      "2022-05-08 02:55:16,170 epoch 67 - iter 48/160 - loss 0.01964468 - samples/sec: 13.99 - lr: 0.012500\n",
      "2022-05-08 02:55:21,354 epoch 67 - iter 64/160 - loss 0.01887033 - samples/sec: 13.63 - lr: 0.012500\n",
      "2022-05-08 02:55:26,333 epoch 67 - iter 80/160 - loss 0.01898685 - samples/sec: 14.26 - lr: 0.012500\n",
      "2022-05-08 02:55:31,880 epoch 67 - iter 96/160 - loss 0.01910135 - samples/sec: 12.68 - lr: 0.012500\n",
      "2022-05-08 02:55:37,160 epoch 67 - iter 112/160 - loss 0.01875349 - samples/sec: 13.33 - lr: 0.012500\n",
      "2022-05-08 02:55:42,473 epoch 67 - iter 128/160 - loss 0.01896952 - samples/sec: 13.27 - lr: 0.012500\n",
      "2022-05-08 02:55:47,827 epoch 67 - iter 144/160 - loss 0.01940755 - samples/sec: 13.14 - lr: 0.012500\n",
      "2022-05-08 02:55:53,395 epoch 67 - iter 160/160 - loss 0.01970275 - samples/sec: 12.61 - lr: 0.012500\n",
      "2022-05-08 02:55:53,886 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:55:53,887 EPOCH 67 done: loss 0.0197 - lr 0.012500\n",
      "2022-05-08 02:55:53,888 BAD EPOCHS (no improvement): 3\n",
      "2022-05-08 02:55:55,225 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:55:59,772 epoch 68 - iter 16/160 - loss 0.02147463 - samples/sec: 14.08 - lr: 0.012500\n",
      "2022-05-08 02:56:05,211 epoch 68 - iter 32/160 - loss 0.02263297 - samples/sec: 12.96 - lr: 0.012500\n",
      "2022-05-08 02:56:11,130 epoch 68 - iter 48/160 - loss 0.02129133 - samples/sec: 11.81 - lr: 0.012500\n",
      "2022-05-08 02:56:16,276 epoch 68 - iter 64/160 - loss 0.01942024 - samples/sec: 13.79 - lr: 0.012500\n",
      "2022-05-08 02:56:21,435 epoch 68 - iter 80/160 - loss 0.02064768 - samples/sec: 13.78 - lr: 0.012500\n",
      "2022-05-08 02:56:27,604 epoch 68 - iter 96/160 - loss 0.02042377 - samples/sec: 11.24 - lr: 0.012500\n",
      "2022-05-08 02:56:32,583 epoch 68 - iter 112/160 - loss 0.01947716 - samples/sec: 14.21 - lr: 0.012500\n",
      "2022-05-08 02:56:37,500 epoch 68 - iter 128/160 - loss 0.01998240 - samples/sec: 14.40 - lr: 0.012500\n",
      "2022-05-08 02:56:42,943 epoch 68 - iter 144/160 - loss 0.02019819 - samples/sec: 12.93 - lr: 0.012500\n",
      "2022-05-08 02:56:47,804 epoch 68 - iter 160/160 - loss 0.01973816 - samples/sec: 14.62 - lr: 0.012500\n",
      "2022-05-08 02:56:48,295 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:56:48,295 EPOCH 68 done: loss 0.0197 - lr 0.012500\n",
      "2022-05-08 02:56:48,296 Epoch    68: reducing learning rate of group 0 to 6.2500e-03.\n",
      "2022-05-08 02:56:48,297 BAD EPOCHS (no improvement): 4\n",
      "2022-05-08 02:56:49,600 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:56:54,933 epoch 69 - iter 16/160 - loss 0.01435942 - samples/sec: 12.01 - lr: 0.006250\n",
      "2022-05-08 02:56:59,438 epoch 69 - iter 32/160 - loss 0.01530089 - samples/sec: 15.97 - lr: 0.006250\n",
      "2022-05-08 02:57:04,211 epoch 69 - iter 48/160 - loss 0.01819757 - samples/sec: 14.91 - lr: 0.006250\n",
      "2022-05-08 02:57:09,990 epoch 69 - iter 64/160 - loss 0.02050415 - samples/sec: 12.10 - lr: 0.006250\n",
      "2022-05-08 02:57:15,603 epoch 69 - iter 80/160 - loss 0.02030507 - samples/sec: 12.51 - lr: 0.006250\n",
      "2022-05-08 02:57:20,789 epoch 69 - iter 96/160 - loss 0.02042434 - samples/sec: 13.63 - lr: 0.006250\n",
      "2022-05-08 02:57:26,279 epoch 69 - iter 112/160 - loss 0.02089171 - samples/sec: 12.76 - lr: 0.006250\n",
      "2022-05-08 02:57:32,198 epoch 69 - iter 128/160 - loss 0.01975016 - samples/sec: 11.77 - lr: 0.006250\n",
      "2022-05-08 02:57:37,511 epoch 69 - iter 144/160 - loss 0.01915709 - samples/sec: 13.25 - lr: 0.006250\n",
      "2022-05-08 02:57:42,398 epoch 69 - iter 160/160 - loss 0.01904908 - samples/sec: 14.55 - lr: 0.006250\n",
      "2022-05-08 02:57:42,872 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:57:42,873 EPOCH 69 done: loss 0.0190 - lr 0.006250\n",
      "2022-05-08 02:57:42,873 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 02:57:44,183 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:57:48,966 epoch 70 - iter 16/160 - loss 0.02589858 - samples/sec: 13.39 - lr: 0.006250\n",
      "2022-05-08 02:57:54,171 epoch 70 - iter 32/160 - loss 0.02118905 - samples/sec: 13.59 - lr: 0.006250\n",
      "2022-05-08 02:57:59,294 epoch 70 - iter 48/160 - loss 0.02115037 - samples/sec: 13.79 - lr: 0.006250\n",
      "2022-05-08 02:58:04,495 epoch 70 - iter 64/160 - loss 0.02090888 - samples/sec: 13.58 - lr: 0.006250\n",
      "2022-05-08 02:58:10,040 epoch 70 - iter 80/160 - loss 0.01953516 - samples/sec: 12.73 - lr: 0.006250\n",
      "2022-05-08 02:58:15,379 epoch 70 - iter 96/160 - loss 0.01960091 - samples/sec: 13.17 - lr: 0.006250\n",
      "2022-05-08 02:58:20,812 epoch 70 - iter 112/160 - loss 0.01978372 - samples/sec: 12.95 - lr: 0.006250\n",
      "2022-05-08 02:58:25,949 epoch 70 - iter 128/160 - loss 0.01965427 - samples/sec: 13.81 - lr: 0.006250\n",
      "2022-05-08 02:58:31,482 epoch 70 - iter 144/160 - loss 0.01900378 - samples/sec: 12.68 - lr: 0.006250\n",
      "2022-05-08 02:58:36,648 epoch 70 - iter 160/160 - loss 0.01970916 - samples/sec: 13.66 - lr: 0.006250\n",
      "2022-05-08 02:58:37,127 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:58:37,127 EPOCH 70 done: loss 0.0197 - lr 0.006250\n",
      "2022-05-08 02:58:37,128 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 02:58:38,433 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:58:43,671 epoch 71 - iter 16/160 - loss 0.02399934 - samples/sec: 12.22 - lr: 0.006250\n",
      "2022-05-08 02:58:48,922 epoch 71 - iter 32/160 - loss 0.02272123 - samples/sec: 13.48 - lr: 0.006250\n",
      "2022-05-08 02:58:54,475 epoch 71 - iter 48/160 - loss 0.02101248 - samples/sec: 12.64 - lr: 0.006250\n",
      "2022-05-08 02:58:59,752 epoch 71 - iter 64/160 - loss 0.01962650 - samples/sec: 13.36 - lr: 0.006250\n",
      "2022-05-08 02:59:04,833 epoch 71 - iter 80/160 - loss 0.01927225 - samples/sec: 13.92 - lr: 0.006250\n",
      "2022-05-08 02:59:09,843 epoch 71 - iter 96/160 - loss 0.01872204 - samples/sec: 14.14 - lr: 0.006250\n",
      "2022-05-08 02:59:15,301 epoch 71 - iter 112/160 - loss 0.01898165 - samples/sec: 12.98 - lr: 0.006250\n",
      "2022-05-08 02:59:20,411 epoch 71 - iter 128/160 - loss 0.01906209 - samples/sec: 13.82 - lr: 0.006250\n",
      "2022-05-08 02:59:26,051 epoch 71 - iter 144/160 - loss 0.01939461 - samples/sec: 12.48 - lr: 0.006250\n",
      "2022-05-08 02:59:30,549 epoch 71 - iter 160/160 - loss 0.01934984 - samples/sec: 16.00 - lr: 0.006250\n",
      "2022-05-08 02:59:31,027 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:59:31,027 EPOCH 71 done: loss 0.0193 - lr 0.006250\n",
      "2022-05-08 02:59:31,028 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 02:59:32,331 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 02:59:37,408 epoch 72 - iter 16/160 - loss 0.02284252 - samples/sec: 12.61 - lr: 0.006250\n",
      "2022-05-08 02:59:42,271 epoch 72 - iter 32/160 - loss 0.02281218 - samples/sec: 14.62 - lr: 0.006250\n",
      "2022-05-08 02:59:47,559 epoch 72 - iter 48/160 - loss 0.02253872 - samples/sec: 13.38 - lr: 0.006250\n",
      "2022-05-08 02:59:52,764 epoch 72 - iter 64/160 - loss 0.02052818 - samples/sec: 13.55 - lr: 0.006250\n",
      "2022-05-08 02:59:58,280 epoch 72 - iter 80/160 - loss 0.01937907 - samples/sec: 12.71 - lr: 0.006250\n",
      "2022-05-08 03:00:03,913 epoch 72 - iter 96/160 - loss 0.02034215 - samples/sec: 12.46 - lr: 0.006250\n",
      "2022-05-08 03:00:09,141 epoch 72 - iter 112/160 - loss 0.02066838 - samples/sec: 13.52 - lr: 0.006250\n",
      "2022-05-08 03:00:14,491 epoch 72 - iter 128/160 - loss 0.01979178 - samples/sec: 13.17 - lr: 0.006250\n",
      "2022-05-08 03:00:19,755 epoch 72 - iter 144/160 - loss 0.01923730 - samples/sec: 13.43 - lr: 0.006250\n",
      "2022-05-08 03:00:24,820 epoch 72 - iter 160/160 - loss 0.01925099 - samples/sec: 14.04 - lr: 0.006250\n",
      "2022-05-08 03:00:25,300 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:00:25,301 EPOCH 72 done: loss 0.0193 - lr 0.006250\n",
      "2022-05-08 03:00:25,302 BAD EPOCHS (no improvement): 3\n",
      "2022-05-08 03:00:26,637 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:00:31,427 epoch 73 - iter 16/160 - loss 0.02387759 - samples/sec: 13.37 - lr: 0.006250\n",
      "2022-05-08 03:00:36,398 epoch 73 - iter 32/160 - loss 0.02326216 - samples/sec: 14.29 - lr: 0.006250\n",
      "2022-05-08 03:00:41,463 epoch 73 - iter 48/160 - loss 0.02182113 - samples/sec: 13.99 - lr: 0.006250\n",
      "2022-05-08 03:00:46,943 epoch 73 - iter 64/160 - loss 0.02330460 - samples/sec: 12.80 - lr: 0.006250\n",
      "2022-05-08 03:00:51,649 epoch 73 - iter 80/160 - loss 0.02286123 - samples/sec: 15.20 - lr: 0.006250\n",
      "2022-05-08 03:00:57,456 epoch 73 - iter 96/160 - loss 0.02304979 - samples/sec: 12.01 - lr: 0.006250\n",
      "2022-05-08 03:01:03,154 epoch 73 - iter 112/160 - loss 0.02312529 - samples/sec: 12.28 - lr: 0.006250\n",
      "2022-05-08 03:01:08,909 epoch 73 - iter 128/160 - loss 0.02191446 - samples/sec: 12.16 - lr: 0.006250\n",
      "2022-05-08 03:01:13,969 epoch 73 - iter 144/160 - loss 0.02148845 - samples/sec: 13.98 - lr: 0.006250\n",
      "2022-05-08 03:01:19,393 epoch 73 - iter 160/160 - loss 0.02097974 - samples/sec: 12.96 - lr: 0.006250\n",
      "2022-05-08 03:01:19,889 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:01:19,890 EPOCH 73 done: loss 0.0210 - lr 0.006250\n",
      "2022-05-08 03:01:19,890 Epoch    73: reducing learning rate of group 0 to 3.1250e-03.\n",
      "2022-05-08 03:01:19,891 BAD EPOCHS (no improvement): 4\n",
      "2022-05-08 03:01:21,216 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:01:25,976 epoch 74 - iter 16/160 - loss 0.01353403 - samples/sec: 13.45 - lr: 0.003125\n",
      "2022-05-08 03:01:31,418 epoch 74 - iter 32/160 - loss 0.01894123 - samples/sec: 12.91 - lr: 0.003125\n",
      "2022-05-08 03:01:37,052 epoch 74 - iter 48/160 - loss 0.02036941 - samples/sec: 12.44 - lr: 0.003125\n",
      "2022-05-08 03:01:42,385 epoch 74 - iter 64/160 - loss 0.02063507 - samples/sec: 13.27 - lr: 0.003125\n",
      "2022-05-08 03:01:47,480 epoch 74 - iter 80/160 - loss 0.02199593 - samples/sec: 13.93 - lr: 0.003125\n",
      "2022-05-08 03:01:52,843 epoch 74 - iter 96/160 - loss 0.02178478 - samples/sec: 13.12 - lr: 0.003125\n",
      "2022-05-08 03:01:57,963 epoch 74 - iter 112/160 - loss 0.02101613 - samples/sec: 13.78 - lr: 0.003125\n",
      "2022-05-08 03:02:03,563 epoch 74 - iter 128/160 - loss 0.02069788 - samples/sec: 12.51 - lr: 0.003125\n",
      "2022-05-08 03:02:08,811 epoch 74 - iter 144/160 - loss 0.01965387 - samples/sec: 13.49 - lr: 0.003125\n",
      "2022-05-08 03:02:14,222 epoch 74 - iter 160/160 - loss 0.01977928 - samples/sec: 12.99 - lr: 0.003125\n",
      "2022-05-08 03:02:14,713 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:02:14,714 EPOCH 74 done: loss 0.0198 - lr 0.003125\n",
      "2022-05-08 03:02:14,714 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 03:02:16,483 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:02:21,747 epoch 75 - iter 16/160 - loss 0.01801196 - samples/sec: 12.16 - lr: 0.003125\n",
      "2022-05-08 03:02:27,681 epoch 75 - iter 32/160 - loss 0.02305609 - samples/sec: 11.74 - lr: 0.003125\n",
      "2022-05-08 03:02:32,796 epoch 75 - iter 48/160 - loss 0.01906945 - samples/sec: 13.82 - lr: 0.003125\n",
      "2022-05-08 03:02:37,916 epoch 75 - iter 64/160 - loss 0.01958690 - samples/sec: 13.87 - lr: 0.003125\n",
      "2022-05-08 03:02:42,803 epoch 75 - iter 80/160 - loss 0.01993783 - samples/sec: 14.57 - lr: 0.003125\n",
      "2022-05-08 03:02:48,198 epoch 75 - iter 96/160 - loss 0.02070438 - samples/sec: 13.03 - lr: 0.003125\n",
      "2022-05-08 03:02:53,534 epoch 75 - iter 112/160 - loss 0.02051792 - samples/sec: 13.18 - lr: 0.003125\n",
      "2022-05-08 03:02:58,568 epoch 75 - iter 128/160 - loss 0.01979839 - samples/sec: 14.05 - lr: 0.003125\n",
      "2022-05-08 03:03:03,695 epoch 75 - iter 144/160 - loss 0.01941280 - samples/sec: 13.86 - lr: 0.003125\n",
      "2022-05-08 03:03:08,843 epoch 75 - iter 160/160 - loss 0.01940057 - samples/sec: 13.70 - lr: 0.003125\n",
      "2022-05-08 03:03:09,347 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:03:09,347 EPOCH 75 done: loss 0.0194 - lr 0.003125\n",
      "2022-05-08 03:03:09,348 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 03:03:10,636 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:03:15,393 epoch 76 - iter 16/160 - loss 0.01778101 - samples/sec: 13.46 - lr: 0.003125\n",
      "2022-05-08 03:03:20,635 epoch 76 - iter 32/160 - loss 0.01956492 - samples/sec: 13.44 - lr: 0.003125\n",
      "2022-05-08 03:03:25,300 epoch 76 - iter 48/160 - loss 0.01995225 - samples/sec: 15.37 - lr: 0.003125\n",
      "2022-05-08 03:03:30,841 epoch 76 - iter 64/160 - loss 0.01822898 - samples/sec: 12.63 - lr: 0.003125\n",
      "2022-05-08 03:03:36,188 epoch 76 - iter 80/160 - loss 0.01841582 - samples/sec: 13.16 - lr: 0.003125\n",
      "2022-05-08 03:03:41,699 epoch 76 - iter 96/160 - loss 0.01957438 - samples/sec: 12.77 - lr: 0.003125\n",
      "2022-05-08 03:03:46,968 epoch 76 - iter 112/160 - loss 0.01881849 - samples/sec: 13.36 - lr: 0.003125\n",
      "2022-05-08 03:03:52,348 epoch 76 - iter 128/160 - loss 0.01924285 - samples/sec: 13.09 - lr: 0.003125\n",
      "2022-05-08 03:03:57,810 epoch 76 - iter 144/160 - loss 0.01959301 - samples/sec: 12.94 - lr: 0.003125\n",
      "2022-05-08 03:04:03,298 epoch 76 - iter 160/160 - loss 0.01945383 - samples/sec: 12.77 - lr: 0.003125\n",
      "2022-05-08 03:04:03,780 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:04:03,780 EPOCH 76 done: loss 0.0195 - lr 0.003125\n",
      "2022-05-08 03:04:03,781 BAD EPOCHS (no improvement): 3\n",
      "2022-05-08 03:04:05,103 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:04:09,976 epoch 77 - iter 16/160 - loss 0.02810219 - samples/sec: 13.14 - lr: 0.003125\n",
      "2022-05-08 03:04:15,378 epoch 77 - iter 32/160 - loss 0.02476701 - samples/sec: 13.00 - lr: 0.003125\n",
      "2022-05-08 03:04:19,893 epoch 77 - iter 48/160 - loss 0.02398683 - samples/sec: 15.88 - lr: 0.003125\n",
      "2022-05-08 03:04:25,188 epoch 77 - iter 64/160 - loss 0.02098550 - samples/sec: 13.29 - lr: 0.003125\n",
      "2022-05-08 03:04:30,625 epoch 77 - iter 80/160 - loss 0.01898932 - samples/sec: 12.93 - lr: 0.003125\n",
      "2022-05-08 03:04:36,397 epoch 77 - iter 96/160 - loss 0.01950384 - samples/sec: 12.14 - lr: 0.003125\n",
      "2022-05-08 03:04:41,720 epoch 77 - iter 112/160 - loss 0.01919280 - samples/sec: 13.26 - lr: 0.003125\n",
      "2022-05-08 03:04:46,764 epoch 77 - iter 128/160 - loss 0.01939304 - samples/sec: 14.03 - lr: 0.003125\n",
      "2022-05-08 03:04:52,106 epoch 77 - iter 144/160 - loss 0.01876970 - samples/sec: 13.16 - lr: 0.003125\n",
      "2022-05-08 03:04:57,555 epoch 77 - iter 160/160 - loss 0.01875445 - samples/sec: 12.89 - lr: 0.003125\n",
      "2022-05-08 03:04:58,034 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:04:58,035 EPOCH 77 done: loss 0.0188 - lr 0.003125\n",
      "2022-05-08 03:04:58,035 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 03:04:59,372 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:05:03,806 epoch 78 - iter 16/160 - loss 0.02291669 - samples/sec: 14.44 - lr: 0.003125\n",
      "2022-05-08 03:05:09,456 epoch 78 - iter 32/160 - loss 0.01788938 - samples/sec: 12.47 - lr: 0.003125\n",
      "2022-05-08 03:05:15,052 epoch 78 - iter 48/160 - loss 0.01754348 - samples/sec: 12.53 - lr: 0.003125\n",
      "2022-05-08 03:05:20,285 epoch 78 - iter 64/160 - loss 0.01820403 - samples/sec: 13.55 - lr: 0.003125\n",
      "2022-05-08 03:05:25,265 epoch 78 - iter 80/160 - loss 0.01850343 - samples/sec: 14.25 - lr: 0.003125\n",
      "2022-05-08 03:05:30,192 epoch 78 - iter 96/160 - loss 0.01765103 - samples/sec: 14.39 - lr: 0.003125\n",
      "2022-05-08 03:05:35,836 epoch 78 - iter 112/160 - loss 0.01870631 - samples/sec: 12.41 - lr: 0.003125\n",
      "2022-05-08 03:05:41,107 epoch 78 - iter 128/160 - loss 0.01933854 - samples/sec: 13.36 - lr: 0.003125\n",
      "2022-05-08 03:05:46,474 epoch 78 - iter 144/160 - loss 0.01972149 - samples/sec: 13.17 - lr: 0.003125\n",
      "2022-05-08 03:05:52,119 epoch 78 - iter 160/160 - loss 0.01955041 - samples/sec: 12.40 - lr: 0.003125\n",
      "2022-05-08 03:05:52,606 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:05:52,607 EPOCH 78 done: loss 0.0196 - lr 0.003125\n",
      "2022-05-08 03:05:52,608 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 03:05:53,897 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:05:58,338 epoch 79 - iter 16/160 - loss 0.01694979 - samples/sec: 14.41 - lr: 0.003125\n",
      "2022-05-08 03:06:03,347 epoch 79 - iter 32/160 - loss 0.01719515 - samples/sec: 14.17 - lr: 0.003125\n",
      "2022-05-08 03:06:08,954 epoch 79 - iter 48/160 - loss 0.01997200 - samples/sec: 12.47 - lr: 0.003125\n",
      "2022-05-08 03:06:13,999 epoch 79 - iter 64/160 - loss 0.01965967 - samples/sec: 14.06 - lr: 0.003125\n",
      "2022-05-08 03:06:19,345 epoch 79 - iter 80/160 - loss 0.01890062 - samples/sec: 13.15 - lr: 0.003125\n",
      "2022-05-08 03:06:24,918 epoch 79 - iter 96/160 - loss 0.01786697 - samples/sec: 12.58 - lr: 0.003125\n",
      "2022-05-08 03:06:30,210 epoch 79 - iter 112/160 - loss 0.01794019 - samples/sec: 13.30 - lr: 0.003125\n",
      "2022-05-08 03:06:35,516 epoch 79 - iter 128/160 - loss 0.01786432 - samples/sec: 13.33 - lr: 0.003125\n",
      "2022-05-08 03:06:40,970 epoch 79 - iter 144/160 - loss 0.01872338 - samples/sec: 12.94 - lr: 0.003125\n",
      "2022-05-08 03:06:46,495 epoch 79 - iter 160/160 - loss 0.01864351 - samples/sec: 12.70 - lr: 0.003125\n",
      "2022-05-08 03:06:46,971 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:06:46,971 EPOCH 79 done: loss 0.0186 - lr 0.003125\n",
      "2022-05-08 03:06:46,972 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 03:06:48,298 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:06:53,032 epoch 80 - iter 16/160 - loss 0.01771635 - samples/sec: 13.53 - lr: 0.003125\n",
      "2022-05-08 03:06:58,371 epoch 80 - iter 32/160 - loss 0.02008391 - samples/sec: 13.22 - lr: 0.003125\n",
      "2022-05-08 03:07:03,887 epoch 80 - iter 48/160 - loss 0.01905885 - samples/sec: 12.72 - lr: 0.003125\n",
      "2022-05-08 03:07:09,663 epoch 80 - iter 64/160 - loss 0.01933794 - samples/sec: 12.10 - lr: 0.003125\n",
      "2022-05-08 03:07:14,469 epoch 80 - iter 80/160 - loss 0.01857715 - samples/sec: 14.85 - lr: 0.003125\n",
      "2022-05-08 03:07:19,842 epoch 80 - iter 96/160 - loss 0.01868075 - samples/sec: 13.10 - lr: 0.003125\n",
      "2022-05-08 03:07:25,090 epoch 80 - iter 112/160 - loss 0.01828760 - samples/sec: 13.46 - lr: 0.003125\n",
      "2022-05-08 03:07:30,383 epoch 80 - iter 128/160 - loss 0.01831991 - samples/sec: 13.30 - lr: 0.003125\n",
      "2022-05-08 03:07:35,851 epoch 80 - iter 144/160 - loss 0.01794285 - samples/sec: 12.87 - lr: 0.003125\n",
      "2022-05-08 03:07:41,331 epoch 80 - iter 160/160 - loss 0.01851864 - samples/sec: 12.82 - lr: 0.003125\n",
      "2022-05-08 03:07:41,813 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:07:41,813 EPOCH 80 done: loss 0.0185 - lr 0.003125\n",
      "2022-05-08 03:07:41,814 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 03:07:43,136 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:07:47,868 epoch 81 - iter 16/160 - loss 0.01918316 - samples/sec: 13.53 - lr: 0.003125\n",
      "2022-05-08 03:07:53,333 epoch 81 - iter 32/160 - loss 0.01987813 - samples/sec: 12.89 - lr: 0.003125\n",
      "2022-05-08 03:07:58,463 epoch 81 - iter 48/160 - loss 0.01903999 - samples/sec: 13.82 - lr: 0.003125\n",
      "2022-05-08 03:08:03,726 epoch 81 - iter 64/160 - loss 0.01835861 - samples/sec: 13.40 - lr: 0.003125\n",
      "2022-05-08 03:08:08,857 epoch 81 - iter 80/160 - loss 0.01756433 - samples/sec: 13.79 - lr: 0.003125\n",
      "2022-05-08 03:08:14,522 epoch 81 - iter 96/160 - loss 0.01724187 - samples/sec: 12.40 - lr: 0.003125\n",
      "2022-05-08 03:08:19,762 epoch 81 - iter 112/160 - loss 0.01814226 - samples/sec: 13.46 - lr: 0.003125\n",
      "2022-05-08 03:08:24,953 epoch 81 - iter 128/160 - loss 0.01879564 - samples/sec: 13.61 - lr: 0.003125\n",
      "2022-05-08 03:08:30,446 epoch 81 - iter 144/160 - loss 0.01887869 - samples/sec: 12.77 - lr: 0.003125\n",
      "2022-05-08 03:08:35,543 epoch 81 - iter 160/160 - loss 0.01902049 - samples/sec: 13.88 - lr: 0.003125\n",
      "2022-05-08 03:08:36,017 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:08:36,018 EPOCH 81 done: loss 0.0190 - lr 0.003125\n",
      "2022-05-08 03:08:36,019 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 03:08:37,331 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:08:42,591 epoch 82 - iter 16/160 - loss 0.01716573 - samples/sec: 12.17 - lr: 0.003125\n",
      "2022-05-08 03:08:47,509 epoch 82 - iter 32/160 - loss 0.01445321 - samples/sec: 14.45 - lr: 0.003125\n",
      "2022-05-08 03:08:52,917 epoch 82 - iter 48/160 - loss 0.01793667 - samples/sec: 13.00 - lr: 0.003125\n",
      "2022-05-08 03:08:58,195 epoch 82 - iter 64/160 - loss 0.01840510 - samples/sec: 13.35 - lr: 0.003125\n",
      "2022-05-08 03:09:03,650 epoch 82 - iter 80/160 - loss 0.01800326 - samples/sec: 12.86 - lr: 0.003125\n",
      "2022-05-08 03:09:08,761 epoch 82 - iter 96/160 - loss 0.01799317 - samples/sec: 13.84 - lr: 0.003125\n",
      "2022-05-08 03:09:13,617 epoch 82 - iter 112/160 - loss 0.01792303 - samples/sec: 14.60 - lr: 0.003125\n",
      "2022-05-08 03:09:18,916 epoch 82 - iter 128/160 - loss 0.01878333 - samples/sec: 13.34 - lr: 0.003125\n",
      "2022-05-08 03:09:24,666 epoch 82 - iter 144/160 - loss 0.01920740 - samples/sec: 12.16 - lr: 0.003125\n",
      "2022-05-08 03:09:29,947 epoch 82 - iter 160/160 - loss 0.01893234 - samples/sec: 13.35 - lr: 0.003125\n",
      "2022-05-08 03:09:30,418 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:09:30,419 EPOCH 82 done: loss 0.0189 - lr 0.003125\n",
      "2022-05-08 03:09:30,419 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 03:09:31,712 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:09:36,211 epoch 83 - iter 16/160 - loss 0.03120142 - samples/sec: 14.23 - lr: 0.003125\n",
      "2022-05-08 03:09:40,781 epoch 83 - iter 32/160 - loss 0.02501066 - samples/sec: 15.68 - lr: 0.003125\n",
      "2022-05-08 03:09:45,923 epoch 83 - iter 48/160 - loss 0.02248002 - samples/sec: 13.78 - lr: 0.003125\n",
      "2022-05-08 03:09:51,746 epoch 83 - iter 64/160 - loss 0.02019099 - samples/sec: 12.03 - lr: 0.003125\n",
      "2022-05-08 03:09:57,222 epoch 83 - iter 80/160 - loss 0.01935302 - samples/sec: 12.82 - lr: 0.003125\n",
      "2022-05-08 03:10:03,103 epoch 83 - iter 96/160 - loss 0.01863782 - samples/sec: 11.90 - lr: 0.003125\n",
      "2022-05-08 03:10:08,361 epoch 83 - iter 112/160 - loss 0.01946936 - samples/sec: 13.40 - lr: 0.003125\n",
      "2022-05-08 03:10:13,712 epoch 83 - iter 128/160 - loss 0.01889748 - samples/sec: 13.14 - lr: 0.003125\n",
      "2022-05-08 03:10:18,965 epoch 83 - iter 144/160 - loss 0.01940969 - samples/sec: 13.39 - lr: 0.003125\n",
      "2022-05-08 03:10:24,527 epoch 83 - iter 160/160 - loss 0.01877256 - samples/sec: 12.60 - lr: 0.003125\n",
      "2022-05-08 03:10:25,013 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:10:25,013 EPOCH 83 done: loss 0.0188 - lr 0.003125\n",
      "2022-05-08 03:10:25,014 BAD EPOCHS (no improvement): 3\n",
      "2022-05-08 03:10:26,310 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:10:31,211 epoch 84 - iter 16/160 - loss 0.01283577 - samples/sec: 13.06 - lr: 0.003125\n",
      "2022-05-08 03:10:36,087 epoch 84 - iter 32/160 - loss 0.01327840 - samples/sec: 14.60 - lr: 0.003125\n",
      "2022-05-08 03:10:41,449 epoch 84 - iter 48/160 - loss 0.01574507 - samples/sec: 13.08 - lr: 0.003125\n",
      "2022-05-08 03:10:46,277 epoch 84 - iter 64/160 - loss 0.01655404 - samples/sec: 14.75 - lr: 0.003125\n",
      "2022-05-08 03:10:52,251 epoch 84 - iter 80/160 - loss 0.01801696 - samples/sec: 11.66 - lr: 0.003125\n",
      "2022-05-08 03:10:57,678 epoch 84 - iter 96/160 - loss 0.01894917 - samples/sec: 12.98 - lr: 0.003125\n",
      "2022-05-08 03:11:03,712 epoch 84 - iter 112/160 - loss 0.01924752 - samples/sec: 11.53 - lr: 0.003125\n",
      "2022-05-08 03:11:08,858 epoch 84 - iter 128/160 - loss 0.01936623 - samples/sec: 13.75 - lr: 0.003125\n",
      "2022-05-08 03:11:13,887 epoch 84 - iter 144/160 - loss 0.01897620 - samples/sec: 14.08 - lr: 0.003125\n",
      "2022-05-08 03:11:18,518 epoch 84 - iter 160/160 - loss 0.01921148 - samples/sec: 15.53 - lr: 0.003125\n",
      "2022-05-08 03:11:19,007 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:11:19,008 EPOCH 84 done: loss 0.0192 - lr 0.003125\n",
      "2022-05-08 03:11:19,008 Epoch    84: reducing learning rate of group 0 to 1.5625e-03.\n",
      "2022-05-08 03:11:19,009 BAD EPOCHS (no improvement): 4\n",
      "2022-05-08 03:11:20,303 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:11:25,597 epoch 85 - iter 16/160 - loss 0.02077618 - samples/sec: 12.09 - lr: 0.001563\n",
      "2022-05-08 03:11:31,051 epoch 85 - iter 32/160 - loss 0.01832539 - samples/sec: 12.89 - lr: 0.001563\n",
      "2022-05-08 03:11:36,302 epoch 85 - iter 48/160 - loss 0.02185775 - samples/sec: 13.44 - lr: 0.001563\n",
      "2022-05-08 03:11:41,215 epoch 85 - iter 64/160 - loss 0.02221171 - samples/sec: 14.48 - lr: 0.001563\n",
      "2022-05-08 03:11:46,304 epoch 85 - iter 80/160 - loss 0.02041298 - samples/sec: 13.96 - lr: 0.001563\n",
      "2022-05-08 03:11:52,093 epoch 85 - iter 96/160 - loss 0.01952234 - samples/sec: 12.05 - lr: 0.001563\n",
      "2022-05-08 03:11:57,543 epoch 85 - iter 112/160 - loss 0.01926699 - samples/sec: 12.93 - lr: 0.001563\n",
      "2022-05-08 03:12:02,701 epoch 85 - iter 128/160 - loss 0.01885795 - samples/sec: 13.67 - lr: 0.001563\n",
      "2022-05-08 03:12:08,206 epoch 85 - iter 144/160 - loss 0.01975179 - samples/sec: 12.83 - lr: 0.001563\n",
      "2022-05-08 03:12:12,908 epoch 85 - iter 160/160 - loss 0.01937321 - samples/sec: 15.20 - lr: 0.001563\n",
      "2022-05-08 03:12:13,385 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:12:13,386 EPOCH 85 done: loss 0.0194 - lr 0.001563\n",
      "2022-05-08 03:12:13,387 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 03:12:14,680 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:12:20,317 epoch 86 - iter 16/160 - loss 0.01291970 - samples/sec: 11.36 - lr: 0.001563\n",
      "2022-05-08 03:12:25,837 epoch 86 - iter 32/160 - loss 0.01419530 - samples/sec: 12.69 - lr: 0.001563\n",
      "2022-05-08 03:12:31,605 epoch 86 - iter 48/160 - loss 0.01428845 - samples/sec: 12.15 - lr: 0.001563\n",
      "2022-05-08 03:12:36,458 epoch 86 - iter 64/160 - loss 0.01528852 - samples/sec: 14.65 - lr: 0.001563\n",
      "2022-05-08 03:12:41,651 epoch 86 - iter 80/160 - loss 0.01553574 - samples/sec: 13.57 - lr: 0.001563\n",
      "2022-05-08 03:12:47,477 epoch 86 - iter 96/160 - loss 0.01766241 - samples/sec: 11.96 - lr: 0.001563\n",
      "2022-05-08 03:12:52,405 epoch 86 - iter 112/160 - loss 0.01769373 - samples/sec: 14.39 - lr: 0.001563\n",
      "2022-05-08 03:12:57,252 epoch 86 - iter 128/160 - loss 0.01826994 - samples/sec: 14.71 - lr: 0.001563\n",
      "2022-05-08 03:13:02,231 epoch 86 - iter 144/160 - loss 0.01838234 - samples/sec: 14.29 - lr: 0.001563\n",
      "2022-05-08 03:13:07,109 epoch 86 - iter 160/160 - loss 0.01865682 - samples/sec: 14.55 - lr: 0.001563\n",
      "2022-05-08 03:13:07,581 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:13:07,582 EPOCH 86 done: loss 0.0187 - lr 0.001563\n",
      "2022-05-08 03:13:07,582 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 03:13:08,906 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:13:14,164 epoch 87 - iter 16/160 - loss 0.02213549 - samples/sec: 12.18 - lr: 0.001563\n",
      "2022-05-08 03:13:19,017 epoch 87 - iter 32/160 - loss 0.02017239 - samples/sec: 14.72 - lr: 0.001563\n",
      "2022-05-08 03:13:23,999 epoch 87 - iter 48/160 - loss 0.02024648 - samples/sec: 14.19 - lr: 0.001563\n",
      "2022-05-08 03:13:29,111 epoch 87 - iter 64/160 - loss 0.01958120 - samples/sec: 13.90 - lr: 0.001563\n",
      "2022-05-08 03:13:34,596 epoch 87 - iter 80/160 - loss 0.01835947 - samples/sec: 12.84 - lr: 0.001563\n",
      "2022-05-08 03:13:39,994 epoch 87 - iter 96/160 - loss 0.01849992 - samples/sec: 13.03 - lr: 0.001563\n",
      "2022-05-08 03:13:45,547 epoch 87 - iter 112/160 - loss 0.01812511 - samples/sec: 12.60 - lr: 0.001563\n",
      "2022-05-08 03:13:50,724 epoch 87 - iter 128/160 - loss 0.01868022 - samples/sec: 13.62 - lr: 0.001563\n",
      "2022-05-08 03:13:55,869 epoch 87 - iter 144/160 - loss 0.01917417 - samples/sec: 13.70 - lr: 0.001563\n",
      "2022-05-08 03:14:01,316 epoch 87 - iter 160/160 - loss 0.01860339 - samples/sec: 12.89 - lr: 0.001563\n",
      "2022-05-08 03:14:01,807 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:14:01,807 EPOCH 87 done: loss 0.0186 - lr 0.001563\n",
      "2022-05-08 03:14:01,808 BAD EPOCHS (no improvement): 3\n",
      "2022-05-08 03:14:03,087 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:14:08,018 epoch 88 - iter 16/160 - loss 0.02262140 - samples/sec: 12.98 - lr: 0.001563\n",
      "2022-05-08 03:14:13,141 epoch 88 - iter 32/160 - loss 0.01911146 - samples/sec: 13.81 - lr: 0.001563\n",
      "2022-05-08 03:14:18,563 epoch 88 - iter 48/160 - loss 0.01698127 - samples/sec: 12.97 - lr: 0.001563\n",
      "2022-05-08 03:14:23,636 epoch 88 - iter 64/160 - loss 0.01765191 - samples/sec: 13.96 - lr: 0.001563\n",
      "2022-05-08 03:14:29,371 epoch 88 - iter 80/160 - loss 0.01777998 - samples/sec: 12.19 - lr: 0.001563\n",
      "2022-05-08 03:14:34,581 epoch 88 - iter 96/160 - loss 0.01742777 - samples/sec: 13.55 - lr: 0.001563\n",
      "2022-05-08 03:14:40,374 epoch 88 - iter 112/160 - loss 0.01775711 - samples/sec: 12.06 - lr: 0.001563\n",
      "2022-05-08 03:14:45,899 epoch 88 - iter 128/160 - loss 0.01880198 - samples/sec: 12.69 - lr: 0.001563\n",
      "2022-05-08 03:14:51,262 epoch 88 - iter 144/160 - loss 0.01883951 - samples/sec: 13.14 - lr: 0.001563\n",
      "2022-05-08 03:14:55,920 epoch 88 - iter 160/160 - loss 0.01904698 - samples/sec: 15.31 - lr: 0.001563\n",
      "2022-05-08 03:14:56,421 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:14:56,421 EPOCH 88 done: loss 0.0190 - lr 0.001563\n",
      "2022-05-08 03:14:56,422 Epoch    88: reducing learning rate of group 0 to 7.8125e-04.\n",
      "2022-05-08 03:14:56,422 BAD EPOCHS (no improvement): 4\n",
      "2022-05-08 03:14:57,725 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:15:02,863 epoch 89 - iter 16/160 - loss 0.01829257 - samples/sec: 12.46 - lr: 0.000781\n",
      "2022-05-08 03:15:07,794 epoch 89 - iter 32/160 - loss 0.01896581 - samples/sec: 14.39 - lr: 0.000781\n",
      "2022-05-08 03:15:13,127 epoch 89 - iter 48/160 - loss 0.01876921 - samples/sec: 13.21 - lr: 0.000781\n",
      "2022-05-08 03:15:18,094 epoch 89 - iter 64/160 - loss 0.01822946 - samples/sec: 14.32 - lr: 0.000781\n",
      "2022-05-08 03:15:23,514 epoch 89 - iter 80/160 - loss 0.01824294 - samples/sec: 12.99 - lr: 0.000781\n",
      "2022-05-08 03:15:28,688 epoch 89 - iter 96/160 - loss 0.01915499 - samples/sec: 13.66 - lr: 0.000781\n",
      "2022-05-08 03:15:34,040 epoch 89 - iter 112/160 - loss 0.01905146 - samples/sec: 13.19 - lr: 0.000781\n",
      "2022-05-08 03:15:39,558 epoch 89 - iter 128/160 - loss 0.01956432 - samples/sec: 12.73 - lr: 0.000781\n",
      "2022-05-08 03:15:44,646 epoch 89 - iter 144/160 - loss 0.01946922 - samples/sec: 14.03 - lr: 0.000781\n",
      "2022-05-08 03:15:49,971 epoch 89 - iter 160/160 - loss 0.01979223 - samples/sec: 13.22 - lr: 0.000781\n",
      "2022-05-08 03:15:50,461 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:15:50,462 EPOCH 89 done: loss 0.0198 - lr 0.000781\n",
      "2022-05-08 03:15:50,463 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 03:15:51,773 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:15:56,819 epoch 90 - iter 16/160 - loss 0.02162255 - samples/sec: 12.69 - lr: 0.000781\n",
      "2022-05-08 03:16:01,504 epoch 90 - iter 32/160 - loss 0.02096812 - samples/sec: 15.28 - lr: 0.000781\n",
      "2022-05-08 03:16:07,336 epoch 90 - iter 48/160 - loss 0.01842129 - samples/sec: 11.99 - lr: 0.000781\n",
      "2022-05-08 03:16:12,261 epoch 90 - iter 64/160 - loss 0.01726553 - samples/sec: 14.54 - lr: 0.000781\n",
      "2022-05-08 03:16:17,621 epoch 90 - iter 80/160 - loss 0.01759840 - samples/sec: 13.13 - lr: 0.000781\n",
      "2022-05-08 03:16:22,256 epoch 90 - iter 96/160 - loss 0.01751586 - samples/sec: 15.41 - lr: 0.000781\n",
      "2022-05-08 03:16:27,691 epoch 90 - iter 112/160 - loss 0.01738693 - samples/sec: 12.94 - lr: 0.000781\n",
      "2022-05-08 03:16:33,207 epoch 90 - iter 128/160 - loss 0.01852294 - samples/sec: 12.72 - lr: 0.000781\n",
      "2022-05-08 03:16:38,828 epoch 90 - iter 144/160 - loss 0.01830194 - samples/sec: 12.46 - lr: 0.000781\n",
      "2022-05-08 03:16:44,151 epoch 90 - iter 160/160 - loss 0.01831039 - samples/sec: 13.20 - lr: 0.000781\n",
      "2022-05-08 03:16:44,631 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:16:44,631 EPOCH 90 done: loss 0.0183 - lr 0.000781\n",
      "2022-05-08 03:16:44,631 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 03:16:45,930 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:16:50,716 epoch 91 - iter 16/160 - loss 0.01115586 - samples/sec: 13.38 - lr: 0.000781\n",
      "2022-05-08 03:16:55,512 epoch 91 - iter 32/160 - loss 0.01536182 - samples/sec: 14.88 - lr: 0.000781\n",
      "2022-05-08 03:17:00,558 epoch 91 - iter 48/160 - loss 0.01669072 - samples/sec: 14.00 - lr: 0.000781\n",
      "2022-05-08 03:17:06,210 epoch 91 - iter 64/160 - loss 0.01890103 - samples/sec: 12.36 - lr: 0.000781\n",
      "2022-05-08 03:17:11,428 epoch 91 - iter 80/160 - loss 0.01802879 - samples/sec: 13.61 - lr: 0.000781\n",
      "2022-05-08 03:17:16,627 epoch 91 - iter 96/160 - loss 0.01845871 - samples/sec: 13.59 - lr: 0.000781\n",
      "2022-05-08 03:17:22,649 epoch 91 - iter 112/160 - loss 0.01775675 - samples/sec: 11.54 - lr: 0.000781\n",
      "2022-05-08 03:17:27,794 epoch 91 - iter 128/160 - loss 0.01767110 - samples/sec: 13.76 - lr: 0.000781\n",
      "2022-05-08 03:17:33,441 epoch 91 - iter 144/160 - loss 0.01776040 - samples/sec: 12.38 - lr: 0.000781\n",
      "2022-05-08 03:17:38,876 epoch 91 - iter 160/160 - loss 0.01763847 - samples/sec: 12.90 - lr: 0.000781\n",
      "2022-05-08 03:17:39,384 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:17:39,385 EPOCH 91 done: loss 0.0176 - lr 0.000781\n",
      "2022-05-08 03:17:39,386 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 03:17:40,696 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:17:45,705 epoch 92 - iter 16/160 - loss 0.01982763 - samples/sec: 12.78 - lr: 0.000781\n",
      "2022-05-08 03:17:51,559 epoch 92 - iter 32/160 - loss 0.01996312 - samples/sec: 11.91 - lr: 0.000781\n",
      "2022-05-08 03:17:56,738 epoch 92 - iter 48/160 - loss 0.02265363 - samples/sec: 13.63 - lr: 0.000781\n",
      "2022-05-08 03:18:01,850 epoch 92 - iter 64/160 - loss 0.02074853 - samples/sec: 13.89 - lr: 0.000781\n",
      "2022-05-08 03:18:07,033 epoch 92 - iter 80/160 - loss 0.02195261 - samples/sec: 13.63 - lr: 0.000781\n",
      "2022-05-08 03:18:12,036 epoch 92 - iter 96/160 - loss 0.02090147 - samples/sec: 14.15 - lr: 0.000781\n",
      "2022-05-08 03:18:17,536 epoch 92 - iter 112/160 - loss 0.02010530 - samples/sec: 12.81 - lr: 0.000781\n",
      "2022-05-08 03:18:23,405 epoch 92 - iter 128/160 - loss 0.01898147 - samples/sec: 11.92 - lr: 0.000781\n",
      "2022-05-08 03:18:28,842 epoch 92 - iter 144/160 - loss 0.01927821 - samples/sec: 12.94 - lr: 0.000781\n",
      "2022-05-08 03:18:33,940 epoch 92 - iter 160/160 - loss 0.01921882 - samples/sec: 13.91 - lr: 0.000781\n",
      "2022-05-08 03:18:34,417 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:18:34,417 EPOCH 92 done: loss 0.0192 - lr 0.000781\n",
      "2022-05-08 03:18:34,418 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 03:18:35,700 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:18:40,702 epoch 93 - iter 16/160 - loss 0.01648403 - samples/sec: 12.80 - lr: 0.000781\n",
      "2022-05-08 03:18:45,859 epoch 93 - iter 32/160 - loss 0.01733677 - samples/sec: 13.72 - lr: 0.000781\n",
      "2022-05-08 03:18:51,177 epoch 93 - iter 48/160 - loss 0.01851769 - samples/sec: 13.22 - lr: 0.000781\n",
      "2022-05-08 03:18:56,646 epoch 93 - iter 64/160 - loss 0.01933874 - samples/sec: 12.83 - lr: 0.000781\n",
      "2022-05-08 03:19:02,054 epoch 93 - iter 80/160 - loss 0.01892798 - samples/sec: 13.07 - lr: 0.000781\n",
      "2022-05-08 03:19:06,964 epoch 93 - iter 96/160 - loss 0.01935022 - samples/sec: 14.40 - lr: 0.000781\n",
      "2022-05-08 03:19:12,274 epoch 93 - iter 112/160 - loss 0.02014560 - samples/sec: 13.22 - lr: 0.000781\n",
      "2022-05-08 03:19:17,707 epoch 93 - iter 128/160 - loss 0.01966100 - samples/sec: 12.92 - lr: 0.000781\n",
      "2022-05-08 03:19:22,382 epoch 93 - iter 144/160 - loss 0.01969631 - samples/sec: 15.26 - lr: 0.000781\n",
      "2022-05-08 03:19:28,017 epoch 93 - iter 160/160 - loss 0.01918629 - samples/sec: 12.41 - lr: 0.000781\n",
      "2022-05-08 03:19:28,511 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:19:28,512 EPOCH 93 done: loss 0.0192 - lr 0.000781\n",
      "2022-05-08 03:19:28,512 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 03:19:29,831 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:19:35,010 epoch 94 - iter 16/160 - loss 0.01711175 - samples/sec: 12.36 - lr: 0.000781\n",
      "2022-05-08 03:19:40,658 epoch 94 - iter 32/160 - loss 0.01620087 - samples/sec: 12.37 - lr: 0.000781\n",
      "2022-05-08 03:19:46,025 epoch 94 - iter 48/160 - loss 0.01858689 - samples/sec: 13.06 - lr: 0.000781\n",
      "2022-05-08 03:19:51,450 epoch 94 - iter 64/160 - loss 0.01700331 - samples/sec: 12.99 - lr: 0.000781\n",
      "2022-05-08 03:19:56,928 epoch 94 - iter 80/160 - loss 0.01713787 - samples/sec: 12.87 - lr: 0.000781\n",
      "2022-05-08 03:20:01,965 epoch 94 - iter 96/160 - loss 0.01939989 - samples/sec: 14.05 - lr: 0.000781\n",
      "2022-05-08 03:20:07,381 epoch 94 - iter 112/160 - loss 0.01824511 - samples/sec: 12.96 - lr: 0.000781\n",
      "2022-05-08 03:20:12,701 epoch 94 - iter 128/160 - loss 0.01837063 - samples/sec: 13.24 - lr: 0.000781\n",
      "2022-05-08 03:20:17,433 epoch 94 - iter 144/160 - loss 0.01883494 - samples/sec: 15.08 - lr: 0.000781\n",
      "2022-05-08 03:20:22,528 epoch 94 - iter 160/160 - loss 0.01865080 - samples/sec: 13.90 - lr: 0.000781\n",
      "2022-05-08 03:20:23,006 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:20:23,006 EPOCH 94 done: loss 0.0187 - lr 0.000781\n",
      "2022-05-08 03:20:23,007 BAD EPOCHS (no improvement): 3\n",
      "2022-05-08 03:20:24,328 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:20:29,121 epoch 95 - iter 16/160 - loss 0.01530978 - samples/sec: 13.36 - lr: 0.000781\n",
      "2022-05-08 03:20:34,327 epoch 95 - iter 32/160 - loss 0.01488493 - samples/sec: 13.58 - lr: 0.000781\n",
      "2022-05-08 03:20:39,324 epoch 95 - iter 48/160 - loss 0.01674909 - samples/sec: 14.21 - lr: 0.000781\n",
      "2022-05-08 03:20:45,354 epoch 95 - iter 64/160 - loss 0.01603773 - samples/sec: 11.53 - lr: 0.000781\n",
      "2022-05-08 03:20:50,923 epoch 95 - iter 80/160 - loss 0.01698525 - samples/sec: 12.61 - lr: 0.000781\n",
      "2022-05-08 03:20:56,011 epoch 95 - iter 96/160 - loss 0.01646128 - samples/sec: 13.89 - lr: 0.000781\n",
      "2022-05-08 03:21:01,526 epoch 95 - iter 112/160 - loss 0.01762943 - samples/sec: 12.71 - lr: 0.000781\n",
      "2022-05-08 03:21:06,841 epoch 95 - iter 128/160 - loss 0.01753124 - samples/sec: 13.31 - lr: 0.000781\n",
      "2022-05-08 03:21:11,909 epoch 95 - iter 144/160 - loss 0.01775600 - samples/sec: 13.97 - lr: 0.000781\n",
      "2022-05-08 03:21:17,219 epoch 95 - iter 160/160 - loss 0.01745067 - samples/sec: 13.31 - lr: 0.000781\n",
      "2022-05-08 03:21:17,695 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:21:17,696 EPOCH 95 done: loss 0.0175 - lr 0.000781\n",
      "2022-05-08 03:21:17,696 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 03:21:19,008 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:21:23,968 epoch 96 - iter 16/160 - loss 0.02008962 - samples/sec: 12.91 - lr: 0.000781\n",
      "2022-05-08 03:21:29,167 epoch 96 - iter 32/160 - loss 0.01782210 - samples/sec: 13.60 - lr: 0.000781\n",
      "2022-05-08 03:21:34,179 epoch 96 - iter 48/160 - loss 0.01837403 - samples/sec: 14.16 - lr: 0.000781\n",
      "2022-05-08 03:21:39,808 epoch 96 - iter 64/160 - loss 0.01789852 - samples/sec: 12.48 - lr: 0.000781\n",
      "2022-05-08 03:21:45,242 epoch 96 - iter 80/160 - loss 0.01767349 - samples/sec: 12.94 - lr: 0.000781\n",
      "2022-05-08 03:21:50,241 epoch 96 - iter 96/160 - loss 0.01725314 - samples/sec: 14.19 - lr: 0.000781\n",
      "2022-05-08 03:21:55,442 epoch 96 - iter 112/160 - loss 0.01741546 - samples/sec: 13.55 - lr: 0.000781\n",
      "2022-05-08 03:22:00,293 epoch 96 - iter 128/160 - loss 0.01772477 - samples/sec: 14.65 - lr: 0.000781\n",
      "2022-05-08 03:22:06,114 epoch 96 - iter 144/160 - loss 0.01783966 - samples/sec: 11.99 - lr: 0.000781\n",
      "2022-05-08 03:22:11,227 epoch 96 - iter 160/160 - loss 0.01783206 - samples/sec: 13.88 - lr: 0.000781\n",
      "2022-05-08 03:22:11,713 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:22:11,713 EPOCH 96 done: loss 0.0178 - lr 0.000781\n",
      "2022-05-08 03:22:11,714 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 03:22:13,020 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:22:18,176 epoch 97 - iter 16/160 - loss 0.02131034 - samples/sec: 12.42 - lr: 0.000781\n",
      "2022-05-08 03:22:23,338 epoch 97 - iter 32/160 - loss 0.01989125 - samples/sec: 13.72 - lr: 0.000781\n",
      "2022-05-08 03:22:28,326 epoch 97 - iter 48/160 - loss 0.02159202 - samples/sec: 14.17 - lr: 0.000781\n",
      "2022-05-08 03:22:33,872 epoch 97 - iter 64/160 - loss 0.02203239 - samples/sec: 12.70 - lr: 0.000781\n",
      "2022-05-08 03:22:39,183 epoch 97 - iter 80/160 - loss 0.02178319 - samples/sec: 13.31 - lr: 0.000781\n",
      "2022-05-08 03:22:44,211 epoch 97 - iter 96/160 - loss 0.02146035 - samples/sec: 14.08 - lr: 0.000781\n",
      "2022-05-08 03:22:49,544 epoch 97 - iter 112/160 - loss 0.02051678 - samples/sec: 13.20 - lr: 0.000781\n",
      "2022-05-08 03:22:54,539 epoch 97 - iter 128/160 - loss 0.01919161 - samples/sec: 14.19 - lr: 0.000781\n",
      "2022-05-08 03:22:59,649 epoch 97 - iter 144/160 - loss 0.01947935 - samples/sec: 13.84 - lr: 0.000781\n",
      "2022-05-08 03:23:05,547 epoch 97 - iter 160/160 - loss 0.01867726 - samples/sec: 11.90 - lr: 0.000781\n",
      "2022-05-08 03:23:06,030 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:23:06,031 EPOCH 97 done: loss 0.0187 - lr 0.000781\n",
      "2022-05-08 03:23:06,031 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 03:23:07,315 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:23:12,337 epoch 98 - iter 16/160 - loss 0.01819444 - samples/sec: 12.75 - lr: 0.000781\n",
      "2022-05-08 03:23:17,914 epoch 98 - iter 32/160 - loss 0.01516011 - samples/sec: 12.57 - lr: 0.000781\n",
      "2022-05-08 03:23:23,044 epoch 98 - iter 48/160 - loss 0.01784606 - samples/sec: 13.77 - lr: 0.000781\n",
      "2022-05-08 03:23:28,071 epoch 98 - iter 64/160 - loss 0.01793350 - samples/sec: 14.09 - lr: 0.000781\n",
      "2022-05-08 03:23:33,516 epoch 98 - iter 80/160 - loss 0.01746817 - samples/sec: 12.93 - lr: 0.000781\n",
      "2022-05-08 03:23:38,631 epoch 98 - iter 96/160 - loss 0.01827737 - samples/sec: 13.83 - lr: 0.000781\n",
      "2022-05-08 03:23:44,281 epoch 98 - iter 112/160 - loss 0.01809291 - samples/sec: 12.38 - lr: 0.000781\n",
      "2022-05-08 03:23:49,271 epoch 98 - iter 128/160 - loss 0.01804257 - samples/sec: 14.20 - lr: 0.000781\n",
      "2022-05-08 03:23:54,517 epoch 98 - iter 144/160 - loss 0.01842316 - samples/sec: 13.47 - lr: 0.000781\n",
      "2022-05-08 03:23:59,973 epoch 98 - iter 160/160 - loss 0.01851614 - samples/sec: 12.90 - lr: 0.000781\n",
      "2022-05-08 03:24:00,456 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:24:00,457 EPOCH 98 done: loss 0.0185 - lr 0.000781\n",
      "2022-05-08 03:24:00,457 BAD EPOCHS (no improvement): 3\n",
      "2022-05-08 03:24:01,772 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:24:07,031 epoch 99 - iter 16/160 - loss 0.01276745 - samples/sec: 12.18 - lr: 0.000781\n",
      "2022-05-08 03:24:12,451 epoch 99 - iter 32/160 - loss 0.01464620 - samples/sec: 13.03 - lr: 0.000781\n",
      "2022-05-08 03:24:17,785 epoch 99 - iter 48/160 - loss 0.01669323 - samples/sec: 13.28 - lr: 0.000781\n",
      "2022-05-08 03:24:23,051 epoch 99 - iter 64/160 - loss 0.01886134 - samples/sec: 13.37 - lr: 0.000781\n",
      "2022-05-08 03:24:28,403 epoch 99 - iter 80/160 - loss 0.01833586 - samples/sec: 13.18 - lr: 0.000781\n",
      "2022-05-08 03:24:33,400 epoch 99 - iter 96/160 - loss 0.01787728 - samples/sec: 14.22 - lr: 0.000781\n",
      "2022-05-08 03:24:38,948 epoch 99 - iter 112/160 - loss 0.01756486 - samples/sec: 12.62 - lr: 0.000781\n",
      "2022-05-08 03:24:43,903 epoch 99 - iter 128/160 - loss 0.01772121 - samples/sec: 14.29 - lr: 0.000781\n",
      "2022-05-08 03:24:49,010 epoch 99 - iter 144/160 - loss 0.01832673 - samples/sec: 13.82 - lr: 0.000781\n",
      "2022-05-08 03:24:54,440 epoch 99 - iter 160/160 - loss 0.01876166 - samples/sec: 12.95 - lr: 0.000781\n",
      "2022-05-08 03:24:54,924 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:24:54,924 EPOCH 99 done: loss 0.0188 - lr 0.000781\n",
      "2022-05-08 03:24:54,925 Epoch    99: reducing learning rate of group 0 to 3.9063e-04.\n",
      "2022-05-08 03:24:54,926 BAD EPOCHS (no improvement): 4\n",
      "2022-05-08 03:24:56,223 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:25:00,750 epoch 100 - iter 16/160 - loss 0.02473183 - samples/sec: 14.15 - lr: 0.000391\n",
      "2022-05-08 03:25:06,200 epoch 100 - iter 32/160 - loss 0.02203243 - samples/sec: 12.87 - lr: 0.000391\n",
      "2022-05-08 03:25:11,719 epoch 100 - iter 48/160 - loss 0.02032183 - samples/sec: 12.74 - lr: 0.000391\n",
      "2022-05-08 03:25:17,733 epoch 100 - iter 64/160 - loss 0.01863939 - samples/sec: 11.57 - lr: 0.000391\n",
      "2022-05-08 03:25:22,971 epoch 100 - iter 80/160 - loss 0.01985370 - samples/sec: 13.50 - lr: 0.000391\n",
      "2022-05-08 03:25:27,706 epoch 100 - iter 96/160 - loss 0.01948823 - samples/sec: 15.04 - lr: 0.000391\n",
      "2022-05-08 03:25:33,255 epoch 100 - iter 112/160 - loss 0.01867942 - samples/sec: 12.63 - lr: 0.000391\n",
      "2022-05-08 03:25:38,344 epoch 100 - iter 128/160 - loss 0.01850619 - samples/sec: 13.91 - lr: 0.000391\n",
      "2022-05-08 03:25:43,702 epoch 100 - iter 144/160 - loss 0.01846924 - samples/sec: 13.19 - lr: 0.000391\n",
      "2022-05-08 03:25:48,713 epoch 100 - iter 160/160 - loss 0.01931619 - samples/sec: 14.15 - lr: 0.000391\n",
      "2022-05-08 03:25:49,205 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:25:49,205 EPOCH 100 done: loss 0.0193 - lr 0.000391\n",
      "2022-05-08 03:25:49,206 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 03:25:50,483 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:25:55,730 epoch 101 - iter 16/160 - loss 0.01818028 - samples/sec: 12.20 - lr: 0.000391\n",
      "2022-05-08 03:26:00,818 epoch 101 - iter 32/160 - loss 0.01792971 - samples/sec: 13.92 - lr: 0.000391\n",
      "2022-05-08 03:26:06,168 epoch 101 - iter 48/160 - loss 0.01951968 - samples/sec: 13.17 - lr: 0.000391\n",
      "2022-05-08 03:26:11,553 epoch 101 - iter 64/160 - loss 0.01936694 - samples/sec: 13.06 - lr: 0.000391\n",
      "2022-05-08 03:26:16,918 epoch 101 - iter 80/160 - loss 0.01890505 - samples/sec: 13.16 - lr: 0.000391\n",
      "2022-05-08 03:26:22,307 epoch 101 - iter 96/160 - loss 0.01825446 - samples/sec: 13.03 - lr: 0.000391\n",
      "2022-05-08 03:26:27,750 epoch 101 - iter 112/160 - loss 0.01884427 - samples/sec: 12.87 - lr: 0.000391\n",
      "2022-05-08 03:26:32,964 epoch 101 - iter 128/160 - loss 0.01925229 - samples/sec: 13.51 - lr: 0.000391\n",
      "2022-05-08 03:26:38,474 epoch 101 - iter 144/160 - loss 0.01963787 - samples/sec: 12.77 - lr: 0.000391\n",
      "2022-05-08 03:26:43,572 epoch 101 - iter 160/160 - loss 0.01935415 - samples/sec: 13.85 - lr: 0.000391\n",
      "2022-05-08 03:26:44,055 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:26:44,055 EPOCH 101 done: loss 0.0194 - lr 0.000391\n",
      "2022-05-08 03:26:44,056 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 03:26:45,341 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:26:50,090 epoch 102 - iter 16/160 - loss 0.01368258 - samples/sec: 13.48 - lr: 0.000391\n",
      "2022-05-08 03:26:55,141 epoch 102 - iter 32/160 - loss 0.01273201 - samples/sec: 14.01 - lr: 0.000391\n",
      "2022-05-08 03:27:00,259 epoch 102 - iter 48/160 - loss 0.01501629 - samples/sec: 13.82 - lr: 0.000391\n",
      "2022-05-08 03:27:05,656 epoch 102 - iter 64/160 - loss 0.01606467 - samples/sec: 13.12 - lr: 0.000391\n",
      "2022-05-08 03:27:10,690 epoch 102 - iter 80/160 - loss 0.01612125 - samples/sec: 14.05 - lr: 0.000391\n",
      "2022-05-08 03:27:15,382 epoch 102 - iter 96/160 - loss 0.01693986 - samples/sec: 15.25 - lr: 0.000391\n",
      "2022-05-08 03:27:20,961 epoch 102 - iter 112/160 - loss 0.01741749 - samples/sec: 12.56 - lr: 0.000391\n",
      "2022-05-08 03:27:26,447 epoch 102 - iter 128/160 - loss 0.01801016 - samples/sec: 12.77 - lr: 0.000391\n",
      "2022-05-08 03:27:32,188 epoch 102 - iter 144/160 - loss 0.01889557 - samples/sec: 12.14 - lr: 0.000391\n",
      "2022-05-08 03:27:37,435 epoch 102 - iter 160/160 - loss 0.01909566 - samples/sec: 13.40 - lr: 0.000391\n",
      "2022-05-08 03:27:37,917 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:27:37,917 EPOCH 102 done: loss 0.0191 - lr 0.000391\n",
      "2022-05-08 03:27:37,918 BAD EPOCHS (no improvement): 3\n",
      "2022-05-08 03:27:39,412 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:27:44,359 epoch 103 - iter 16/160 - loss 0.02269480 - samples/sec: 12.94 - lr: 0.000391\n",
      "2022-05-08 03:27:50,482 epoch 103 - iter 32/160 - loss 0.02102123 - samples/sec: 11.38 - lr: 0.000391\n",
      "2022-05-08 03:27:55,479 epoch 103 - iter 48/160 - loss 0.01966875 - samples/sec: 14.18 - lr: 0.000391\n",
      "2022-05-08 03:28:00,604 epoch 103 - iter 64/160 - loss 0.01878535 - samples/sec: 13.84 - lr: 0.000391\n",
      "2022-05-08 03:28:06,031 epoch 103 - iter 80/160 - loss 0.01979204 - samples/sec: 12.95 - lr: 0.000391\n",
      "2022-05-08 03:28:10,938 epoch 103 - iter 96/160 - loss 0.01982601 - samples/sec: 14.47 - lr: 0.000391\n",
      "2022-05-08 03:28:16,811 epoch 103 - iter 112/160 - loss 0.01890837 - samples/sec: 11.91 - lr: 0.000391\n",
      "2022-05-08 03:28:22,139 epoch 103 - iter 128/160 - loss 0.01978398 - samples/sec: 13.21 - lr: 0.000391\n",
      "2022-05-08 03:28:27,223 epoch 103 - iter 144/160 - loss 0.01937871 - samples/sec: 13.93 - lr: 0.000391\n",
      "2022-05-08 03:28:32,193 epoch 103 - iter 160/160 - loss 0.01908415 - samples/sec: 14.32 - lr: 0.000391\n",
      "2022-05-08 03:28:32,682 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:28:32,683 EPOCH 103 done: loss 0.0191 - lr 0.000391\n",
      "2022-05-08 03:28:32,683 Epoch   103: reducing learning rate of group 0 to 1.9531e-04.\n",
      "2022-05-08 03:28:32,684 BAD EPOCHS (no improvement): 4\n",
      "2022-05-08 03:28:33,991 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:28:38,597 epoch 104 - iter 16/160 - loss 0.02180451 - samples/sec: 13.90 - lr: 0.000195\n",
      "2022-05-08 03:28:43,728 epoch 104 - iter 32/160 - loss 0.02021130 - samples/sec: 13.83 - lr: 0.000195\n",
      "2022-05-08 03:28:49,362 epoch 104 - iter 48/160 - loss 0.01843496 - samples/sec: 12.41 - lr: 0.000195\n",
      "2022-05-08 03:28:55,155 epoch 104 - iter 64/160 - loss 0.02001269 - samples/sec: 12.07 - lr: 0.000195\n",
      "2022-05-08 03:29:00,525 epoch 104 - iter 80/160 - loss 0.01959265 - samples/sec: 13.08 - lr: 0.000195\n",
      "2022-05-08 03:29:05,694 epoch 104 - iter 96/160 - loss 0.01878494 - samples/sec: 13.67 - lr: 0.000195\n",
      "2022-05-08 03:29:10,929 epoch 104 - iter 112/160 - loss 0.01907450 - samples/sec: 13.46 - lr: 0.000195\n",
      "2022-05-08 03:29:16,117 epoch 104 - iter 128/160 - loss 0.01942410 - samples/sec: 13.60 - lr: 0.000195\n",
      "2022-05-08 03:29:21,547 epoch 104 - iter 144/160 - loss 0.01939100 - samples/sec: 12.99 - lr: 0.000195\n",
      "2022-05-08 03:29:26,817 epoch 104 - iter 160/160 - loss 0.01892500 - samples/sec: 13.47 - lr: 0.000195\n",
      "2022-05-08 03:29:27,301 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:29:27,301 EPOCH 104 done: loss 0.0189 - lr 0.000195\n",
      "2022-05-08 03:29:27,302 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 03:29:28,584 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:29:33,339 epoch 105 - iter 16/160 - loss 0.01996351 - samples/sec: 13.47 - lr: 0.000195\n",
      "2022-05-08 03:29:38,641 epoch 105 - iter 32/160 - loss 0.02254309 - samples/sec: 13.31 - lr: 0.000195\n",
      "2022-05-08 03:29:43,711 epoch 105 - iter 48/160 - loss 0.02029241 - samples/sec: 13.98 - lr: 0.000195\n",
      "2022-05-08 03:29:49,370 epoch 105 - iter 64/160 - loss 0.01970033 - samples/sec: 12.38 - lr: 0.000195\n",
      "2022-05-08 03:29:55,069 epoch 105 - iter 80/160 - loss 0.02016938 - samples/sec: 12.33 - lr: 0.000195\n",
      "2022-05-08 03:30:00,254 epoch 105 - iter 96/160 - loss 0.02031569 - samples/sec: 13.59 - lr: 0.000195\n",
      "2022-05-08 03:30:05,719 epoch 105 - iter 112/160 - loss 0.02002150 - samples/sec: 12.84 - lr: 0.000195\n",
      "2022-05-08 03:30:11,299 epoch 105 - iter 128/160 - loss 0.01939437 - samples/sec: 12.59 - lr: 0.000195\n",
      "2022-05-08 03:30:16,572 epoch 105 - iter 144/160 - loss 0.01899385 - samples/sec: 13.44 - lr: 0.000195\n",
      "2022-05-08 03:30:21,461 epoch 105 - iter 160/160 - loss 0.01848571 - samples/sec: 14.58 - lr: 0.000195\n",
      "2022-05-08 03:30:21,961 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:30:21,962 EPOCH 105 done: loss 0.0185 - lr 0.000195\n",
      "2022-05-08 03:30:21,962 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 03:30:23,306 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:30:28,544 epoch 106 - iter 16/160 - loss 0.02164925 - samples/sec: 12.23 - lr: 0.000195\n",
      "2022-05-08 03:30:33,775 epoch 106 - iter 32/160 - loss 0.02084221 - samples/sec: 13.50 - lr: 0.000195\n",
      "2022-05-08 03:30:39,008 epoch 106 - iter 48/160 - loss 0.02208482 - samples/sec: 13.50 - lr: 0.000195\n",
      "2022-05-08 03:30:44,336 epoch 106 - iter 64/160 - loss 0.02216971 - samples/sec: 13.22 - lr: 0.000195\n",
      "2022-05-08 03:30:49,893 epoch 106 - iter 80/160 - loss 0.02121378 - samples/sec: 12.64 - lr: 0.000195\n",
      "2022-05-08 03:30:55,047 epoch 106 - iter 96/160 - loss 0.02011100 - samples/sec: 13.69 - lr: 0.000195\n",
      "2022-05-08 03:31:00,343 epoch 106 - iter 112/160 - loss 0.02103975 - samples/sec: 13.30 - lr: 0.000195\n",
      "2022-05-08 03:31:06,026 epoch 106 - iter 128/160 - loss 0.02029816 - samples/sec: 12.31 - lr: 0.000195\n",
      "2022-05-08 03:31:11,212 epoch 106 - iter 144/160 - loss 0.02016365 - samples/sec: 13.60 - lr: 0.000195\n",
      "2022-05-08 03:31:15,925 epoch 106 - iter 160/160 - loss 0.01951820 - samples/sec: 15.21 - lr: 0.000195\n",
      "2022-05-08 03:31:16,413 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:31:16,413 EPOCH 106 done: loss 0.0195 - lr 0.000195\n",
      "2022-05-08 03:31:16,414 BAD EPOCHS (no improvement): 3\n",
      "2022-05-08 03:31:17,717 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:31:22,915 epoch 107 - iter 16/160 - loss 0.01808744 - samples/sec: 12.32 - lr: 0.000195\n",
      "2022-05-08 03:31:28,066 epoch 107 - iter 32/160 - loss 0.02018438 - samples/sec: 13.75 - lr: 0.000195\n",
      "2022-05-08 03:31:33,570 epoch 107 - iter 48/160 - loss 0.01884148 - samples/sec: 12.77 - lr: 0.000195\n",
      "2022-05-08 03:31:38,636 epoch 107 - iter 64/160 - loss 0.01985186 - samples/sec: 14.05 - lr: 0.000195\n",
      "2022-05-08 03:31:43,574 epoch 107 - iter 80/160 - loss 0.01943246 - samples/sec: 14.43 - lr: 0.000195\n",
      "2022-05-08 03:31:49,183 epoch 107 - iter 96/160 - loss 0.01941065 - samples/sec: 12.51 - lr: 0.000195\n",
      "2022-05-08 03:31:54,602 epoch 107 - iter 112/160 - loss 0.01965235 - samples/sec: 12.96 - lr: 0.000195\n",
      "2022-05-08 03:31:59,895 epoch 107 - iter 128/160 - loss 0.01968337 - samples/sec: 13.35 - lr: 0.000195\n",
      "2022-05-08 03:32:04,912 epoch 107 - iter 144/160 - loss 0.01889150 - samples/sec: 14.14 - lr: 0.000195\n",
      "2022-05-08 03:32:10,261 epoch 107 - iter 160/160 - loss 0.01864265 - samples/sec: 13.15 - lr: 0.000195\n",
      "2022-05-08 03:32:10,741 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:32:10,742 EPOCH 107 done: loss 0.0186 - lr 0.000195\n",
      "2022-05-08 03:32:10,742 Epoch   107: reducing learning rate of group 0 to 9.7656e-05.\n",
      "2022-05-08 03:32:10,743 BAD EPOCHS (no improvement): 4\n",
      "2022-05-08 03:32:12,064 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:32:12,065 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:32:12,065 learning rate too small - quitting training!\n",
      "2022-05-08 03:32:12,066 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:32:13,357 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:32:13,358 Testing using last state of model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 52/52 [00:12<00:00,  4.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-08 03:32:25,369 Evaluating as a multi-label problem: False\n",
      "2022-05-08 03:32:25,377 0.8918\t0.8228\t0.8559\t0.7481\n",
      "2022-05-08 03:32:25,378 \n",
      "Results:\n",
      "- F-score (micro) 0.8559\n",
      "- F-score (macro) 0.6024\n",
      "- Accuracy 0.7481\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        DRUG     0.9074    0.9095    0.9085       431\n",
      "         DIS     0.5714    0.2000    0.2963        60\n",
      "\n",
      "   micro avg     0.8918    0.8228    0.8559       491\n",
      "   macro avg     0.7394    0.5548    0.6024       491\n",
      "weighted avg     0.8664    0.8228    0.8337       491\n",
      "\n",
      "2022-05-08 03:32:25,378 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:32:25,381 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:32:25,381 WARNING: No LOSS found for test split in this data.\n",
      "2022-05-08 03:32:25,381 Are you sure you want to plot LOSS and not another value?\n",
      "2022-05-08 03:32:25,383 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 03:32:25,397 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-08 03:32:25,397 WARNING: No F1 found for test split in this data.\n",
      "2022-05-08 03:32:25,398 Are you sure you want to plot F1 and not another value?\n",
      "2022-05-08 03:32:25,399 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-08 03:32:25,982 Loss and F1 plots are saved in ..\\resources\\taggers\\FA_CADEC_glove_roberta\\training.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = 'CADEC'\n",
    "selected_embeddings = defaultdict(lambda: 0, {'glove':1, 'roberta':1})\n",
    "train(model, selected_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddd103c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-08 15:17:12,879 Reading data from ..\\data\\AMT\\labels\n",
      "2022-05-08 15:17:12,902 Train: ..\\data\\AMT\\labels\\NER_Reddit_AMT_labels_href_train.csv\n",
      "2022-05-08 15:17:12,918 Dev: ..\\data\\AMT\\labels\\NER_Reddit_AMT_labels_href_dev.csv\n",
      "2022-05-08 15:17:12,918 Test: ..\\data\\AMT\\labels\\NER_Reddit_AMT_labels_href_test.csv\n",
      "2022-05-08 15:17:31,922 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "968it [00:00, 48394.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-08 15:17:31,987 Dictionary created for label 'ner' with 3 values: DIS (seen 1428 times), DRUG (seen 403 times)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-08 15:18:06,983 SequenceTagger predicts: Dictionary with 9 tags: O, S-DIS, B-DIS, E-DIS, I-DIS, S-DRUG, B-DRUG, E-DRUG, I-DRUG\n",
      "2022-05-08 15:18:07,783 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:18:07,784 Model: \"SequenceTagger(\n",
      "  (embeddings): StackedEmbeddings(\n",
      "    (list_embedding_0): PooledFlairEmbeddings(\n",
      "      (context_embeddings): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (list_embedding_1): PooledFlairEmbeddings(\n",
      "      (context_embeddings): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=8192, out_features=8192, bias=True)\n",
      "  (rnn): LSTM(8192, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=11, bias=True)\n",
      "  (loss_function): ViterbiLoss()\n",
      "  (crf): CRF()\n",
      ")\"\n",
      "2022-05-08 15:18:07,799 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:18:07,800 Corpus: \"Corpus: 968 train + 523 dev + 537 test sentences\"\n",
      "2022-05-08 15:18:07,800 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:18:07,809 Parameters:\n",
      "2022-05-08 15:18:07,810  - learning_rate: \"0.100000\"\n",
      "2022-05-08 15:18:07,811  - mini_batch_size: \"4\"\n",
      "2022-05-08 15:18:07,811  - patience: \"3\"\n",
      "2022-05-08 15:18:07,812  - anneal_factor: \"0.5\"\n",
      "2022-05-08 15:18:07,813  - max_epochs: \"200\"\n",
      "2022-05-08 15:18:07,814  - shuffle: \"True\"\n",
      "2022-05-08 15:18:07,814  - train_with_dev: \"True\"\n",
      "2022-05-08 15:18:07,815  - batch_growth_annealing: \"False\"\n",
      "2022-05-08 15:18:07,816 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:18:07,816 Model training base path: \"..\\resources\\taggers\\FA_MedRed_pooled-flair\"\n",
      "2022-05-08 15:18:07,817 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:18:07,818 Device: cuda:0\n",
      "2022-05-08 15:18:07,818 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:18:07,818 Embeddings storage mode: cpu\n",
      "2022-05-08 15:18:07,820 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:18:07,821 train mode resetting embeddings\n",
      "2022-05-08 15:18:07,821 train mode resetting embeddings\n",
      "2022-05-08 15:18:53,048 epoch 1 - iter 37/373 - loss 0.46149877 - samples/sec: 3.27 - lr: 0.100000\n",
      "2022-05-08 15:19:08,039 epoch 1 - iter 74/373 - loss 0.33196593 - samples/sec: 9.94 - lr: 0.100000\n",
      "2022-05-08 15:19:24,365 epoch 1 - iter 111/373 - loss 0.28922233 - samples/sec: 9.10 - lr: 0.100000\n",
      "2022-05-08 15:19:41,433 epoch 1 - iter 148/373 - loss 0.26525424 - samples/sec: 8.71 - lr: 0.100000\n",
      "2022-05-08 15:19:56,483 epoch 1 - iter 185/373 - loss 0.24563332 - samples/sec: 9.88 - lr: 0.100000\n",
      "2022-05-08 15:20:10,835 epoch 1 - iter 222/373 - loss 0.22978405 - samples/sec: 10.36 - lr: 0.100000\n",
      "2022-05-08 15:20:24,917 epoch 1 - iter 259/373 - loss 0.21626880 - samples/sec: 10.56 - lr: 0.100000\n",
      "2022-05-08 15:20:40,325 epoch 1 - iter 296/373 - loss 0.20372730 - samples/sec: 9.65 - lr: 0.100000\n",
      "2022-05-08 15:20:53,423 epoch 1 - iter 333/373 - loss 0.19598886 - samples/sec: 11.36 - lr: 0.100000\n",
      "2022-05-08 15:21:10,449 epoch 1 - iter 370/373 - loss 0.18856142 - samples/sec: 8.73 - lr: 0.100000\n",
      "2022-05-08 15:21:11,131 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:21:11,131 EPOCH 1 done: loss 0.1881 - lr 0.100000\n",
      "2022-05-08 15:21:11,133 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 15:21:13,624 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:21:13,625 train mode resetting embeddings\n",
      "2022-05-08 15:21:13,634 train mode resetting embeddings\n",
      "2022-05-08 15:21:21,275 epoch 2 - iter 37/373 - loss 0.12818617 - samples/sec: 19.40 - lr: 0.100000\n",
      "2022-05-08 15:21:29,833 epoch 2 - iter 74/373 - loss 0.12364737 - samples/sec: 17.44 - lr: 0.100000\n",
      "2022-05-08 15:21:36,885 epoch 2 - iter 111/373 - loss 0.11611826 - samples/sec: 21.20 - lr: 0.100000\n",
      "2022-05-08 15:21:44,041 epoch 2 - iter 148/373 - loss 0.11317316 - samples/sec: 20.87 - lr: 0.100000\n",
      "2022-05-08 15:21:51,725 epoch 2 - iter 185/373 - loss 0.11230338 - samples/sec: 19.45 - lr: 0.100000\n",
      "2022-05-08 15:21:58,046 epoch 2 - iter 222/373 - loss 0.11042063 - samples/sec: 23.71 - lr: 0.100000\n",
      "2022-05-08 15:22:05,724 epoch 2 - iter 259/373 - loss 0.10862167 - samples/sec: 19.47 - lr: 0.100000\n",
      "2022-05-08 15:22:13,205 epoch 2 - iter 296/373 - loss 0.10666077 - samples/sec: 19.96 - lr: 0.100000\n",
      "2022-05-08 15:22:19,981 epoch 2 - iter 333/373 - loss 0.10655371 - samples/sec: 22.07 - lr: 0.100000\n",
      "2022-05-08 15:22:26,818 epoch 2 - iter 370/373 - loss 0.10690267 - samples/sec: 21.86 - lr: 0.100000\n",
      "2022-05-08 15:22:27,535 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:22:27,536 EPOCH 2 done: loss 0.1066 - lr 0.100000\n",
      "2022-05-08 15:22:27,536 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 15:22:29,742 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:22:29,743 train mode resetting embeddings\n",
      "2022-05-08 15:22:29,749 train mode resetting embeddings\n",
      "2022-05-08 15:22:37,745 epoch 3 - iter 37/373 - loss 0.09530040 - samples/sec: 18.52 - lr: 0.100000\n",
      "2022-05-08 15:22:45,230 epoch 3 - iter 74/373 - loss 0.09384144 - samples/sec: 19.98 - lr: 0.100000\n",
      "2022-05-08 15:22:52,164 epoch 3 - iter 111/373 - loss 0.09663383 - samples/sec: 21.56 - lr: 0.100000\n",
      "2022-05-08 15:22:59,774 epoch 3 - iter 148/373 - loss 0.09183501 - samples/sec: 19.63 - lr: 0.100000\n",
      "2022-05-08 15:23:06,533 epoch 3 - iter 185/373 - loss 0.09381918 - samples/sec: 22.13 - lr: 0.100000\n",
      "2022-05-08 15:23:13,288 epoch 3 - iter 222/373 - loss 0.09324635 - samples/sec: 22.15 - lr: 0.100000\n",
      "2022-05-08 15:23:20,602 epoch 3 - iter 259/373 - loss 0.09242260 - samples/sec: 20.44 - lr: 0.100000\n",
      "2022-05-08 15:23:28,317 epoch 3 - iter 296/373 - loss 0.09383383 - samples/sec: 19.38 - lr: 0.100000\n",
      "2022-05-08 15:23:35,121 epoch 3 - iter 333/373 - loss 0.09217902 - samples/sec: 21.96 - lr: 0.100000\n",
      "2022-05-08 15:23:43,078 epoch 3 - iter 370/373 - loss 0.09016540 - samples/sec: 18.78 - lr: 0.100000\n",
      "2022-05-08 15:23:43,624 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:23:43,625 EPOCH 3 done: loss 0.0899 - lr 0.100000\n",
      "2022-05-08 15:23:43,626 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 15:23:45,983 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:23:46,482 train mode resetting embeddings\n",
      "2022-05-08 15:23:46,488 train mode resetting embeddings\n",
      "2022-05-08 15:23:53,894 epoch 4 - iter 37/373 - loss 0.07982734 - samples/sec: 20.01 - lr: 0.100000\n",
      "2022-05-08 15:24:02,246 epoch 4 - iter 74/373 - loss 0.07549235 - samples/sec: 17.87 - lr: 0.100000\n",
      "2022-05-08 15:24:09,614 epoch 4 - iter 111/373 - loss 0.07812885 - samples/sec: 20.30 - lr: 0.100000\n",
      "2022-05-08 15:24:17,041 epoch 4 - iter 148/373 - loss 0.07665355 - samples/sec: 20.12 - lr: 0.100000\n",
      "2022-05-08 15:24:23,508 epoch 4 - iter 185/373 - loss 0.07770252 - samples/sec: 23.14 - lr: 0.100000\n",
      "2022-05-08 15:24:31,204 epoch 4 - iter 222/373 - loss 0.07835922 - samples/sec: 19.41 - lr: 0.100000\n",
      "2022-05-08 15:24:38,675 epoch 4 - iter 259/373 - loss 0.07915849 - samples/sec: 19.99 - lr: 0.100000\n",
      "2022-05-08 15:24:45,993 epoch 4 - iter 296/373 - loss 0.07798342 - samples/sec: 20.42 - lr: 0.100000\n",
      "2022-05-08 15:24:53,192 epoch 4 - iter 333/373 - loss 0.07990296 - samples/sec: 20.77 - lr: 0.100000\n",
      "2022-05-08 15:24:59,188 epoch 4 - iter 370/373 - loss 0.07997700 - samples/sec: 24.96 - lr: 0.100000\n",
      "2022-05-08 15:24:59,556 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:24:59,557 EPOCH 4 done: loss 0.0799 - lr 0.100000\n",
      "2022-05-08 15:24:59,558 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 15:25:01,760 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:25:01,761 train mode resetting embeddings\n",
      "2022-05-08 15:25:01,767 train mode resetting embeddings\n",
      "2022-05-08 15:25:09,124 epoch 5 - iter 37/373 - loss 0.07500627 - samples/sec: 20.14 - lr: 0.100000\n",
      "2022-05-08 15:25:15,979 epoch 5 - iter 74/373 - loss 0.08297512 - samples/sec: 21.83 - lr: 0.100000\n",
      "2022-05-08 15:25:22,366 epoch 5 - iter 111/373 - loss 0.08108043 - samples/sec: 23.45 - lr: 0.100000\n",
      "2022-05-08 15:25:29,466 epoch 5 - iter 148/373 - loss 0.08135100 - samples/sec: 21.07 - lr: 0.100000\n",
      "2022-05-08 15:25:37,084 epoch 5 - iter 185/373 - loss 0.07948071 - samples/sec: 19.62 - lr: 0.100000\n",
      "2022-05-08 15:25:44,377 epoch 5 - iter 222/373 - loss 0.07749768 - samples/sec: 20.49 - lr: 0.100000\n",
      "2022-05-08 15:25:51,410 epoch 5 - iter 259/373 - loss 0.07703777 - samples/sec: 21.28 - lr: 0.100000\n",
      "2022-05-08 15:25:58,981 epoch 5 - iter 296/373 - loss 0.07705678 - samples/sec: 19.74 - lr: 0.100000\n",
      "2022-05-08 15:26:07,306 epoch 5 - iter 333/373 - loss 0.07679908 - samples/sec: 17.92 - lr: 0.100000\n",
      "2022-05-08 15:26:14,921 epoch 5 - iter 370/373 - loss 0.07758495 - samples/sec: 19.64 - lr: 0.100000\n",
      "2022-05-08 15:26:15,723 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:26:15,724 EPOCH 5 done: loss 0.0780 - lr 0.100000\n",
      "2022-05-08 15:26:15,725 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 15:26:18,011 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:26:18,012 train mode resetting embeddings\n",
      "2022-05-08 15:26:18,017 train mode resetting embeddings\n",
      "2022-05-08 15:26:24,124 epoch 6 - iter 37/373 - loss 0.07055443 - samples/sec: 24.26 - lr: 0.100000\n",
      "2022-05-08 15:26:31,997 epoch 6 - iter 74/373 - loss 0.06218545 - samples/sec: 18.96 - lr: 0.100000\n",
      "2022-05-08 15:26:39,256 epoch 6 - iter 111/373 - loss 0.06182488 - samples/sec: 20.59 - lr: 0.100000\n",
      "2022-05-08 15:26:47,155 epoch 6 - iter 148/373 - loss 0.06443212 - samples/sec: 18.92 - lr: 0.100000\n",
      "2022-05-08 15:26:54,796 epoch 6 - iter 185/373 - loss 0.06724776 - samples/sec: 19.55 - lr: 0.100000\n",
      "2022-05-08 15:27:02,008 epoch 6 - iter 222/373 - loss 0.06666813 - samples/sec: 20.73 - lr: 0.100000\n",
      "2022-05-08 15:27:09,414 epoch 6 - iter 259/373 - loss 0.06711099 - samples/sec: 20.19 - lr: 0.100000\n",
      "2022-05-08 15:27:16,943 epoch 6 - iter 296/373 - loss 0.06921869 - samples/sec: 19.84 - lr: 0.100000\n",
      "2022-05-08 15:27:24,477 epoch 6 - iter 333/373 - loss 0.06912341 - samples/sec: 19.84 - lr: 0.100000\n",
      "2022-05-08 15:27:31,611 epoch 6 - iter 370/373 - loss 0.06944214 - samples/sec: 20.95 - lr: 0.100000\n",
      "2022-05-08 15:27:32,028 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:27:32,029 EPOCH 6 done: loss 0.0694 - lr 0.100000\n",
      "2022-05-08 15:27:32,029 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 15:27:34,296 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:27:34,297 train mode resetting embeddings\n",
      "2022-05-08 15:27:34,303 train mode resetting embeddings\n",
      "2022-05-08 15:27:42,787 epoch 7 - iter 37/373 - loss 0.05969634 - samples/sec: 17.46 - lr: 0.100000\n",
      "2022-05-08 15:27:50,755 epoch 7 - iter 74/373 - loss 0.06142234 - samples/sec: 18.77 - lr: 0.100000\n",
      "2022-05-08 15:27:58,252 epoch 7 - iter 111/373 - loss 0.06092116 - samples/sec: 19.94 - lr: 0.100000\n",
      "2022-05-08 15:28:05,919 epoch 7 - iter 148/373 - loss 0.06441444 - samples/sec: 19.49 - lr: 0.100000\n",
      "2022-05-08 15:28:12,745 epoch 7 - iter 185/373 - loss 0.06601466 - samples/sec: 21.90 - lr: 0.100000\n",
      "2022-05-08 15:28:20,552 epoch 7 - iter 222/373 - loss 0.06706006 - samples/sec: 19.13 - lr: 0.100000\n",
      "2022-05-08 15:28:28,031 epoch 7 - iter 259/373 - loss 0.06704570 - samples/sec: 19.98 - lr: 0.100000\n",
      "2022-05-08 15:28:34,975 epoch 7 - iter 296/373 - loss 0.06648396 - samples/sec: 21.54 - lr: 0.100000\n",
      "2022-05-08 15:28:42,205 epoch 7 - iter 333/373 - loss 0.06659693 - samples/sec: 20.68 - lr: 0.100000\n",
      "2022-05-08 15:28:48,325 epoch 7 - iter 370/373 - loss 0.06637943 - samples/sec: 24.46 - lr: 0.100000\n",
      "2022-05-08 15:28:48,947 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:28:48,948 EPOCH 7 done: loss 0.0666 - lr 0.100000\n",
      "2022-05-08 15:28:48,949 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 15:28:51,165 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:28:51,165 train mode resetting embeddings\n",
      "2022-05-08 15:28:51,172 train mode resetting embeddings\n",
      "2022-05-08 15:28:58,284 epoch 8 - iter 37/373 - loss 0.06284861 - samples/sec: 20.83 - lr: 0.100000\n",
      "2022-05-08 15:29:05,497 epoch 8 - iter 74/373 - loss 0.06843765 - samples/sec: 20.72 - lr: 0.100000\n",
      "2022-05-08 15:29:12,499 epoch 8 - iter 111/373 - loss 0.06524122 - samples/sec: 21.38 - lr: 0.100000\n",
      "2022-05-08 15:29:20,480 epoch 8 - iter 148/373 - loss 0.06572726 - samples/sec: 18.71 - lr: 0.100000\n",
      "2022-05-08 15:29:27,551 epoch 8 - iter 185/373 - loss 0.06634916 - samples/sec: 21.15 - lr: 0.100000\n",
      "2022-05-08 15:29:33,722 epoch 8 - iter 222/373 - loss 0.06502669 - samples/sec: 24.26 - lr: 0.100000\n",
      "2022-05-08 15:29:41,041 epoch 8 - iter 259/373 - loss 0.06396002 - samples/sec: 20.44 - lr: 0.100000\n",
      "2022-05-08 15:29:48,902 epoch 8 - iter 296/373 - loss 0.06469936 - samples/sec: 19.02 - lr: 0.100000\n",
      "2022-05-08 15:29:56,554 epoch 8 - iter 333/373 - loss 0.06402038 - samples/sec: 19.54 - lr: 0.100000\n",
      "2022-05-08 15:30:03,766 epoch 8 - iter 370/373 - loss 0.06437409 - samples/sec: 20.78 - lr: 0.100000\n",
      "2022-05-08 15:30:04,519 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:30:04,520 EPOCH 8 done: loss 0.0642 - lr 0.100000\n",
      "2022-05-08 15:30:04,521 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 15:30:06,964 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:30:06,966 train mode resetting embeddings\n",
      "2022-05-08 15:30:06,976 train mode resetting embeddings\n",
      "2022-05-08 15:30:14,684 epoch 9 - iter 37/373 - loss 0.06138251 - samples/sec: 19.22 - lr: 0.100000\n",
      "2022-05-08 15:30:22,138 epoch 9 - iter 74/373 - loss 0.05402878 - samples/sec: 20.07 - lr: 0.100000\n",
      "2022-05-08 15:30:28,982 epoch 9 - iter 111/373 - loss 0.05790376 - samples/sec: 21.87 - lr: 0.100000\n",
      "2022-05-08 15:30:35,964 epoch 9 - iter 148/373 - loss 0.06000369 - samples/sec: 21.42 - lr: 0.100000\n",
      "2022-05-08 15:30:44,467 epoch 9 - iter 185/373 - loss 0.06098295 - samples/sec: 17.55 - lr: 0.100000\n",
      "2022-05-08 15:30:51,120 epoch 9 - iter 222/373 - loss 0.06210724 - samples/sec: 22.55 - lr: 0.100000\n",
      "2022-05-08 15:30:58,187 epoch 9 - iter 259/373 - loss 0.06230527 - samples/sec: 21.15 - lr: 0.100000\n",
      "2022-05-08 15:31:05,823 epoch 9 - iter 296/373 - loss 0.06130544 - samples/sec: 19.57 - lr: 0.100000\n",
      "2022-05-08 15:31:13,221 epoch 9 - iter 333/373 - loss 0.06216784 - samples/sec: 20.21 - lr: 0.100000\n",
      "2022-05-08 15:31:19,863 epoch 9 - iter 370/373 - loss 0.06095198 - samples/sec: 22.53 - lr: 0.100000\n",
      "2022-05-08 15:31:20,386 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:31:20,387 EPOCH 9 done: loss 0.0610 - lr 0.100000\n",
      "2022-05-08 15:31:20,388 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 15:31:22,580 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:31:22,581 train mode resetting embeddings\n",
      "2022-05-08 15:31:22,586 train mode resetting embeddings\n",
      "2022-05-08 15:31:30,333 epoch 10 - iter 37/373 - loss 0.06343890 - samples/sec: 19.12 - lr: 0.100000\n",
      "2022-05-08 15:31:37,998 epoch 10 - iter 74/373 - loss 0.05942358 - samples/sec: 19.50 - lr: 0.100000\n",
      "2022-05-08 15:31:44,811 epoch 10 - iter 111/373 - loss 0.05845109 - samples/sec: 21.98 - lr: 0.100000\n",
      "2022-05-08 15:31:51,853 epoch 10 - iter 148/373 - loss 0.05709846 - samples/sec: 21.23 - lr: 0.100000\n",
      "2022-05-08 15:31:59,219 epoch 10 - iter 185/373 - loss 0.05751887 - samples/sec: 20.30 - lr: 0.100000\n",
      "2022-05-08 15:32:07,007 epoch 10 - iter 222/373 - loss 0.05966371 - samples/sec: 19.19 - lr: 0.100000\n",
      "2022-05-08 15:32:14,998 epoch 10 - iter 259/373 - loss 0.06056622 - samples/sec: 18.69 - lr: 0.100000\n",
      "2022-05-08 15:32:22,666 epoch 10 - iter 296/373 - loss 0.06115151 - samples/sec: 19.48 - lr: 0.100000\n",
      "2022-05-08 15:32:29,692 epoch 10 - iter 333/373 - loss 0.06051918 - samples/sec: 21.27 - lr: 0.100000\n",
      "2022-05-08 15:32:36,016 epoch 10 - iter 370/373 - loss 0.06022376 - samples/sec: 23.66 - lr: 0.100000\n",
      "2022-05-08 15:32:36,523 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:32:36,524 EPOCH 10 done: loss 0.0602 - lr 0.100000\n",
      "2022-05-08 15:32:36,525 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 15:32:38,936 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:32:38,936 train mode resetting embeddings\n",
      "2022-05-08 15:32:38,942 train mode resetting embeddings\n",
      "2022-05-08 15:32:45,669 epoch 11 - iter 37/373 - loss 0.05307339 - samples/sec: 22.02 - lr: 0.100000\n",
      "2022-05-08 15:32:53,101 epoch 11 - iter 74/373 - loss 0.05560034 - samples/sec: 20.10 - lr: 0.100000\n",
      "2022-05-08 15:33:00,193 epoch 11 - iter 111/373 - loss 0.05481375 - samples/sec: 21.09 - lr: 0.100000\n",
      "2022-05-08 15:33:07,395 epoch 11 - iter 148/373 - loss 0.05558801 - samples/sec: 20.80 - lr: 0.100000\n",
      "2022-05-08 15:33:15,085 epoch 11 - iter 185/373 - loss 0.05740262 - samples/sec: 19.44 - lr: 0.100000\n",
      "2022-05-08 15:33:22,237 epoch 11 - iter 222/373 - loss 0.05787998 - samples/sec: 20.91 - lr: 0.100000\n",
      "2022-05-08 15:33:29,345 epoch 11 - iter 259/373 - loss 0.05834232 - samples/sec: 21.03 - lr: 0.100000\n",
      "2022-05-08 15:33:36,248 epoch 11 - iter 296/373 - loss 0.05781309 - samples/sec: 21.67 - lr: 0.100000\n",
      "2022-05-08 15:33:44,443 epoch 11 - iter 333/373 - loss 0.05886603 - samples/sec: 18.21 - lr: 0.100000\n",
      "2022-05-08 15:33:52,965 epoch 11 - iter 370/373 - loss 0.05797311 - samples/sec: 17.52 - lr: 0.100000\n",
      "2022-05-08 15:33:53,509 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:33:53,510 EPOCH 11 done: loss 0.0581 - lr 0.100000\n",
      "2022-05-08 15:33:53,511 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 15:33:55,824 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:33:55,825 train mode resetting embeddings\n",
      "2022-05-08 15:33:55,844 train mode resetting embeddings\n",
      "2022-05-08 15:34:03,469 epoch 12 - iter 37/373 - loss 0.05554439 - samples/sec: 19.43 - lr: 0.100000\n",
      "2022-05-08 15:34:11,159 epoch 12 - iter 74/373 - loss 0.05471166 - samples/sec: 19.44 - lr: 0.100000\n",
      "2022-05-08 15:34:18,216 epoch 12 - iter 111/373 - loss 0.05423142 - samples/sec: 21.21 - lr: 0.100000\n",
      "2022-05-08 15:34:25,435 epoch 12 - iter 148/373 - loss 0.05444013 - samples/sec: 20.73 - lr: 0.100000\n",
      "2022-05-08 15:34:33,205 epoch 12 - iter 185/373 - loss 0.05414514 - samples/sec: 19.22 - lr: 0.100000\n",
      "2022-05-08 15:34:39,869 epoch 12 - iter 222/373 - loss 0.05436756 - samples/sec: 22.49 - lr: 0.100000\n",
      "2022-05-08 15:34:47,644 epoch 12 - iter 259/373 - loss 0.05359876 - samples/sec: 19.22 - lr: 0.100000\n",
      "2022-05-08 15:34:55,427 epoch 12 - iter 296/373 - loss 0.05456628 - samples/sec: 19.20 - lr: 0.100000\n",
      "2022-05-08 15:35:02,062 epoch 12 - iter 333/373 - loss 0.05501322 - samples/sec: 22.57 - lr: 0.100000\n",
      "2022-05-08 15:35:09,065 epoch 12 - iter 370/373 - loss 0.05473652 - samples/sec: 21.34 - lr: 0.100000\n",
      "2022-05-08 15:35:09,853 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:35:09,854 EPOCH 12 done: loss 0.0547 - lr 0.100000\n",
      "2022-05-08 15:35:09,855 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 15:35:12,334 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:35:12,335 train mode resetting embeddings\n",
      "2022-05-08 15:35:12,340 train mode resetting embeddings\n",
      "2022-05-08 15:35:19,670 epoch 13 - iter 37/373 - loss 0.04817832 - samples/sec: 20.22 - lr: 0.100000\n",
      "2022-05-08 15:35:26,641 epoch 13 - iter 74/373 - loss 0.04986719 - samples/sec: 21.45 - lr: 0.100000\n",
      "2022-05-08 15:35:33,564 epoch 13 - iter 111/373 - loss 0.05065375 - samples/sec: 21.60 - lr: 0.100000\n",
      "2022-05-08 15:35:40,398 epoch 13 - iter 148/373 - loss 0.05186423 - samples/sec: 21.88 - lr: 0.100000\n",
      "2022-05-08 15:35:48,907 epoch 13 - iter 185/373 - loss 0.05228184 - samples/sec: 17.54 - lr: 0.100000\n",
      "2022-05-08 15:35:56,247 epoch 13 - iter 222/373 - loss 0.05079793 - samples/sec: 20.38 - lr: 0.100000\n",
      "2022-05-08 15:36:02,944 epoch 13 - iter 259/373 - loss 0.05145808 - samples/sec: 22.36 - lr: 0.100000\n",
      "2022-05-08 15:36:10,310 epoch 13 - iter 296/373 - loss 0.05206967 - samples/sec: 20.29 - lr: 0.100000\n",
      "2022-05-08 15:36:17,017 epoch 13 - iter 333/373 - loss 0.05141483 - samples/sec: 22.32 - lr: 0.100000\n",
      "2022-05-08 15:36:24,765 epoch 13 - iter 370/373 - loss 0.05233404 - samples/sec: 19.29 - lr: 0.100000\n",
      "2022-05-08 15:36:25,263 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:36:25,264 EPOCH 13 done: loss 0.0525 - lr 0.100000\n",
      "2022-05-08 15:36:25,265 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 15:36:27,482 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:36:27,483 train mode resetting embeddings\n",
      "2022-05-08 15:36:27,489 train mode resetting embeddings\n",
      "2022-05-08 15:36:34,069 epoch 14 - iter 37/373 - loss 0.04408599 - samples/sec: 22.52 - lr: 0.100000\n",
      "2022-05-08 15:36:41,208 epoch 14 - iter 74/373 - loss 0.05202629 - samples/sec: 20.94 - lr: 0.100000\n",
      "2022-05-08 15:36:48,866 epoch 14 - iter 111/373 - loss 0.04829314 - samples/sec: 19.50 - lr: 0.100000\n",
      "2022-05-08 15:36:55,548 epoch 14 - iter 148/373 - loss 0.05013099 - samples/sec: 22.38 - lr: 0.100000\n",
      "2022-05-08 15:37:03,196 epoch 14 - iter 185/373 - loss 0.04861318 - samples/sec: 19.55 - lr: 0.100000\n",
      "2022-05-08 15:37:10,568 epoch 14 - iter 222/373 - loss 0.04896961 - samples/sec: 20.29 - lr: 0.100000\n",
      "2022-05-08 15:37:17,873 epoch 14 - iter 259/373 - loss 0.04912957 - samples/sec: 20.47 - lr: 0.100000\n",
      "2022-05-08 15:37:26,286 epoch 14 - iter 296/373 - loss 0.04945309 - samples/sec: 17.75 - lr: 0.100000\n",
      "2022-05-08 15:37:34,180 epoch 14 - iter 333/373 - loss 0.04885160 - samples/sec: 18.95 - lr: 0.100000\n",
      "2022-05-08 15:37:41,090 epoch 14 - iter 370/373 - loss 0.04960399 - samples/sec: 21.64 - lr: 0.100000\n",
      "2022-05-08 15:37:41,441 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:37:41,441 EPOCH 14 done: loss 0.0497 - lr 0.100000\n",
      "2022-05-08 15:37:41,442 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 15:37:43,924 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:37:43,924 train mode resetting embeddings\n",
      "2022-05-08 15:37:43,930 train mode resetting embeddings\n",
      "2022-05-08 15:37:50,783 epoch 15 - iter 37/373 - loss 0.05377201 - samples/sec: 21.62 - lr: 0.100000\n",
      "2022-05-08 15:37:58,040 epoch 15 - iter 74/373 - loss 0.05071733 - samples/sec: 20.59 - lr: 0.100000\n",
      "2022-05-08 15:38:05,150 epoch 15 - iter 111/373 - loss 0.04856637 - samples/sec: 21.03 - lr: 0.100000\n",
      "2022-05-08 15:38:13,243 epoch 15 - iter 148/373 - loss 0.04753386 - samples/sec: 18.46 - lr: 0.100000\n",
      "2022-05-08 15:38:21,111 epoch 15 - iter 185/373 - loss 0.04713319 - samples/sec: 18.98 - lr: 0.100000\n",
      "2022-05-08 15:38:28,348 epoch 15 - iter 222/373 - loss 0.04720203 - samples/sec: 20.66 - lr: 0.100000\n",
      "2022-05-08 15:38:34,509 epoch 15 - iter 259/373 - loss 0.04766019 - samples/sec: 24.35 - lr: 0.100000\n",
      "2022-05-08 15:38:42,296 epoch 15 - iter 296/373 - loss 0.04743506 - samples/sec: 19.19 - lr: 0.100000\n",
      "2022-05-08 15:38:49,950 epoch 15 - iter 333/373 - loss 0.04858237 - samples/sec: 19.54 - lr: 0.100000\n",
      "2022-05-08 15:38:57,272 epoch 15 - iter 370/373 - loss 0.04947328 - samples/sec: 20.46 - lr: 0.100000\n",
      "2022-05-08 15:38:57,647 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:38:57,648 EPOCH 15 done: loss 0.0494 - lr 0.100000\n",
      "2022-05-08 15:38:57,649 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 15:38:59,920 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:38:59,921 train mode resetting embeddings\n",
      "2022-05-08 15:38:59,928 train mode resetting embeddings\n",
      "2022-05-08 15:39:06,845 epoch 16 - iter 37/373 - loss 0.04416635 - samples/sec: 21.42 - lr: 0.100000\n",
      "2022-05-08 15:39:15,170 epoch 16 - iter 74/373 - loss 0.04555986 - samples/sec: 17.94 - lr: 0.100000\n",
      "2022-05-08 15:39:22,396 epoch 16 - iter 111/373 - loss 0.04589685 - samples/sec: 20.70 - lr: 0.100000\n",
      "2022-05-08 15:39:29,587 epoch 16 - iter 148/373 - loss 0.04601447 - samples/sec: 20.80 - lr: 0.100000\n",
      "2022-05-08 15:39:36,973 epoch 16 - iter 185/373 - loss 0.04536438 - samples/sec: 20.27 - lr: 0.100000\n",
      "2022-05-08 15:39:44,692 epoch 16 - iter 222/373 - loss 0.04568144 - samples/sec: 19.36 - lr: 0.100000\n",
      "2022-05-08 15:39:51,612 epoch 16 - iter 259/373 - loss 0.04737353 - samples/sec: 21.61 - lr: 0.100000\n",
      "2022-05-08 15:39:59,795 epoch 16 - iter 296/373 - loss 0.04735457 - samples/sec: 18.25 - lr: 0.100000\n",
      "2022-05-08 15:40:05,526 epoch 16 - iter 333/373 - loss 0.04752000 - samples/sec: 26.16 - lr: 0.100000\n",
      "2022-05-08 15:40:11,836 epoch 16 - iter 370/373 - loss 0.04837340 - samples/sec: 23.74 - lr: 0.100000\n",
      "2022-05-08 15:40:12,748 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:40:12,749 EPOCH 16 done: loss 0.0484 - lr 0.100000\n",
      "2022-05-08 15:40:12,749 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 15:40:15,155 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:40:15,156 train mode resetting embeddings\n",
      "2022-05-08 15:40:15,162 train mode resetting embeddings\n",
      "2022-05-08 15:40:21,865 epoch 17 - iter 37/373 - loss 0.04146993 - samples/sec: 22.11 - lr: 0.100000\n",
      "2022-05-08 15:40:28,827 epoch 17 - iter 74/373 - loss 0.04612034 - samples/sec: 21.47 - lr: 0.100000\n",
      "2022-05-08 15:40:36,227 epoch 17 - iter 111/373 - loss 0.04460990 - samples/sec: 20.19 - lr: 0.100000\n",
      "2022-05-08 15:40:42,360 epoch 17 - iter 148/373 - loss 0.04553081 - samples/sec: 24.44 - lr: 0.100000\n",
      "2022-05-08 15:40:50,201 epoch 17 - iter 185/373 - loss 0.04746070 - samples/sec: 19.05 - lr: 0.100000\n",
      "2022-05-08 15:40:58,077 epoch 17 - iter 222/373 - loss 0.04746463 - samples/sec: 18.97 - lr: 0.100000\n",
      "2022-05-08 15:41:05,551 epoch 17 - iter 259/373 - loss 0.04674829 - samples/sec: 19.99 - lr: 0.100000\n",
      "2022-05-08 15:41:13,587 epoch 17 - iter 296/373 - loss 0.04622775 - samples/sec: 18.58 - lr: 0.100000\n",
      "2022-05-08 15:41:20,609 epoch 17 - iter 333/373 - loss 0.04531760 - samples/sec: 21.31 - lr: 0.100000\n",
      "2022-05-08 15:41:27,995 epoch 17 - iter 370/373 - loss 0.04493483 - samples/sec: 20.25 - lr: 0.100000\n",
      "2022-05-08 15:41:28,809 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:41:28,810 EPOCH 17 done: loss 0.0448 - lr 0.100000\n",
      "2022-05-08 15:41:28,811 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 15:41:30,994 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:41:30,996 train mode resetting embeddings\n",
      "2022-05-08 15:41:31,001 train mode resetting embeddings\n",
      "2022-05-08 15:41:39,160 epoch 18 - iter 37/373 - loss 0.04633930 - samples/sec: 18.16 - lr: 0.100000\n",
      "2022-05-08 15:41:46,479 epoch 18 - iter 74/373 - loss 0.04371828 - samples/sec: 20.44 - lr: 0.100000\n",
      "2022-05-08 15:41:53,368 epoch 18 - iter 111/373 - loss 0.04452836 - samples/sec: 21.72 - lr: 0.100000\n",
      "2022-05-08 15:42:00,558 epoch 18 - iter 148/373 - loss 0.04431522 - samples/sec: 20.79 - lr: 0.100000\n",
      "2022-05-08 15:42:07,846 epoch 18 - iter 185/373 - loss 0.04298038 - samples/sec: 20.51 - lr: 0.100000\n",
      "2022-05-08 15:42:14,733 epoch 18 - iter 222/373 - loss 0.04449950 - samples/sec: 21.72 - lr: 0.100000\n",
      "2022-05-08 15:42:21,039 epoch 18 - iter 259/373 - loss 0.04588138 - samples/sec: 23.74 - lr: 0.100000\n",
      "2022-05-08 15:42:28,438 epoch 18 - iter 296/373 - loss 0.04486486 - samples/sec: 20.21 - lr: 0.100000\n",
      "2022-05-08 15:42:35,919 epoch 18 - iter 333/373 - loss 0.04415102 - samples/sec: 19.97 - lr: 0.100000\n",
      "2022-05-08 15:42:43,611 epoch 18 - iter 370/373 - loss 0.04451138 - samples/sec: 19.43 - lr: 0.100000\n",
      "2022-05-08 15:42:44,211 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:42:44,212 EPOCH 18 done: loss 0.0446 - lr 0.100000\n",
      "2022-05-08 15:42:44,213 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 15:42:46,574 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:42:46,575 train mode resetting embeddings\n",
      "2022-05-08 15:42:46,582 train mode resetting embeddings\n",
      "2022-05-08 15:42:53,082 epoch 19 - iter 37/373 - loss 0.04529105 - samples/sec: 22.80 - lr: 0.100000\n",
      "2022-05-08 15:42:59,606 epoch 19 - iter 74/373 - loss 0.04206198 - samples/sec: 22.93 - lr: 0.100000\n",
      "2022-05-08 15:43:06,693 epoch 19 - iter 111/373 - loss 0.04292139 - samples/sec: 21.10 - lr: 0.100000\n",
      "2022-05-08 15:43:14,015 epoch 19 - iter 148/373 - loss 0.04428738 - samples/sec: 20.42 - lr: 0.100000\n",
      "2022-05-08 15:43:20,920 epoch 19 - iter 185/373 - loss 0.04551580 - samples/sec: 21.64 - lr: 0.100000\n",
      "2022-05-08 15:43:28,311 epoch 19 - iter 222/373 - loss 0.04464204 - samples/sec: 20.24 - lr: 0.100000\n",
      "2022-05-08 15:43:35,888 epoch 19 - iter 259/373 - loss 0.04500890 - samples/sec: 19.74 - lr: 0.100000\n",
      "2022-05-08 15:43:43,910 epoch 19 - iter 296/373 - loss 0.04368546 - samples/sec: 18.63 - lr: 0.100000\n",
      "2022-05-08 15:43:50,795 epoch 19 - iter 333/373 - loss 0.04357423 - samples/sec: 21.76 - lr: 0.100000\n",
      "2022-05-08 15:43:58,777 epoch 19 - iter 370/373 - loss 0.04246780 - samples/sec: 18.70 - lr: 0.100000\n",
      "2022-05-08 15:43:59,298 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:43:59,300 EPOCH 19 done: loss 0.0423 - lr 0.100000\n",
      "2022-05-08 15:43:59,301 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 15:44:01,467 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:44:01,468 train mode resetting embeddings\n",
      "2022-05-08 15:44:01,474 train mode resetting embeddings\n",
      "2022-05-08 15:44:08,163 epoch 20 - iter 37/373 - loss 0.03858150 - samples/sec: 22.15 - lr: 0.100000\n",
      "2022-05-08 15:44:15,941 epoch 20 - iter 74/373 - loss 0.04056388 - samples/sec: 19.22 - lr: 0.100000\n",
      "2022-05-08 15:44:24,103 epoch 20 - iter 111/373 - loss 0.04174702 - samples/sec: 18.30 - lr: 0.100000\n",
      "2022-05-08 15:44:31,431 epoch 20 - iter 148/373 - loss 0.04116205 - samples/sec: 20.40 - lr: 0.100000\n",
      "2022-05-08 15:44:37,554 epoch 20 - iter 185/373 - loss 0.04149132 - samples/sec: 24.45 - lr: 0.100000\n",
      "2022-05-08 15:44:44,810 epoch 20 - iter 222/373 - loss 0.04126770 - samples/sec: 20.61 - lr: 0.100000\n",
      "2022-05-08 15:44:52,010 epoch 20 - iter 259/373 - loss 0.04322575 - samples/sec: 20.81 - lr: 0.100000\n",
      "2022-05-08 15:44:59,641 epoch 20 - iter 296/373 - loss 0.04328821 - samples/sec: 19.58 - lr: 0.100000\n",
      "2022-05-08 15:45:07,142 epoch 20 - iter 333/373 - loss 0.04312997 - samples/sec: 19.92 - lr: 0.100000\n",
      "2022-05-08 15:45:13,893 epoch 20 - iter 370/373 - loss 0.04389869 - samples/sec: 22.15 - lr: 0.100000\n",
      "2022-05-08 15:45:14,412 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:45:14,413 EPOCH 20 done: loss 0.0439 - lr 0.100000\n",
      "2022-05-08 15:45:14,413 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 15:45:16,568 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:45:16,569 train mode resetting embeddings\n",
      "2022-05-08 15:45:16,575 train mode resetting embeddings\n",
      "2022-05-08 15:45:23,220 epoch 21 - iter 37/373 - loss 0.03838804 - samples/sec: 22.30 - lr: 0.100000\n",
      "2022-05-08 15:45:30,171 epoch 21 - iter 74/373 - loss 0.04324207 - samples/sec: 21.52 - lr: 0.100000\n",
      "2022-05-08 15:45:36,826 epoch 21 - iter 111/373 - loss 0.04171513 - samples/sec: 22.48 - lr: 0.100000\n",
      "2022-05-08 15:45:44,086 epoch 21 - iter 148/373 - loss 0.04205513 - samples/sec: 20.60 - lr: 0.100000\n",
      "2022-05-08 15:45:51,873 epoch 21 - iter 185/373 - loss 0.04125243 - samples/sec: 19.20 - lr: 0.100000\n",
      "2022-05-08 15:45:58,392 epoch 21 - iter 222/373 - loss 0.04126329 - samples/sec: 22.96 - lr: 0.100000\n",
      "2022-05-08 15:46:05,119 epoch 21 - iter 259/373 - loss 0.04167441 - samples/sec: 22.24 - lr: 0.100000\n",
      "2022-05-08 15:46:12,865 epoch 21 - iter 296/373 - loss 0.04297517 - samples/sec: 19.29 - lr: 0.100000\n",
      "2022-05-08 15:46:19,830 epoch 21 - iter 333/373 - loss 0.04242512 - samples/sec: 21.48 - lr: 0.100000\n",
      "2022-05-08 15:46:27,203 epoch 21 - iter 370/373 - loss 0.04237462 - samples/sec: 20.26 - lr: 0.100000\n",
      "2022-05-08 15:46:27,971 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:46:27,972 EPOCH 21 done: loss 0.0422 - lr 0.100000\n",
      "2022-05-08 15:46:27,973 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 15:46:30,372 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:46:30,373 train mode resetting embeddings\n",
      "2022-05-08 15:46:30,380 train mode resetting embeddings\n",
      "2022-05-08 15:46:36,936 epoch 22 - iter 37/373 - loss 0.04202392 - samples/sec: 22.60 - lr: 0.100000\n",
      "2022-05-08 15:46:43,869 epoch 22 - iter 74/373 - loss 0.04143183 - samples/sec: 21.61 - lr: 0.100000\n",
      "2022-05-08 15:46:50,315 epoch 22 - iter 111/373 - loss 0.03803444 - samples/sec: 23.26 - lr: 0.100000\n",
      "2022-05-08 15:46:58,035 epoch 22 - iter 148/373 - loss 0.03720856 - samples/sec: 19.35 - lr: 0.100000\n",
      "2022-05-08 15:47:05,023 epoch 22 - iter 185/373 - loss 0.03826635 - samples/sec: 21.45 - lr: 0.100000\n",
      "2022-05-08 15:47:12,951 epoch 22 - iter 222/373 - loss 0.03744966 - samples/sec: 18.86 - lr: 0.100000\n",
      "2022-05-08 15:47:20,242 epoch 22 - iter 259/373 - loss 0.03756248 - samples/sec: 20.50 - lr: 0.100000\n",
      "2022-05-08 15:47:28,356 epoch 22 - iter 296/373 - loss 0.03803569 - samples/sec: 18.41 - lr: 0.100000\n",
      "2022-05-08 15:47:35,737 epoch 22 - iter 333/373 - loss 0.03773574 - samples/sec: 20.25 - lr: 0.100000\n",
      "2022-05-08 15:47:43,540 epoch 22 - iter 370/373 - loss 0.03887671 - samples/sec: 19.15 - lr: 0.100000\n",
      "2022-05-08 15:47:44,321 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:47:44,322 EPOCH 22 done: loss 0.0387 - lr 0.100000\n",
      "2022-05-08 15:47:44,322 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 15:47:46,671 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:47:46,671 train mode resetting embeddings\n",
      "2022-05-08 15:47:46,679 train mode resetting embeddings\n",
      "2022-05-08 15:47:53,720 epoch 23 - iter 37/373 - loss 0.02963676 - samples/sec: 21.04 - lr: 0.100000\n",
      "2022-05-08 15:48:02,319 epoch 23 - iter 74/373 - loss 0.03356074 - samples/sec: 17.35 - lr: 0.100000\n",
      "2022-05-08 15:48:08,157 epoch 23 - iter 111/373 - loss 0.03637151 - samples/sec: 25.68 - lr: 0.100000\n",
      "2022-05-08 15:48:14,929 epoch 23 - iter 148/373 - loss 0.03549043 - samples/sec: 22.11 - lr: 0.100000\n",
      "2022-05-08 15:48:23,266 epoch 23 - iter 185/373 - loss 0.03660183 - samples/sec: 17.92 - lr: 0.100000\n",
      "2022-05-08 15:48:30,973 epoch 23 - iter 222/373 - loss 0.03806838 - samples/sec: 19.40 - lr: 0.100000\n",
      "2022-05-08 15:48:39,028 epoch 23 - iter 259/373 - loss 0.03806689 - samples/sec: 18.56 - lr: 0.100000\n",
      "2022-05-08 15:48:46,238 epoch 23 - iter 296/373 - loss 0.03764425 - samples/sec: 20.74 - lr: 0.100000\n",
      "2022-05-08 15:48:52,887 epoch 23 - iter 333/373 - loss 0.03793854 - samples/sec: 22.55 - lr: 0.100000\n",
      "2022-05-08 15:48:59,674 epoch 23 - iter 370/373 - loss 0.03842685 - samples/sec: 22.05 - lr: 0.100000\n",
      "2022-05-08 15:49:00,283 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:49:00,284 EPOCH 23 done: loss 0.0386 - lr 0.100000\n",
      "2022-05-08 15:49:00,286 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 15:49:02,751 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:49:02,753 train mode resetting embeddings\n",
      "2022-05-08 15:49:02,759 train mode resetting embeddings\n",
      "2022-05-08 15:49:10,761 epoch 24 - iter 37/373 - loss 0.04439800 - samples/sec: 18.51 - lr: 0.100000\n",
      "2022-05-08 15:49:18,101 epoch 24 - iter 74/373 - loss 0.04193223 - samples/sec: 20.37 - lr: 0.100000\n",
      "2022-05-08 15:49:25,587 epoch 24 - iter 111/373 - loss 0.03893636 - samples/sec: 19.96 - lr: 0.100000\n",
      "2022-05-08 15:49:32,921 epoch 24 - iter 148/373 - loss 0.03821524 - samples/sec: 20.40 - lr: 0.100000\n",
      "2022-05-08 15:49:40,740 epoch 24 - iter 185/373 - loss 0.03735380 - samples/sec: 19.10 - lr: 0.100000\n",
      "2022-05-08 15:49:46,969 epoch 24 - iter 222/373 - loss 0.03843742 - samples/sec: 24.07 - lr: 0.100000\n",
      "2022-05-08 15:49:54,211 epoch 24 - iter 259/373 - loss 0.03839123 - samples/sec: 20.64 - lr: 0.100000\n",
      "2022-05-08 15:50:00,954 epoch 24 - iter 296/373 - loss 0.03848403 - samples/sec: 22.21 - lr: 0.100000\n",
      "2022-05-08 15:50:08,160 epoch 24 - iter 333/373 - loss 0.03846090 - samples/sec: 20.75 - lr: 0.100000\n",
      "2022-05-08 15:50:14,827 epoch 24 - iter 370/373 - loss 0.03827877 - samples/sec: 22.44 - lr: 0.100000\n",
      "2022-05-08 15:50:15,561 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:50:15,562 EPOCH 24 done: loss 0.0383 - lr 0.100000\n",
      "2022-05-08 15:50:15,563 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 15:50:17,706 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:50:17,707 train mode resetting embeddings\n",
      "2022-05-08 15:50:17,713 train mode resetting embeddings\n",
      "2022-05-08 15:50:24,126 epoch 25 - iter 37/373 - loss 0.04007938 - samples/sec: 23.10 - lr: 0.100000\n",
      "2022-05-08 15:50:31,583 epoch 25 - iter 74/373 - loss 0.03872435 - samples/sec: 20.03 - lr: 0.100000\n",
      "2022-05-08 15:50:37,741 epoch 25 - iter 111/373 - loss 0.03848200 - samples/sec: 24.30 - lr: 0.100000\n",
      "2022-05-08 15:50:44,780 epoch 25 - iter 148/373 - loss 0.04063894 - samples/sec: 21.24 - lr: 0.100000\n",
      "2022-05-08 15:50:51,339 epoch 25 - iter 185/373 - loss 0.03965995 - samples/sec: 22.80 - lr: 0.100000\n",
      "2022-05-08 15:50:58,632 epoch 25 - iter 222/373 - loss 0.03960298 - samples/sec: 20.50 - lr: 0.100000\n",
      "2022-05-08 15:51:05,146 epoch 25 - iter 259/373 - loss 0.03998113 - samples/sec: 23.02 - lr: 0.100000\n",
      "2022-05-08 15:51:12,018 epoch 25 - iter 296/373 - loss 0.03978802 - samples/sec: 21.76 - lr: 0.100000\n",
      "2022-05-08 15:51:19,491 epoch 25 - iter 333/373 - loss 0.03862206 - samples/sec: 20.00 - lr: 0.100000\n",
      "2022-05-08 15:51:27,105 epoch 25 - iter 370/373 - loss 0.03769011 - samples/sec: 19.63 - lr: 0.100000\n",
      "2022-05-08 15:51:27,650 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:51:27,650 EPOCH 25 done: loss 0.0379 - lr 0.100000\n",
      "2022-05-08 15:51:27,651 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 15:51:29,787 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:51:29,787 train mode resetting embeddings\n",
      "2022-05-08 15:51:29,794 train mode resetting embeddings\n",
      "2022-05-08 15:51:36,453 epoch 26 - iter 37/373 - loss 0.03245264 - samples/sec: 22.26 - lr: 0.100000\n",
      "2022-05-08 15:51:42,276 epoch 26 - iter 74/373 - loss 0.03190733 - samples/sec: 25.74 - lr: 0.100000\n",
      "2022-05-08 15:51:50,077 epoch 26 - iter 111/373 - loss 0.03409534 - samples/sec: 19.14 - lr: 0.100000\n",
      "2022-05-08 15:51:57,240 epoch 26 - iter 148/373 - loss 0.03560566 - samples/sec: 20.88 - lr: 0.100000\n",
      "2022-05-08 15:52:04,596 epoch 26 - iter 185/373 - loss 0.03683611 - samples/sec: 20.31 - lr: 0.100000\n",
      "2022-05-08 15:52:11,336 epoch 26 - iter 222/373 - loss 0.03603396 - samples/sec: 22.18 - lr: 0.100000\n",
      "2022-05-08 15:52:18,331 epoch 26 - iter 259/373 - loss 0.03571731 - samples/sec: 21.37 - lr: 0.100000\n",
      "2022-05-08 15:52:24,893 epoch 26 - iter 296/373 - loss 0.03604399 - samples/sec: 22.81 - lr: 0.100000\n",
      "2022-05-08 15:52:31,561 epoch 26 - iter 333/373 - loss 0.03575720 - samples/sec: 22.44 - lr: 0.100000\n",
      "2022-05-08 15:52:39,576 epoch 26 - iter 370/373 - loss 0.03537708 - samples/sec: 18.63 - lr: 0.100000\n",
      "2022-05-08 15:52:40,459 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:52:40,460 EPOCH 26 done: loss 0.0352 - lr 0.100000\n",
      "2022-05-08 15:52:40,461 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 15:52:42,678 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:52:42,679 train mode resetting embeddings\n",
      "2022-05-08 15:52:42,686 train mode resetting embeddings\n",
      "2022-05-08 15:52:50,841 epoch 27 - iter 37/373 - loss 0.03442195 - samples/sec: 18.17 - lr: 0.100000\n",
      "2022-05-08 15:52:57,667 epoch 27 - iter 74/373 - loss 0.03488967 - samples/sec: 21.91 - lr: 0.100000\n",
      "2022-05-08 15:53:05,234 epoch 27 - iter 111/373 - loss 0.03452178 - samples/sec: 19.74 - lr: 0.100000\n",
      "2022-05-08 15:53:11,538 epoch 27 - iter 148/373 - loss 0.03293622 - samples/sec: 23.77 - lr: 0.100000\n",
      "2022-05-08 15:53:19,255 epoch 27 - iter 185/373 - loss 0.03449830 - samples/sec: 19.37 - lr: 0.100000\n",
      "2022-05-08 15:53:26,296 epoch 27 - iter 222/373 - loss 0.03375259 - samples/sec: 21.24 - lr: 0.100000\n",
      "2022-05-08 15:53:32,533 epoch 27 - iter 259/373 - loss 0.03570934 - samples/sec: 23.98 - lr: 0.100000\n",
      "2022-05-08 15:53:38,589 epoch 27 - iter 296/373 - loss 0.03506517 - samples/sec: 24.71 - lr: 0.100000\n",
      "2022-05-08 15:53:45,698 epoch 27 - iter 333/373 - loss 0.03451712 - samples/sec: 21.03 - lr: 0.100000\n",
      "2022-05-08 15:53:52,162 epoch 27 - iter 370/373 - loss 0.03467089 - samples/sec: 23.16 - lr: 0.100000\n",
      "2022-05-08 15:53:52,783 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:53:52,784 EPOCH 27 done: loss 0.0348 - lr 0.100000\n",
      "2022-05-08 15:53:52,784 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 15:53:55,288 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:53:55,289 train mode resetting embeddings\n",
      "2022-05-08 15:53:55,297 train mode resetting embeddings\n",
      "2022-05-08 15:54:02,298 epoch 28 - iter 37/373 - loss 0.03557548 - samples/sec: 21.16 - lr: 0.100000\n",
      "2022-05-08 15:54:08,858 epoch 28 - iter 74/373 - loss 0.03757339 - samples/sec: 22.82 - lr: 0.100000\n",
      "2022-05-08 15:54:15,441 epoch 28 - iter 111/373 - loss 0.03792428 - samples/sec: 22.74 - lr: 0.100000\n",
      "2022-05-08 15:54:22,164 epoch 28 - iter 148/373 - loss 0.04046724 - samples/sec: 22.28 - lr: 0.100000\n",
      "2022-05-08 15:54:28,883 epoch 28 - iter 185/373 - loss 0.03938071 - samples/sec: 22.27 - lr: 0.100000\n",
      "2022-05-08 15:54:35,842 epoch 28 - iter 222/373 - loss 0.03802844 - samples/sec: 21.49 - lr: 0.100000\n",
      "2022-05-08 15:54:42,506 epoch 28 - iter 259/373 - loss 0.03694824 - samples/sec: 22.45 - lr: 0.100000\n",
      "2022-05-08 15:54:49,874 epoch 28 - iter 296/373 - loss 0.03531381 - samples/sec: 20.27 - lr: 0.100000\n",
      "2022-05-08 15:54:56,454 epoch 28 - iter 333/373 - loss 0.03518588 - samples/sec: 22.72 - lr: 0.100000\n",
      "2022-05-08 15:55:04,066 epoch 28 - iter 370/373 - loss 0.03480414 - samples/sec: 19.62 - lr: 0.100000\n",
      "2022-05-08 15:55:04,566 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:55:04,567 EPOCH 28 done: loss 0.0349 - lr 0.100000\n",
      "2022-05-08 15:55:04,568 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 15:55:06,655 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:55:06,657 train mode resetting embeddings\n",
      "2022-05-08 15:55:06,663 train mode resetting embeddings\n",
      "2022-05-08 15:55:13,853 epoch 29 - iter 37/373 - loss 0.02621301 - samples/sec: 20.61 - lr: 0.100000\n",
      "2022-05-08 15:55:20,224 epoch 29 - iter 74/373 - loss 0.02745457 - samples/sec: 23.51 - lr: 0.100000\n",
      "2022-05-08 15:55:26,692 epoch 29 - iter 111/373 - loss 0.03291205 - samples/sec: 23.13 - lr: 0.100000\n",
      "2022-05-08 15:55:34,584 epoch 29 - iter 148/373 - loss 0.03212928 - samples/sec: 18.92 - lr: 0.100000\n",
      "2022-05-08 15:55:41,927 epoch 29 - iter 185/373 - loss 0.03310594 - samples/sec: 20.35 - lr: 0.100000\n",
      "2022-05-08 15:55:49,141 epoch 29 - iter 222/373 - loss 0.03430554 - samples/sec: 20.71 - lr: 0.100000\n",
      "2022-05-08 15:55:55,935 epoch 29 - iter 259/373 - loss 0.03424652 - samples/sec: 22.02 - lr: 0.100000\n",
      "2022-05-08 15:56:02,803 epoch 29 - iter 296/373 - loss 0.03359225 - samples/sec: 21.77 - lr: 0.100000\n",
      "2022-05-08 15:56:09,561 epoch 29 - iter 333/373 - loss 0.03417492 - samples/sec: 22.12 - lr: 0.100000\n",
      "2022-05-08 15:56:16,260 epoch 29 - iter 370/373 - loss 0.03323959 - samples/sec: 22.34 - lr: 0.100000\n",
      "2022-05-08 15:56:16,985 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:56:16,986 EPOCH 29 done: loss 0.0333 - lr 0.100000\n",
      "2022-05-08 15:56:16,986 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 15:56:19,293 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:56:19,294 train mode resetting embeddings\n",
      "2022-05-08 15:56:19,299 train mode resetting embeddings\n",
      "2022-05-08 15:56:27,614 epoch 30 - iter 37/373 - loss 0.03438906 - samples/sec: 17.81 - lr: 0.100000\n",
      "2022-05-08 15:56:32,972 epoch 30 - iter 74/373 - loss 0.03297924 - samples/sec: 28.05 - lr: 0.100000\n",
      "2022-05-08 15:56:39,930 epoch 30 - iter 111/373 - loss 0.03209984 - samples/sec: 21.48 - lr: 0.100000\n",
      "2022-05-08 15:56:46,917 epoch 30 - iter 148/373 - loss 0.03149756 - samples/sec: 21.39 - lr: 0.100000\n",
      "2022-05-08 15:56:53,347 epoch 30 - iter 185/373 - loss 0.03102938 - samples/sec: 23.26 - lr: 0.100000\n",
      "2022-05-08 15:56:59,617 epoch 30 - iter 222/373 - loss 0.03187938 - samples/sec: 23.86 - lr: 0.100000\n",
      "2022-05-08 15:57:07,206 epoch 30 - iter 259/373 - loss 0.03204351 - samples/sec: 19.70 - lr: 0.100000\n",
      "2022-05-08 15:57:14,256 epoch 30 - iter 296/373 - loss 0.03226184 - samples/sec: 21.21 - lr: 0.100000\n",
      "2022-05-08 15:57:21,359 epoch 30 - iter 333/373 - loss 0.03284775 - samples/sec: 21.04 - lr: 0.100000\n",
      "2022-05-08 15:57:27,770 epoch 30 - iter 370/373 - loss 0.03315154 - samples/sec: 23.34 - lr: 0.100000\n",
      "2022-05-08 15:57:28,438 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:57:28,439 EPOCH 30 done: loss 0.0333 - lr 0.100000\n",
      "2022-05-08 15:57:28,440 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 15:57:30,555 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:57:30,556 train mode resetting embeddings\n",
      "2022-05-08 15:57:30,562 train mode resetting embeddings\n",
      "2022-05-08 15:57:38,215 epoch 31 - iter 37/373 - loss 0.03025338 - samples/sec: 19.36 - lr: 0.100000\n",
      "2022-05-08 15:57:44,606 epoch 31 - iter 74/373 - loss 0.03037219 - samples/sec: 23.41 - lr: 0.100000\n",
      "2022-05-08 15:57:51,903 epoch 31 - iter 111/373 - loss 0.02887184 - samples/sec: 20.49 - lr: 0.100000\n",
      "2022-05-08 15:57:58,908 epoch 31 - iter 148/373 - loss 0.03152748 - samples/sec: 21.33 - lr: 0.100000\n",
      "2022-05-08 15:58:04,936 epoch 31 - iter 185/373 - loss 0.03194971 - samples/sec: 24.84 - lr: 0.100000\n",
      "2022-05-08 15:58:12,362 epoch 31 - iter 222/373 - loss 0.03090734 - samples/sec: 20.12 - lr: 0.100000\n",
      "2022-05-08 15:58:19,132 epoch 31 - iter 259/373 - loss 0.03207807 - samples/sec: 22.09 - lr: 0.100000\n",
      "2022-05-08 15:58:26,290 epoch 31 - iter 296/373 - loss 0.03218739 - samples/sec: 20.87 - lr: 0.100000\n",
      "2022-05-08 15:58:32,913 epoch 31 - iter 333/373 - loss 0.03241902 - samples/sec: 22.59 - lr: 0.100000\n",
      "2022-05-08 15:58:38,852 epoch 31 - iter 370/373 - loss 0.03181514 - samples/sec: 25.20 - lr: 0.100000\n",
      "2022-05-08 15:58:39,667 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:58:39,668 EPOCH 31 done: loss 0.0318 - lr 0.100000\n",
      "2022-05-08 15:58:39,669 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 15:58:41,970 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:58:41,971 train mode resetting embeddings\n",
      "2022-05-08 15:58:41,978 train mode resetting embeddings\n",
      "2022-05-08 15:58:47,850 epoch 32 - iter 37/373 - loss 0.03622566 - samples/sec: 25.23 - lr: 0.100000\n",
      "2022-05-08 15:58:54,594 epoch 32 - iter 74/373 - loss 0.03369039 - samples/sec: 22.17 - lr: 0.100000\n",
      "2022-05-08 15:59:02,358 epoch 32 - iter 111/373 - loss 0.03052979 - samples/sec: 19.23 - lr: 0.100000\n",
      "2022-05-08 15:59:08,866 epoch 32 - iter 148/373 - loss 0.02949457 - samples/sec: 23.00 - lr: 0.100000\n",
      "2022-05-08 15:59:15,911 epoch 32 - iter 185/373 - loss 0.02923871 - samples/sec: 21.21 - lr: 0.100000\n",
      "2022-05-08 15:59:23,008 epoch 32 - iter 222/373 - loss 0.02836376 - samples/sec: 21.07 - lr: 0.100000\n",
      "2022-05-08 15:59:30,131 epoch 32 - iter 259/373 - loss 0.02831954 - samples/sec: 20.99 - lr: 0.100000\n",
      "2022-05-08 15:59:37,234 epoch 32 - iter 296/373 - loss 0.02940135 - samples/sec: 21.05 - lr: 0.100000\n",
      "2022-05-08 15:59:44,454 epoch 32 - iter 333/373 - loss 0.02923587 - samples/sec: 20.69 - lr: 0.100000\n",
      "2022-05-08 15:59:51,383 epoch 32 - iter 370/373 - loss 0.02918951 - samples/sec: 21.58 - lr: 0.100000\n",
      "2022-05-08 15:59:51,826 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:59:51,826 EPOCH 32 done: loss 0.0292 - lr 0.100000\n",
      "2022-05-08 15:59:51,828 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 15:59:53,963 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 15:59:53,964 train mode resetting embeddings\n",
      "2022-05-08 15:59:53,970 train mode resetting embeddings\n",
      "2022-05-08 16:00:01,991 epoch 33 - iter 37/373 - loss 0.03333287 - samples/sec: 18.47 - lr: 0.100000\n",
      "2022-05-08 16:00:08,778 epoch 33 - iter 74/373 - loss 0.03381012 - samples/sec: 22.02 - lr: 0.100000\n",
      "2022-05-08 16:00:15,860 epoch 33 - iter 111/373 - loss 0.03401767 - samples/sec: 21.11 - lr: 0.100000\n",
      "2022-05-08 16:00:22,406 epoch 33 - iter 148/373 - loss 0.03504334 - samples/sec: 22.86 - lr: 0.100000\n",
      "2022-05-08 16:00:29,528 epoch 33 - iter 185/373 - loss 0.03241038 - samples/sec: 20.98 - lr: 0.100000\n",
      "2022-05-08 16:00:35,642 epoch 33 - iter 222/373 - loss 0.03072625 - samples/sec: 24.48 - lr: 0.100000\n",
      "2022-05-08 16:00:41,983 epoch 33 - iter 259/373 - loss 0.03055091 - samples/sec: 23.60 - lr: 0.100000\n",
      "2022-05-08 16:00:48,214 epoch 33 - iter 296/373 - loss 0.03061303 - samples/sec: 24.00 - lr: 0.100000\n",
      "2022-05-08 16:00:54,658 epoch 33 - iter 333/373 - loss 0.03057436 - samples/sec: 23.21 - lr: 0.100000\n",
      "2022-05-08 16:01:01,416 epoch 33 - iter 370/373 - loss 0.03101159 - samples/sec: 22.13 - lr: 0.100000\n",
      "2022-05-08 16:01:02,254 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:01:02,255 EPOCH 33 done: loss 0.0310 - lr 0.100000\n",
      "2022-05-08 16:01:02,255 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 16:01:04,658 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:01:04,659 train mode resetting embeddings\n",
      "2022-05-08 16:01:04,665 train mode resetting embeddings\n",
      "2022-05-08 16:01:10,996 epoch 34 - iter 37/373 - loss 0.03238711 - samples/sec: 23.40 - lr: 0.100000\n",
      "2022-05-08 16:01:18,247 epoch 34 - iter 74/373 - loss 0.02784608 - samples/sec: 20.60 - lr: 0.100000\n",
      "2022-05-08 16:01:24,559 epoch 34 - iter 111/373 - loss 0.02664162 - samples/sec: 23.71 - lr: 0.100000\n",
      "2022-05-08 16:01:31,161 epoch 34 - iter 148/373 - loss 0.02718232 - samples/sec: 22.66 - lr: 0.100000\n",
      "2022-05-08 16:01:38,066 epoch 34 - iter 185/373 - loss 0.02739457 - samples/sec: 21.66 - lr: 0.100000\n",
      "2022-05-08 16:01:44,051 epoch 34 - iter 222/373 - loss 0.02905784 - samples/sec: 25.01 - lr: 0.100000\n",
      "2022-05-08 16:01:51,620 epoch 34 - iter 259/373 - loss 0.02915094 - samples/sec: 19.73 - lr: 0.100000\n",
      "2022-05-08 16:01:58,810 epoch 34 - iter 296/373 - loss 0.02973207 - samples/sec: 20.79 - lr: 0.100000\n",
      "2022-05-08 16:02:05,369 epoch 34 - iter 333/373 - loss 0.03030010 - samples/sec: 22.80 - lr: 0.100000\n",
      "2022-05-08 16:02:11,812 epoch 34 - iter 370/373 - loss 0.03015869 - samples/sec: 23.25 - lr: 0.100000\n",
      "2022-05-08 16:02:12,466 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:02:12,466 EPOCH 34 done: loss 0.0304 - lr 0.100000\n",
      "2022-05-08 16:02:12,467 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 16:02:14,589 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:02:14,590 train mode resetting embeddings\n",
      "2022-05-08 16:02:14,596 train mode resetting embeddings\n",
      "2022-05-08 16:02:20,703 epoch 35 - iter 37/373 - loss 0.03015777 - samples/sec: 24.27 - lr: 0.100000\n",
      "2022-05-08 16:02:28,081 epoch 35 - iter 74/373 - loss 0.03097129 - samples/sec: 20.28 - lr: 0.100000\n",
      "2022-05-08 16:02:34,561 epoch 35 - iter 111/373 - loss 0.02948876 - samples/sec: 23.09 - lr: 0.100000\n",
      "2022-05-08 16:02:41,688 epoch 35 - iter 148/373 - loss 0.02853912 - samples/sec: 21.00 - lr: 0.100000\n",
      "2022-05-08 16:02:48,064 epoch 35 - iter 185/373 - loss 0.02903609 - samples/sec: 23.46 - lr: 0.100000\n",
      "2022-05-08 16:02:55,020 epoch 35 - iter 222/373 - loss 0.02934855 - samples/sec: 21.49 - lr: 0.100000\n",
      "2022-05-08 16:03:01,447 epoch 35 - iter 259/373 - loss 0.02992293 - samples/sec: 23.27 - lr: 0.100000\n",
      "2022-05-08 16:03:08,672 epoch 35 - iter 296/373 - loss 0.02888716 - samples/sec: 20.70 - lr: 0.100000\n",
      "2022-05-08 16:03:15,026 epoch 35 - iter 333/373 - loss 0.02904293 - samples/sec: 23.56 - lr: 0.100000\n",
      "2022-05-08 16:03:21,866 epoch 35 - iter 370/373 - loss 0.02942149 - samples/sec: 21.85 - lr: 0.100000\n",
      "2022-05-08 16:03:22,949 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:03:22,950 EPOCH 35 done: loss 0.0293 - lr 0.100000\n",
      "2022-05-08 16:03:22,950 BAD EPOCHS (no improvement): 3\n",
      "2022-05-08 16:03:25,317 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:03:25,318 train mode resetting embeddings\n",
      "2022-05-08 16:03:25,325 train mode resetting embeddings\n",
      "2022-05-08 16:03:31,970 epoch 36 - iter 37/373 - loss 0.02885363 - samples/sec: 22.30 - lr: 0.100000\n",
      "2022-05-08 16:03:38,827 epoch 36 - iter 74/373 - loss 0.02822427 - samples/sec: 21.81 - lr: 0.100000\n",
      "2022-05-08 16:03:45,148 epoch 36 - iter 111/373 - loss 0.02826273 - samples/sec: 23.67 - lr: 0.100000\n",
      "2022-05-08 16:03:51,263 epoch 36 - iter 148/373 - loss 0.03036939 - samples/sec: 24.47 - lr: 0.100000\n",
      "2022-05-08 16:03:59,460 epoch 36 - iter 185/373 - loss 0.02804034 - samples/sec: 18.22 - lr: 0.100000\n",
      "2022-05-08 16:04:07,062 epoch 36 - iter 222/373 - loss 0.02834649 - samples/sec: 19.65 - lr: 0.100000\n",
      "2022-05-08 16:04:13,207 epoch 36 - iter 259/373 - loss 0.02685459 - samples/sec: 24.36 - lr: 0.100000\n",
      "2022-05-08 16:04:20,150 epoch 36 - iter 296/373 - loss 0.02824404 - samples/sec: 21.55 - lr: 0.100000\n",
      "2022-05-08 16:04:25,863 epoch 36 - iter 333/373 - loss 0.02804580 - samples/sec: 26.22 - lr: 0.100000\n",
      "2022-05-08 16:04:33,629 epoch 36 - iter 370/373 - loss 0.02805567 - samples/sec: 19.24 - lr: 0.100000\n",
      "2022-05-08 16:04:34,369 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:04:34,370 EPOCH 36 done: loss 0.0279 - lr 0.100000\n",
      "2022-05-08 16:04:34,371 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 16:04:36,479 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:04:36,481 train mode resetting embeddings\n",
      "2022-05-08 16:04:36,488 train mode resetting embeddings\n",
      "2022-05-08 16:04:44,009 epoch 37 - iter 37/373 - loss 0.02761755 - samples/sec: 19.71 - lr: 0.100000\n",
      "2022-05-08 16:04:51,735 epoch 37 - iter 74/373 - loss 0.02576342 - samples/sec: 19.34 - lr: 0.100000\n",
      "2022-05-08 16:04:58,145 epoch 37 - iter 111/373 - loss 0.02662233 - samples/sec: 23.35 - lr: 0.100000\n",
      "2022-05-08 16:05:05,343 epoch 37 - iter 148/373 - loss 0.02586419 - samples/sec: 20.76 - lr: 0.100000\n",
      "2022-05-08 16:05:11,262 epoch 37 - iter 185/373 - loss 0.02622777 - samples/sec: 25.29 - lr: 0.100000\n",
      "2022-05-08 16:05:18,800 epoch 37 - iter 222/373 - loss 0.02701031 - samples/sec: 19.83 - lr: 0.100000\n",
      "2022-05-08 16:05:26,238 epoch 37 - iter 259/373 - loss 0.02720629 - samples/sec: 20.10 - lr: 0.100000\n",
      "2022-05-08 16:05:33,503 epoch 37 - iter 296/373 - loss 0.02689913 - samples/sec: 20.57 - lr: 0.100000\n",
      "2022-05-08 16:05:39,953 epoch 37 - iter 333/373 - loss 0.02671328 - samples/sec: 23.20 - lr: 0.100000\n",
      "2022-05-08 16:05:46,167 epoch 37 - iter 370/373 - loss 0.02647747 - samples/sec: 24.11 - lr: 0.100000\n",
      "2022-05-08 16:05:47,018 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:05:47,020 EPOCH 37 done: loss 0.0265 - lr 0.100000\n",
      "2022-05-08 16:05:47,021 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 16:05:49,104 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:05:49,105 train mode resetting embeddings\n",
      "2022-05-08 16:05:49,111 train mode resetting embeddings\n",
      "2022-05-08 16:05:56,442 epoch 38 - iter 37/373 - loss 0.02694010 - samples/sec: 20.21 - lr: 0.100000\n",
      "2022-05-08 16:06:03,633 epoch 38 - iter 74/373 - loss 0.02554537 - samples/sec: 20.79 - lr: 0.100000\n",
      "2022-05-08 16:06:10,868 epoch 38 - iter 111/373 - loss 0.02624014 - samples/sec: 20.65 - lr: 0.100000\n",
      "2022-05-08 16:06:17,288 epoch 38 - iter 148/373 - loss 0.02681055 - samples/sec: 23.31 - lr: 0.100000\n",
      "2022-05-08 16:06:24,365 epoch 38 - iter 185/373 - loss 0.02543364 - samples/sec: 21.12 - lr: 0.100000\n",
      "2022-05-08 16:06:31,081 epoch 38 - iter 222/373 - loss 0.02603020 - samples/sec: 22.29 - lr: 0.100000\n",
      "2022-05-08 16:06:38,345 epoch 38 - iter 259/373 - loss 0.02593832 - samples/sec: 20.56 - lr: 0.100000\n",
      "2022-05-08 16:06:45,309 epoch 38 - iter 296/373 - loss 0.02590470 - samples/sec: 21.48 - lr: 0.100000\n",
      "2022-05-08 16:06:52,039 epoch 38 - iter 333/373 - loss 0.02567051 - samples/sec: 22.23 - lr: 0.100000\n",
      "2022-05-08 16:06:59,066 epoch 38 - iter 370/373 - loss 0.02570216 - samples/sec: 21.27 - lr: 0.100000\n",
      "2022-05-08 16:06:59,662 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:06:59,663 EPOCH 38 done: loss 0.0257 - lr 0.100000\n",
      "2022-05-08 16:06:59,664 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 16:07:02,102 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:07:02,103 train mode resetting embeddings\n",
      "2022-05-08 16:07:02,108 train mode resetting embeddings\n",
      "2022-05-08 16:07:08,594 epoch 39 - iter 37/373 - loss 0.02525429 - samples/sec: 22.85 - lr: 0.100000\n",
      "2022-05-08 16:07:16,200 epoch 39 - iter 74/373 - loss 0.02497951 - samples/sec: 19.63 - lr: 0.100000\n",
      "2022-05-08 16:07:22,876 epoch 39 - iter 111/373 - loss 0.02605832 - samples/sec: 22.43 - lr: 0.100000\n",
      "2022-05-08 16:07:30,475 epoch 39 - iter 148/373 - loss 0.02509815 - samples/sec: 19.65 - lr: 0.100000\n",
      "2022-05-08 16:07:37,947 epoch 39 - iter 185/373 - loss 0.02429773 - samples/sec: 20.02 - lr: 0.100000\n",
      "2022-05-08 16:07:44,031 epoch 39 - iter 222/373 - loss 0.02399381 - samples/sec: 24.65 - lr: 0.100000\n",
      "2022-05-08 16:07:50,708 epoch 39 - iter 259/373 - loss 0.02436803 - samples/sec: 22.39 - lr: 0.100000\n",
      "2022-05-08 16:07:57,403 epoch 39 - iter 296/373 - loss 0.02424543 - samples/sec: 22.35 - lr: 0.100000\n",
      "2022-05-08 16:08:03,124 epoch 39 - iter 333/373 - loss 0.02437805 - samples/sec: 26.17 - lr: 0.100000\n",
      "2022-05-08 16:08:11,067 epoch 39 - iter 370/373 - loss 0.02492391 - samples/sec: 18.80 - lr: 0.100000\n",
      "2022-05-08 16:08:11,716 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:08:11,717 EPOCH 39 done: loss 0.0248 - lr 0.100000\n",
      "2022-05-08 16:08:11,718 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 16:08:13,849 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:08:13,849 train mode resetting embeddings\n",
      "2022-05-08 16:08:13,854 train mode resetting embeddings\n",
      "2022-05-08 16:08:21,124 epoch 40 - iter 37/373 - loss 0.02703875 - samples/sec: 20.38 - lr: 0.100000\n",
      "2022-05-08 16:08:28,412 epoch 40 - iter 74/373 - loss 0.02900786 - samples/sec: 20.50 - lr: 0.100000\n",
      "2022-05-08 16:08:35,583 epoch 40 - iter 111/373 - loss 0.02720485 - samples/sec: 20.84 - lr: 0.100000\n",
      "2022-05-08 16:08:42,965 epoch 40 - iter 148/373 - loss 0.02710839 - samples/sec: 20.24 - lr: 0.100000\n",
      "2022-05-08 16:08:49,168 epoch 40 - iter 185/373 - loss 0.02788851 - samples/sec: 24.16 - lr: 0.100000\n",
      "2022-05-08 16:08:57,008 epoch 40 - iter 222/373 - loss 0.02763151 - samples/sec: 19.05 - lr: 0.100000\n",
      "2022-05-08 16:09:03,442 epoch 40 - iter 259/373 - loss 0.02774057 - samples/sec: 23.26 - lr: 0.100000\n",
      "2022-05-08 16:09:10,422 epoch 40 - iter 296/373 - loss 0.02695038 - samples/sec: 21.43 - lr: 0.100000\n",
      "2022-05-08 16:09:17,682 epoch 40 - iter 333/373 - loss 0.02711055 - samples/sec: 20.61 - lr: 0.100000\n",
      "2022-05-08 16:09:24,124 epoch 40 - iter 370/373 - loss 0.02639799 - samples/sec: 23.22 - lr: 0.100000\n",
      "2022-05-08 16:09:25,024 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:09:25,025 EPOCH 40 done: loss 0.0263 - lr 0.100000\n",
      "2022-05-08 16:09:25,026 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 16:09:27,450 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:09:27,451 train mode resetting embeddings\n",
      "2022-05-08 16:09:27,457 train mode resetting embeddings\n",
      "2022-05-08 16:09:33,968 epoch 41 - iter 37/373 - loss 0.02275372 - samples/sec: 22.76 - lr: 0.100000\n",
      "2022-05-08 16:09:41,077 epoch 41 - iter 74/373 - loss 0.02344604 - samples/sec: 21.03 - lr: 0.100000\n",
      "2022-05-08 16:09:48,011 epoch 41 - iter 111/373 - loss 0.02257126 - samples/sec: 21.60 - lr: 0.100000\n",
      "2022-05-08 16:09:55,085 epoch 41 - iter 148/373 - loss 0.02428221 - samples/sec: 21.13 - lr: 0.100000\n",
      "2022-05-08 16:10:02,716 epoch 41 - iter 185/373 - loss 0.02389946 - samples/sec: 19.57 - lr: 0.100000\n",
      "2022-05-08 16:10:09,910 epoch 41 - iter 222/373 - loss 0.02352419 - samples/sec: 20.78 - lr: 0.100000\n",
      "2022-05-08 16:10:16,579 epoch 41 - iter 259/373 - loss 0.02354026 - samples/sec: 22.42 - lr: 0.100000\n",
      "2022-05-08 16:10:23,326 epoch 41 - iter 296/373 - loss 0.02324843 - samples/sec: 22.19 - lr: 0.100000\n",
      "2022-05-08 16:10:30,648 epoch 41 - iter 333/373 - loss 0.02315987 - samples/sec: 20.41 - lr: 0.100000\n",
      "2022-05-08 16:10:37,971 epoch 41 - iter 370/373 - loss 0.02355804 - samples/sec: 20.43 - lr: 0.100000\n",
      "2022-05-08 16:10:38,749 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:10:38,750 EPOCH 41 done: loss 0.0236 - lr 0.100000\n",
      "2022-05-08 16:10:38,751 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 16:10:40,839 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:10:40,841 train mode resetting embeddings\n",
      "2022-05-08 16:10:40,846 train mode resetting embeddings\n",
      "2022-05-08 16:10:47,775 epoch 42 - iter 37/373 - loss 0.02545369 - samples/sec: 21.38 - lr: 0.100000\n",
      "2022-05-08 16:10:54,744 epoch 42 - iter 74/373 - loss 0.02426028 - samples/sec: 21.45 - lr: 0.100000\n",
      "2022-05-08 16:11:00,817 epoch 42 - iter 111/373 - loss 0.02596316 - samples/sec: 24.65 - lr: 0.100000\n",
      "2022-05-08 16:11:08,451 epoch 42 - iter 148/373 - loss 0.02513127 - samples/sec: 19.63 - lr: 0.100000\n",
      "2022-05-08 16:11:15,745 epoch 42 - iter 185/373 - loss 0.02560282 - samples/sec: 20.48 - lr: 0.100000\n",
      "2022-05-08 16:11:23,286 epoch 42 - iter 222/373 - loss 0.02526125 - samples/sec: 19.80 - lr: 0.100000\n",
      "2022-05-08 16:11:29,531 epoch 42 - iter 259/373 - loss 0.02505177 - samples/sec: 23.98 - lr: 0.100000\n",
      "2022-05-08 16:11:36,759 epoch 42 - iter 296/373 - loss 0.02473817 - samples/sec: 20.68 - lr: 0.100000\n",
      "2022-05-08 16:11:43,232 epoch 42 - iter 333/373 - loss 0.02452450 - samples/sec: 23.12 - lr: 0.100000\n",
      "2022-05-08 16:11:50,708 epoch 42 - iter 370/373 - loss 0.02423472 - samples/sec: 19.97 - lr: 0.100000\n",
      "2022-05-08 16:11:51,077 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:11:51,078 EPOCH 42 done: loss 0.0242 - lr 0.100000\n",
      "2022-05-08 16:11:51,079 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 16:11:53,470 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:11:53,471 train mode resetting embeddings\n",
      "2022-05-08 16:11:53,477 train mode resetting embeddings\n",
      "2022-05-08 16:12:00,655 epoch 43 - iter 37/373 - loss 0.02649111 - samples/sec: 20.64 - lr: 0.100000\n",
      "2022-05-08 16:12:07,496 epoch 43 - iter 74/373 - loss 0.02277961 - samples/sec: 21.88 - lr: 0.100000\n",
      "2022-05-08 16:12:14,577 epoch 43 - iter 111/373 - loss 0.02220025 - samples/sec: 21.12 - lr: 0.100000\n",
      "2022-05-08 16:12:20,993 epoch 43 - iter 148/373 - loss 0.02217840 - samples/sec: 23.31 - lr: 0.100000\n",
      "2022-05-08 16:12:27,287 epoch 43 - iter 185/373 - loss 0.02180876 - samples/sec: 23.79 - lr: 0.100000\n",
      "2022-05-08 16:12:33,658 epoch 43 - iter 222/373 - loss 0.02263569 - samples/sec: 23.47 - lr: 0.100000\n",
      "2022-05-08 16:12:41,049 epoch 43 - iter 259/373 - loss 0.02310412 - samples/sec: 20.21 - lr: 0.100000\n",
      "2022-05-08 16:12:49,704 epoch 43 - iter 296/373 - loss 0.02473797 - samples/sec: 17.23 - lr: 0.100000\n",
      "2022-05-08 16:12:55,867 epoch 43 - iter 333/373 - loss 0.02445319 - samples/sec: 24.29 - lr: 0.100000\n",
      "2022-05-08 16:13:02,571 epoch 43 - iter 370/373 - loss 0.02402229 - samples/sec: 22.31 - lr: 0.100000\n",
      "2022-05-08 16:13:03,306 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:13:03,307 EPOCH 43 done: loss 0.0240 - lr 0.100000\n",
      "2022-05-08 16:13:03,308 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 16:13:05,347 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:13:05,348 train mode resetting embeddings\n",
      "2022-05-08 16:13:05,354 train mode resetting embeddings\n",
      "2022-05-08 16:13:12,541 epoch 44 - iter 37/373 - loss 0.02727925 - samples/sec: 20.62 - lr: 0.100000\n",
      "2022-05-08 16:13:19,622 epoch 44 - iter 74/373 - loss 0.02223033 - samples/sec: 21.11 - lr: 0.100000\n",
      "2022-05-08 16:13:26,150 epoch 44 - iter 111/373 - loss 0.02198567 - samples/sec: 22.92 - lr: 0.100000\n",
      "2022-05-08 16:13:33,244 epoch 44 - iter 148/373 - loss 0.02247190 - samples/sec: 21.07 - lr: 0.100000\n",
      "2022-05-08 16:13:39,715 epoch 44 - iter 185/373 - loss 0.02367425 - samples/sec: 23.12 - lr: 0.100000\n",
      "2022-05-08 16:13:46,691 epoch 44 - iter 222/373 - loss 0.02244522 - samples/sec: 21.42 - lr: 0.100000\n",
      "2022-05-08 16:13:53,503 epoch 44 - iter 259/373 - loss 0.02261580 - samples/sec: 21.94 - lr: 0.100000\n",
      "2022-05-08 16:14:00,653 epoch 44 - iter 296/373 - loss 0.02422939 - samples/sec: 20.90 - lr: 0.100000\n",
      "2022-05-08 16:14:08,308 epoch 44 - iter 333/373 - loss 0.02416443 - samples/sec: 19.53 - lr: 0.100000\n",
      "2022-05-08 16:14:14,199 epoch 44 - iter 370/373 - loss 0.02421893 - samples/sec: 25.45 - lr: 0.100000\n",
      "2022-05-08 16:14:14,807 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:14:14,808 EPOCH 44 done: loss 0.0244 - lr 0.100000\n",
      "2022-05-08 16:14:14,809 BAD EPOCHS (no improvement): 3\n",
      "2022-05-08 16:14:17,171 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:14:17,172 train mode resetting embeddings\n",
      "2022-05-08 16:14:17,177 train mode resetting embeddings\n",
      "2022-05-08 16:14:24,082 epoch 45 - iter 37/373 - loss 0.01378211 - samples/sec: 21.46 - lr: 0.100000\n",
      "2022-05-08 16:14:30,244 epoch 45 - iter 74/373 - loss 0.01848216 - samples/sec: 24.29 - lr: 0.100000\n",
      "2022-05-08 16:14:37,161 epoch 45 - iter 111/373 - loss 0.01833688 - samples/sec: 21.63 - lr: 0.100000\n",
      "2022-05-08 16:14:44,155 epoch 45 - iter 148/373 - loss 0.01967185 - samples/sec: 21.38 - lr: 0.100000\n",
      "2022-05-08 16:14:51,724 epoch 45 - iter 185/373 - loss 0.02104258 - samples/sec: 19.74 - lr: 0.100000\n",
      "2022-05-08 16:14:58,315 epoch 45 - iter 222/373 - loss 0.02035347 - samples/sec: 22.68 - lr: 0.100000\n",
      "2022-05-08 16:15:05,319 epoch 45 - iter 259/373 - loss 0.02006339 - samples/sec: 21.36 - lr: 0.100000\n",
      "2022-05-08 16:15:12,063 epoch 45 - iter 296/373 - loss 0.02103246 - samples/sec: 22.18 - lr: 0.100000\n",
      "2022-05-08 16:15:19,504 epoch 45 - iter 333/373 - loss 0.02120147 - samples/sec: 20.08 - lr: 0.100000\n",
      "2022-05-08 16:15:26,250 epoch 45 - iter 370/373 - loss 0.02148267 - samples/sec: 22.18 - lr: 0.100000\n",
      "2022-05-08 16:15:26,770 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:15:26,770 EPOCH 45 done: loss 0.0214 - lr 0.100000\n",
      "2022-05-08 16:15:26,771 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 16:15:28,872 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:15:28,873 train mode resetting embeddings\n",
      "2022-05-08 16:15:28,880 train mode resetting embeddings\n",
      "2022-05-08 16:15:35,928 epoch 46 - iter 37/373 - loss 0.02409818 - samples/sec: 21.02 - lr: 0.100000\n",
      "2022-05-08 16:15:43,071 epoch 46 - iter 74/373 - loss 0.02303823 - samples/sec: 20.94 - lr: 0.100000\n",
      "2022-05-08 16:15:50,059 epoch 46 - iter 111/373 - loss 0.02283919 - samples/sec: 21.39 - lr: 0.100000\n",
      "2022-05-08 16:15:56,915 epoch 46 - iter 148/373 - loss 0.02216527 - samples/sec: 21.80 - lr: 0.100000\n",
      "2022-05-08 16:16:04,343 epoch 46 - iter 185/373 - loss 0.02333967 - samples/sec: 20.11 - lr: 0.100000\n",
      "2022-05-08 16:16:10,982 epoch 46 - iter 222/373 - loss 0.02242650 - samples/sec: 22.52 - lr: 0.100000\n",
      "2022-05-08 16:16:17,297 epoch 46 - iter 259/373 - loss 0.02281588 - samples/sec: 23.69 - lr: 0.100000\n",
      "2022-05-08 16:16:24,196 epoch 46 - iter 296/373 - loss 0.02282458 - samples/sec: 21.68 - lr: 0.100000\n",
      "2022-05-08 16:16:31,607 epoch 46 - iter 333/373 - loss 0.02307009 - samples/sec: 20.17 - lr: 0.100000\n",
      "2022-05-08 16:16:38,199 epoch 46 - iter 370/373 - loss 0.02364933 - samples/sec: 22.69 - lr: 0.100000\n",
      "2022-05-08 16:16:38,640 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:16:38,641 EPOCH 46 done: loss 0.0236 - lr 0.100000\n",
      "2022-05-08 16:16:38,642 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 16:16:41,000 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:16:41,001 train mode resetting embeddings\n",
      "2022-05-08 16:16:41,007 train mode resetting embeddings\n",
      "2022-05-08 16:16:47,152 epoch 47 - iter 37/373 - loss 0.01787527 - samples/sec: 24.12 - lr: 0.100000\n",
      "2022-05-08 16:16:53,267 epoch 47 - iter 74/373 - loss 0.01729327 - samples/sec: 24.48 - lr: 0.100000\n",
      "2022-05-08 16:17:01,102 epoch 47 - iter 111/373 - loss 0.01811435 - samples/sec: 19.06 - lr: 0.100000\n",
      "2022-05-08 16:17:07,600 epoch 47 - iter 148/373 - loss 0.01991285 - samples/sec: 23.04 - lr: 0.100000\n",
      "2022-05-08 16:17:15,405 epoch 47 - iter 185/373 - loss 0.02278693 - samples/sec: 19.15 - lr: 0.100000\n",
      "2022-05-08 16:17:21,827 epoch 47 - iter 222/373 - loss 0.02251582 - samples/sec: 23.29 - lr: 0.100000\n",
      "2022-05-08 16:17:29,427 epoch 47 - iter 259/373 - loss 0.02245861 - samples/sec: 19.65 - lr: 0.100000\n",
      "2022-05-08 16:17:37,290 epoch 47 - iter 296/373 - loss 0.02168032 - samples/sec: 18.98 - lr: 0.100000\n",
      "2022-05-08 16:17:44,484 epoch 47 - iter 333/373 - loss 0.02232005 - samples/sec: 20.80 - lr: 0.100000\n",
      "2022-05-08 16:17:51,391 epoch 47 - iter 370/373 - loss 0.02236297 - samples/sec: 21.66 - lr: 0.100000\n",
      "2022-05-08 16:17:51,958 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:17:51,959 EPOCH 47 done: loss 0.0224 - lr 0.100000\n",
      "2022-05-08 16:17:51,961 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 16:17:54,186 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:17:54,187 train mode resetting embeddings\n",
      "2022-05-08 16:17:54,194 train mode resetting embeddings\n",
      "2022-05-08 16:18:01,045 epoch 48 - iter 37/373 - loss 0.01926585 - samples/sec: 21.63 - lr: 0.100000\n",
      "2022-05-08 16:18:07,987 epoch 48 - iter 74/373 - loss 0.01888637 - samples/sec: 21.54 - lr: 0.100000\n",
      "2022-05-08 16:18:15,220 epoch 48 - iter 111/373 - loss 0.02041595 - samples/sec: 20.69 - lr: 0.100000\n",
      "2022-05-08 16:18:22,002 epoch 48 - iter 148/373 - loss 0.02107780 - samples/sec: 22.05 - lr: 0.100000\n",
      "2022-05-08 16:18:29,041 epoch 48 - iter 185/373 - loss 0.02177646 - samples/sec: 21.24 - lr: 0.100000\n",
      "2022-05-08 16:18:36,165 epoch 48 - iter 222/373 - loss 0.02111868 - samples/sec: 20.98 - lr: 0.100000\n",
      "2022-05-08 16:18:43,191 epoch 48 - iter 259/373 - loss 0.02136908 - samples/sec: 21.27 - lr: 0.100000\n",
      "2022-05-08 16:18:50,299 epoch 48 - iter 296/373 - loss 0.02072604 - samples/sec: 21.05 - lr: 0.100000\n",
      "2022-05-08 16:18:57,592 epoch 48 - iter 333/373 - loss 0.02049484 - samples/sec: 20.50 - lr: 0.100000\n",
      "2022-05-08 16:19:04,230 epoch 48 - iter 370/373 - loss 0.02066734 - samples/sec: 22.53 - lr: 0.100000\n",
      "2022-05-08 16:19:04,860 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:19:04,861 EPOCH 48 done: loss 0.0207 - lr 0.100000\n",
      "2022-05-08 16:19:04,862 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 16:19:07,241 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:19:07,241 train mode resetting embeddings\n",
      "2022-05-08 16:19:07,246 train mode resetting embeddings\n",
      "2022-05-08 16:19:14,106 epoch 49 - iter 37/373 - loss 0.01913576 - samples/sec: 21.60 - lr: 0.100000\n",
      "2022-05-08 16:19:20,562 epoch 49 - iter 74/373 - loss 0.01860538 - samples/sec: 23.17 - lr: 0.100000\n",
      "2022-05-08 16:19:27,808 epoch 49 - iter 111/373 - loss 0.01918433 - samples/sec: 20.63 - lr: 0.100000\n",
      "2022-05-08 16:19:34,906 epoch 49 - iter 148/373 - loss 0.02025589 - samples/sec: 21.08 - lr: 0.100000\n",
      "2022-05-08 16:19:42,044 epoch 49 - iter 185/373 - loss 0.02070463 - samples/sec: 20.94 - lr: 0.100000\n",
      "2022-05-08 16:19:48,717 epoch 49 - iter 222/373 - loss 0.02133087 - samples/sec: 22.41 - lr: 0.100000\n",
      "2022-05-08 16:19:55,949 epoch 49 - iter 259/373 - loss 0.02196745 - samples/sec: 20.66 - lr: 0.100000\n",
      "2022-05-08 16:20:03,218 epoch 49 - iter 296/373 - loss 0.02183317 - samples/sec: 20.54 - lr: 0.100000\n",
      "2022-05-08 16:20:09,635 epoch 49 - iter 333/373 - loss 0.02159722 - samples/sec: 23.34 - lr: 0.100000\n",
      "2022-05-08 16:20:17,056 epoch 49 - iter 370/373 - loss 0.02149652 - samples/sec: 20.14 - lr: 0.100000\n",
      "2022-05-08 16:20:17,860 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:20:17,861 EPOCH 49 done: loss 0.0214 - lr 0.100000\n",
      "2022-05-08 16:20:17,862 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 16:20:19,970 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:20:19,971 train mode resetting embeddings\n",
      "2022-05-08 16:20:19,977 train mode resetting embeddings\n",
      "2022-05-08 16:20:26,992 epoch 50 - iter 37/373 - loss 0.02022833 - samples/sec: 21.12 - lr: 0.100000\n",
      "2022-05-08 16:20:34,286 epoch 50 - iter 74/373 - loss 0.01930424 - samples/sec: 20.49 - lr: 0.100000\n",
      "2022-05-08 16:20:40,608 epoch 50 - iter 111/373 - loss 0.02009942 - samples/sec: 23.66 - lr: 0.100000\n",
      "2022-05-08 16:20:47,692 epoch 50 - iter 148/373 - loss 0.02072804 - samples/sec: 21.10 - lr: 0.100000\n",
      "2022-05-08 16:20:54,807 epoch 50 - iter 185/373 - loss 0.02187788 - samples/sec: 21.00 - lr: 0.100000\n",
      "2022-05-08 16:21:00,872 epoch 50 - iter 222/373 - loss 0.02223287 - samples/sec: 24.68 - lr: 0.100000\n",
      "2022-05-08 16:21:07,881 epoch 50 - iter 259/373 - loss 0.02195476 - samples/sec: 21.32 - lr: 0.100000\n",
      "2022-05-08 16:21:15,502 epoch 50 - iter 296/373 - loss 0.02140325 - samples/sec: 19.60 - lr: 0.100000\n",
      "2022-05-08 16:21:21,672 epoch 50 - iter 333/373 - loss 0.02079584 - samples/sec: 24.26 - lr: 0.100000\n",
      "2022-05-08 16:21:28,604 epoch 50 - iter 370/373 - loss 0.02068864 - samples/sec: 21.56 - lr: 0.100000\n",
      "2022-05-08 16:21:29,338 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:21:29,339 EPOCH 50 done: loss 0.0208 - lr 0.100000\n",
      "2022-05-08 16:21:29,340 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 16:21:31,715 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:21:31,716 train mode resetting embeddings\n",
      "2022-05-08 16:21:31,721 train mode resetting embeddings\n",
      "2022-05-08 16:21:38,686 epoch 51 - iter 37/373 - loss 0.02360734 - samples/sec: 21.27 - lr: 0.100000\n",
      "2022-05-08 16:21:45,589 epoch 51 - iter 74/373 - loss 0.02400274 - samples/sec: 21.68 - lr: 0.100000\n",
      "2022-05-08 16:21:52,300 epoch 51 - iter 111/373 - loss 0.02251584 - samples/sec: 22.29 - lr: 0.100000\n",
      "2022-05-08 16:21:58,775 epoch 51 - iter 148/373 - loss 0.02295028 - samples/sec: 23.10 - lr: 0.100000\n",
      "2022-05-08 16:22:06,227 epoch 51 - iter 185/373 - loss 0.02138001 - samples/sec: 20.06 - lr: 0.100000\n",
      "2022-05-08 16:22:14,083 epoch 51 - iter 222/373 - loss 0.02109322 - samples/sec: 19.01 - lr: 0.100000\n",
      "2022-05-08 16:22:20,271 epoch 51 - iter 259/373 - loss 0.02066315 - samples/sec: 24.19 - lr: 0.100000\n",
      "2022-05-08 16:22:26,932 epoch 51 - iter 296/373 - loss 0.02042542 - samples/sec: 22.47 - lr: 0.100000\n",
      "2022-05-08 16:22:33,812 epoch 51 - iter 333/373 - loss 0.02086197 - samples/sec: 21.74 - lr: 0.100000\n",
      "2022-05-08 16:22:41,288 epoch 51 - iter 370/373 - loss 0.02074664 - samples/sec: 19.98 - lr: 0.100000\n",
      "2022-05-08 16:22:41,978 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:22:41,980 EPOCH 51 done: loss 0.0207 - lr 0.100000\n",
      "2022-05-08 16:22:41,980 BAD EPOCHS (no improvement): 3\n",
      "2022-05-08 16:22:44,201 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:22:44,203 train mode resetting embeddings\n",
      "2022-05-08 16:22:44,208 train mode resetting embeddings\n",
      "2022-05-08 16:22:51,482 epoch 52 - iter 37/373 - loss 0.02002029 - samples/sec: 20.37 - lr: 0.100000\n",
      "2022-05-08 16:22:58,274 epoch 52 - iter 74/373 - loss 0.02145884 - samples/sec: 22.01 - lr: 0.100000\n",
      "2022-05-08 16:23:05,482 epoch 52 - iter 111/373 - loss 0.02092782 - samples/sec: 20.73 - lr: 0.100000\n",
      "2022-05-08 16:23:12,493 epoch 52 - iter 148/373 - loss 0.01932841 - samples/sec: 21.32 - lr: 0.100000\n",
      "2022-05-08 16:23:18,890 epoch 52 - iter 185/373 - loss 0.01873625 - samples/sec: 23.40 - lr: 0.100000\n",
      "2022-05-08 16:23:25,384 epoch 52 - iter 222/373 - loss 0.01941988 - samples/sec: 23.04 - lr: 0.100000\n",
      "2022-05-08 16:23:32,882 epoch 52 - iter 259/373 - loss 0.02046648 - samples/sec: 19.92 - lr: 0.100000\n",
      "2022-05-08 16:23:39,327 epoch 52 - iter 296/373 - loss 0.02089358 - samples/sec: 23.21 - lr: 0.100000\n",
      "2022-05-08 16:23:46,688 epoch 52 - iter 333/373 - loss 0.02131228 - samples/sec: 20.31 - lr: 0.100000\n",
      "2022-05-08 16:23:52,536 epoch 52 - iter 370/373 - loss 0.02122750 - samples/sec: 25.60 - lr: 0.100000\n",
      "2022-05-08 16:23:53,258 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:23:53,259 EPOCH 52 done: loss 0.0212 - lr 0.100000\n",
      "2022-05-08 16:23:53,260 Epoch    52: reducing learning rate of group 0 to 5.0000e-02.\n",
      "2022-05-08 16:23:53,261 BAD EPOCHS (no improvement): 4\n",
      "2022-05-08 16:23:55,601 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:23:55,601 train mode resetting embeddings\n",
      "2022-05-08 16:23:55,608 train mode resetting embeddings\n",
      "2022-05-08 16:24:01,505 epoch 53 - iter 37/373 - loss 0.02306617 - samples/sec: 25.13 - lr: 0.050000\n",
      "2022-05-08 16:24:08,215 epoch 53 - iter 74/373 - loss 0.01940144 - samples/sec: 22.28 - lr: 0.050000\n",
      "2022-05-08 16:24:15,049 epoch 53 - iter 111/373 - loss 0.01783824 - samples/sec: 21.88 - lr: 0.050000\n",
      "2022-05-08 16:24:21,859 epoch 53 - iter 148/373 - loss 0.01781536 - samples/sec: 21.96 - lr: 0.050000\n",
      "2022-05-08 16:24:28,841 epoch 53 - iter 185/373 - loss 0.01780465 - samples/sec: 21.41 - lr: 0.050000\n",
      "2022-05-08 16:24:35,984 epoch 53 - iter 222/373 - loss 0.01736633 - samples/sec: 20.92 - lr: 0.050000\n",
      "2022-05-08 16:24:42,969 epoch 53 - iter 259/373 - loss 0.01705661 - samples/sec: 21.41 - lr: 0.050000\n",
      "2022-05-08 16:24:50,563 epoch 53 - iter 296/373 - loss 0.01638024 - samples/sec: 19.66 - lr: 0.050000\n",
      "2022-05-08 16:24:57,916 epoch 53 - iter 333/373 - loss 0.01670512 - samples/sec: 20.32 - lr: 0.050000\n",
      "2022-05-08 16:25:04,056 epoch 53 - iter 370/373 - loss 0.01679040 - samples/sec: 24.37 - lr: 0.050000\n",
      "2022-05-08 16:25:04,873 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:25:04,874 EPOCH 53 done: loss 0.0166 - lr 0.050000\n",
      "2022-05-08 16:25:04,875 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 16:25:07,017 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:25:07,018 train mode resetting embeddings\n",
      "2022-05-08 16:25:07,023 train mode resetting embeddings\n",
      "2022-05-08 16:25:13,457 epoch 54 - iter 37/373 - loss 0.01847660 - samples/sec: 23.03 - lr: 0.050000\n",
      "2022-05-08 16:25:20,298 epoch 54 - iter 74/373 - loss 0.01529004 - samples/sec: 21.86 - lr: 0.050000\n",
      "2022-05-08 16:25:27,789 epoch 54 - iter 111/373 - loss 0.01622255 - samples/sec: 19.94 - lr: 0.050000\n",
      "2022-05-08 16:25:34,644 epoch 54 - iter 148/373 - loss 0.01522799 - samples/sec: 21.81 - lr: 0.050000\n",
      "2022-05-08 16:25:41,301 epoch 54 - iter 185/373 - loss 0.01561192 - samples/sec: 22.47 - lr: 0.050000\n",
      "2022-05-08 16:25:48,341 epoch 54 - iter 222/373 - loss 0.01553441 - samples/sec: 21.24 - lr: 0.050000\n",
      "2022-05-08 16:25:55,463 epoch 54 - iter 259/373 - loss 0.01557357 - samples/sec: 20.98 - lr: 0.050000\n",
      "2022-05-08 16:26:01,317 epoch 54 - iter 296/373 - loss 0.01535589 - samples/sec: 25.58 - lr: 0.050000\n",
      "2022-05-08 16:26:08,389 epoch 54 - iter 333/373 - loss 0.01545182 - samples/sec: 21.13 - lr: 0.050000\n",
      "2022-05-08 16:26:15,916 epoch 54 - iter 370/373 - loss 0.01536764 - samples/sec: 19.85 - lr: 0.050000\n",
      "2022-05-08 16:26:16,439 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:26:16,440 EPOCH 54 done: loss 0.0153 - lr 0.050000\n",
      "2022-05-08 16:26:16,441 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 16:26:18,812 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:26:18,812 train mode resetting embeddings\n",
      "2022-05-08 16:26:18,818 train mode resetting embeddings\n",
      "2022-05-08 16:26:26,112 epoch 55 - iter 37/373 - loss 0.01711991 - samples/sec: 20.31 - lr: 0.050000\n",
      "2022-05-08 16:26:32,591 epoch 55 - iter 74/373 - loss 0.01653652 - samples/sec: 23.08 - lr: 0.050000\n",
      "2022-05-08 16:26:39,649 epoch 55 - iter 111/373 - loss 0.01515791 - samples/sec: 21.18 - lr: 0.050000\n",
      "2022-05-08 16:26:46,160 epoch 55 - iter 148/373 - loss 0.01468389 - samples/sec: 22.97 - lr: 0.050000\n",
      "2022-05-08 16:26:53,405 epoch 55 - iter 185/373 - loss 0.01502392 - samples/sec: 20.63 - lr: 0.050000\n",
      "2022-05-08 16:27:00,634 epoch 55 - iter 222/373 - loss 0.01560436 - samples/sec: 20.68 - lr: 0.050000\n",
      "2022-05-08 16:27:07,022 epoch 55 - iter 259/373 - loss 0.01591898 - samples/sec: 23.43 - lr: 0.050000\n",
      "2022-05-08 16:27:13,879 epoch 55 - iter 296/373 - loss 0.01598172 - samples/sec: 21.80 - lr: 0.050000\n",
      "2022-05-08 16:27:21,038 epoch 55 - iter 333/373 - loss 0.01563625 - samples/sec: 20.90 - lr: 0.050000\n",
      "2022-05-08 16:27:27,635 epoch 55 - iter 370/373 - loss 0.01573231 - samples/sec: 22.68 - lr: 0.050000\n",
      "2022-05-08 16:27:28,106 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:27:28,107 EPOCH 55 done: loss 0.0158 - lr 0.050000\n",
      "2022-05-08 16:27:28,108 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 16:27:30,174 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:27:30,175 train mode resetting embeddings\n",
      "2022-05-08 16:27:30,180 train mode resetting embeddings\n",
      "2022-05-08 16:27:37,812 epoch 56 - iter 37/373 - loss 0.01086057 - samples/sec: 19.41 - lr: 0.050000\n",
      "2022-05-08 16:27:44,981 epoch 56 - iter 74/373 - loss 0.01323028 - samples/sec: 20.83 - lr: 0.050000\n",
      "2022-05-08 16:27:52,057 epoch 56 - iter 111/373 - loss 0.01432582 - samples/sec: 21.13 - lr: 0.050000\n",
      "2022-05-08 16:27:59,499 epoch 56 - iter 148/373 - loss 0.01547638 - samples/sec: 20.07 - lr: 0.050000\n",
      "2022-05-08 16:28:05,816 epoch 56 - iter 185/373 - loss 0.01532448 - samples/sec: 23.70 - lr: 0.050000\n",
      "2022-05-08 16:28:12,531 epoch 56 - iter 222/373 - loss 0.01470703 - samples/sec: 22.28 - lr: 0.050000\n",
      "2022-05-08 16:28:18,814 epoch 56 - iter 259/373 - loss 0.01523346 - samples/sec: 23.81 - lr: 0.050000\n",
      "2022-05-08 16:28:25,264 epoch 56 - iter 296/373 - loss 0.01518365 - samples/sec: 23.20 - lr: 0.050000\n",
      "2022-05-08 16:28:32,873 epoch 56 - iter 333/373 - loss 0.01504704 - samples/sec: 19.63 - lr: 0.050000\n",
      "2022-05-08 16:28:40,061 epoch 56 - iter 370/373 - loss 0.01541924 - samples/sec: 20.78 - lr: 0.050000\n",
      "2022-05-08 16:28:40,794 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:28:40,795 EPOCH 56 done: loss 0.0154 - lr 0.050000\n",
      "2022-05-08 16:28:40,796 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 16:28:43,128 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:28:43,129 train mode resetting embeddings\n",
      "2022-05-08 16:28:43,135 train mode resetting embeddings\n",
      "2022-05-08 16:28:51,144 epoch 57 - iter 37/373 - loss 0.01334442 - samples/sec: 18.50 - lr: 0.050000\n",
      "2022-05-08 16:28:58,231 epoch 57 - iter 74/373 - loss 0.01280353 - samples/sec: 21.11 - lr: 0.050000\n",
      "2022-05-08 16:29:05,257 epoch 57 - iter 111/373 - loss 0.01268318 - samples/sec: 21.32 - lr: 0.050000\n",
      "2022-05-08 16:29:11,723 epoch 57 - iter 148/373 - loss 0.01384259 - samples/sec: 23.16 - lr: 0.050000\n",
      "2022-05-08 16:29:18,409 epoch 57 - iter 185/373 - loss 0.01395891 - samples/sec: 22.36 - lr: 0.050000\n",
      "2022-05-08 16:29:25,268 epoch 57 - iter 222/373 - loss 0.01339199 - samples/sec: 21.79 - lr: 0.050000\n",
      "2022-05-08 16:29:31,707 epoch 57 - iter 259/373 - loss 0.01360100 - samples/sec: 23.25 - lr: 0.050000\n",
      "2022-05-08 16:29:39,045 epoch 57 - iter 296/373 - loss 0.01439401 - samples/sec: 20.35 - lr: 0.050000\n",
      "2022-05-08 16:29:45,323 epoch 57 - iter 333/373 - loss 0.01435373 - samples/sec: 23.83 - lr: 0.050000\n",
      "2022-05-08 16:29:52,718 epoch 57 - iter 370/373 - loss 0.01432860 - samples/sec: 20.20 - lr: 0.050000\n",
      "2022-05-08 16:29:53,056 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:29:53,057 EPOCH 57 done: loss 0.0143 - lr 0.050000\n",
      "2022-05-08 16:29:53,057 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 16:29:55,198 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:29:55,198 train mode resetting embeddings\n",
      "2022-05-08 16:29:55,204 train mode resetting embeddings\n",
      "2022-05-08 16:30:02,572 epoch 58 - iter 37/373 - loss 0.01137270 - samples/sec: 20.11 - lr: 0.050000\n",
      "2022-05-08 16:30:10,327 epoch 58 - iter 74/373 - loss 0.01156099 - samples/sec: 19.25 - lr: 0.050000\n",
      "2022-05-08 16:30:17,707 epoch 58 - iter 111/373 - loss 0.01279361 - samples/sec: 20.25 - lr: 0.050000\n",
      "2022-05-08 16:30:24,656 epoch 58 - iter 148/373 - loss 0.01315029 - samples/sec: 21.51 - lr: 0.050000\n",
      "2022-05-08 16:30:31,887 epoch 58 - iter 185/373 - loss 0.01375324 - samples/sec: 20.65 - lr: 0.050000\n",
      "2022-05-08 16:30:38,444 epoch 58 - iter 222/373 - loss 0.01355360 - samples/sec: 22.86 - lr: 0.050000\n",
      "2022-05-08 16:30:45,925 epoch 58 - iter 259/373 - loss 0.01398480 - samples/sec: 19.96 - lr: 0.050000\n",
      "2022-05-08 16:30:51,785 epoch 58 - iter 296/373 - loss 0.01407674 - samples/sec: 25.55 - lr: 0.050000\n",
      "2022-05-08 16:30:59,378 epoch 58 - iter 333/373 - loss 0.01399571 - samples/sec: 19.67 - lr: 0.050000\n",
      "2022-05-08 16:31:05,118 epoch 58 - iter 370/373 - loss 0.01417546 - samples/sec: 26.14 - lr: 0.050000\n",
      "2022-05-08 16:31:05,776 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:31:05,777 EPOCH 58 done: loss 0.0142 - lr 0.050000\n",
      "2022-05-08 16:31:05,778 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 16:31:08,142 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:31:08,143 train mode resetting embeddings\n",
      "2022-05-08 16:31:08,148 train mode resetting embeddings\n",
      "2022-05-08 16:31:15,926 epoch 59 - iter 37/373 - loss 0.01449043 - samples/sec: 19.05 - lr: 0.050000\n",
      "2022-05-08 16:31:21,879 epoch 59 - iter 74/373 - loss 0.01437816 - samples/sec: 25.15 - lr: 0.050000\n",
      "2022-05-08 16:31:28,358 epoch 59 - iter 111/373 - loss 0.01440838 - samples/sec: 23.09 - lr: 0.050000\n",
      "2022-05-08 16:31:35,147 epoch 59 - iter 148/373 - loss 0.01387497 - samples/sec: 22.02 - lr: 0.050000\n",
      "2022-05-08 16:31:42,250 epoch 59 - iter 185/373 - loss 0.01389330 - samples/sec: 21.03 - lr: 0.050000\n",
      "2022-05-08 16:31:49,449 epoch 59 - iter 222/373 - loss 0.01404218 - samples/sec: 20.76 - lr: 0.050000\n",
      "2022-05-08 16:31:56,604 epoch 59 - iter 259/373 - loss 0.01430994 - samples/sec: 20.90 - lr: 0.050000\n",
      "2022-05-08 16:32:03,719 epoch 59 - iter 296/373 - loss 0.01480059 - samples/sec: 21.01 - lr: 0.050000\n",
      "2022-05-08 16:32:10,680 epoch 59 - iter 333/373 - loss 0.01486587 - samples/sec: 21.48 - lr: 0.050000\n",
      "2022-05-08 16:32:16,889 epoch 59 - iter 370/373 - loss 0.01472722 - samples/sec: 24.10 - lr: 0.050000\n",
      "2022-05-08 16:32:17,511 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:32:17,512 EPOCH 59 done: loss 0.0147 - lr 0.050000\n",
      "2022-05-08 16:32:17,513 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 16:32:19,630 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:32:19,631 train mode resetting embeddings\n",
      "2022-05-08 16:32:19,637 train mode resetting embeddings\n",
      "2022-05-08 16:32:26,502 epoch 60 - iter 37/373 - loss 0.01259735 - samples/sec: 21.59 - lr: 0.050000\n",
      "2022-05-08 16:32:33,230 epoch 60 - iter 74/373 - loss 0.01218306 - samples/sec: 22.23 - lr: 0.050000\n",
      "2022-05-08 16:32:40,308 epoch 60 - iter 111/373 - loss 0.01199654 - samples/sec: 21.12 - lr: 0.050000\n",
      "2022-05-08 16:32:47,722 epoch 60 - iter 148/373 - loss 0.01266661 - samples/sec: 20.15 - lr: 0.050000\n",
      "2022-05-08 16:32:55,009 epoch 60 - iter 185/373 - loss 0.01213658 - samples/sec: 20.51 - lr: 0.050000\n",
      "2022-05-08 16:33:02,038 epoch 60 - iter 222/373 - loss 0.01184645 - samples/sec: 21.27 - lr: 0.050000\n",
      "2022-05-08 16:33:08,394 epoch 60 - iter 259/373 - loss 0.01188284 - samples/sec: 23.54 - lr: 0.050000\n",
      "2022-05-08 16:33:14,780 epoch 60 - iter 296/373 - loss 0.01225288 - samples/sec: 23.43 - lr: 0.050000\n",
      "2022-05-08 16:33:21,157 epoch 60 - iter 333/373 - loss 0.01263158 - samples/sec: 23.46 - lr: 0.050000\n",
      "2022-05-08 16:33:28,181 epoch 60 - iter 370/373 - loss 0.01297915 - samples/sec: 21.27 - lr: 0.050000\n",
      "2022-05-08 16:33:28,695 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:33:28,696 EPOCH 60 done: loss 0.0130 - lr 0.050000\n",
      "2022-05-08 16:33:28,697 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 16:33:31,000 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:33:31,001 train mode resetting embeddings\n",
      "2022-05-08 16:33:31,007 train mode resetting embeddings\n",
      "2022-05-08 16:33:36,935 epoch 61 - iter 37/373 - loss 0.00905528 - samples/sec: 25.00 - lr: 0.050000\n",
      "2022-05-08 16:33:43,359 epoch 61 - iter 74/373 - loss 0.01080647 - samples/sec: 23.28 - lr: 0.050000\n",
      "2022-05-08 16:33:50,477 epoch 61 - iter 111/373 - loss 0.01172095 - samples/sec: 21.00 - lr: 0.050000\n",
      "2022-05-08 16:33:57,979 epoch 61 - iter 148/373 - loss 0.01328973 - samples/sec: 19.92 - lr: 0.050000\n",
      "2022-05-08 16:34:04,500 epoch 61 - iter 185/373 - loss 0.01333881 - samples/sec: 22.94 - lr: 0.050000\n",
      "2022-05-08 16:34:11,535 epoch 61 - iter 222/373 - loss 0.01264379 - samples/sec: 21.26 - lr: 0.050000\n",
      "2022-05-08 16:34:19,077 epoch 61 - iter 259/373 - loss 0.01286255 - samples/sec: 19.80 - lr: 0.050000\n",
      "2022-05-08 16:34:25,737 epoch 61 - iter 296/373 - loss 0.01257438 - samples/sec: 22.49 - lr: 0.050000\n",
      "2022-05-08 16:34:32,939 epoch 61 - iter 333/373 - loss 0.01257281 - samples/sec: 20.74 - lr: 0.050000\n",
      "2022-05-08 16:34:40,131 epoch 61 - iter 370/373 - loss 0.01244235 - samples/sec: 20.80 - lr: 0.050000\n",
      "2022-05-08 16:34:40,941 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:34:40,942 EPOCH 61 done: loss 0.0127 - lr 0.050000\n",
      "2022-05-08 16:34:40,943 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 16:34:43,108 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:34:43,109 train mode resetting embeddings\n",
      "2022-05-08 16:34:43,116 train mode resetting embeddings\n",
      "2022-05-08 16:34:49,327 epoch 62 - iter 37/373 - loss 0.00848651 - samples/sec: 23.86 - lr: 0.050000\n",
      "2022-05-08 16:34:56,930 epoch 62 - iter 74/373 - loss 0.01073635 - samples/sec: 19.64 - lr: 0.050000\n",
      "2022-05-08 16:35:04,102 epoch 62 - iter 111/373 - loss 0.01243561 - samples/sec: 20.83 - lr: 0.050000\n",
      "2022-05-08 16:35:10,324 epoch 62 - iter 148/373 - loss 0.01316981 - samples/sec: 24.05 - lr: 0.050000\n",
      "2022-05-08 16:35:17,270 epoch 62 - iter 185/373 - loss 0.01310829 - samples/sec: 21.53 - lr: 0.050000\n",
      "2022-05-08 16:35:23,990 epoch 62 - iter 222/373 - loss 0.01285759 - samples/sec: 22.26 - lr: 0.050000\n",
      "2022-05-08 16:35:30,917 epoch 62 - iter 259/373 - loss 0.01249166 - samples/sec: 21.59 - lr: 0.050000\n",
      "2022-05-08 16:35:38,250 epoch 62 - iter 296/373 - loss 0.01286477 - samples/sec: 20.37 - lr: 0.050000\n",
      "2022-05-08 16:35:45,642 epoch 62 - iter 333/373 - loss 0.01319775 - samples/sec: 20.24 - lr: 0.050000\n",
      "2022-05-08 16:35:52,493 epoch 62 - iter 370/373 - loss 0.01353806 - samples/sec: 21.83 - lr: 0.050000\n",
      "2022-05-08 16:35:52,847 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:35:52,848 EPOCH 62 done: loss 0.0135 - lr 0.050000\n",
      "2022-05-08 16:35:52,848 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 16:35:55,173 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:35:55,174 train mode resetting embeddings\n",
      "2022-05-08 16:35:55,179 train mode resetting embeddings\n",
      "2022-05-08 16:36:01,420 epoch 63 - iter 37/373 - loss 0.01406191 - samples/sec: 23.75 - lr: 0.050000\n",
      "2022-05-08 16:36:08,586 epoch 63 - iter 74/373 - loss 0.01277550 - samples/sec: 20.84 - lr: 0.050000\n",
      "2022-05-08 16:36:15,485 epoch 63 - iter 111/373 - loss 0.01539391 - samples/sec: 21.67 - lr: 0.050000\n",
      "2022-05-08 16:36:22,117 epoch 63 - iter 148/373 - loss 0.01433209 - samples/sec: 22.59 - lr: 0.050000\n",
      "2022-05-08 16:36:29,050 epoch 63 - iter 185/373 - loss 0.01398831 - samples/sec: 21.56 - lr: 0.050000\n",
      "2022-05-08 16:36:36,433 epoch 63 - iter 222/373 - loss 0.01395500 - samples/sec: 20.23 - lr: 0.050000\n",
      "2022-05-08 16:36:43,045 epoch 63 - iter 259/373 - loss 0.01424069 - samples/sec: 22.61 - lr: 0.050000\n",
      "2022-05-08 16:36:49,889 epoch 63 - iter 296/373 - loss 0.01509489 - samples/sec: 21.85 - lr: 0.050000\n",
      "2022-05-08 16:36:56,239 epoch 63 - iter 333/373 - loss 0.01489466 - samples/sec: 23.56 - lr: 0.050000\n",
      "2022-05-08 16:37:04,073 epoch 63 - iter 370/373 - loss 0.01483056 - samples/sec: 19.09 - lr: 0.050000\n",
      "2022-05-08 16:37:04,576 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:37:04,577 EPOCH 63 done: loss 0.0148 - lr 0.050000\n",
      "2022-05-08 16:37:04,577 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 16:37:06,715 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:37:06,716 train mode resetting embeddings\n",
      "2022-05-08 16:37:06,723 train mode resetting embeddings\n",
      "2022-05-08 16:37:14,176 epoch 64 - iter 37/373 - loss 0.01057068 - samples/sec: 19.89 - lr: 0.050000\n",
      "2022-05-08 16:37:20,742 epoch 64 - iter 74/373 - loss 0.01231530 - samples/sec: 22.80 - lr: 0.050000\n",
      "2022-05-08 16:37:27,787 epoch 64 - iter 111/373 - loss 0.01294944 - samples/sec: 21.22 - lr: 0.050000\n",
      "2022-05-08 16:37:34,477 epoch 64 - iter 148/373 - loss 0.01299290 - samples/sec: 22.34 - lr: 0.050000\n",
      "2022-05-08 16:37:41,438 epoch 64 - iter 185/373 - loss 0.01247200 - samples/sec: 21.46 - lr: 0.050000\n",
      "2022-05-08 16:37:47,797 epoch 64 - iter 222/373 - loss 0.01304260 - samples/sec: 23.53 - lr: 0.050000\n",
      "2022-05-08 16:37:53,707 epoch 64 - iter 259/373 - loss 0.01298154 - samples/sec: 25.33 - lr: 0.050000\n",
      "2022-05-08 16:38:00,942 epoch 64 - iter 296/373 - loss 0.01226546 - samples/sec: 20.65 - lr: 0.050000\n",
      "2022-05-08 16:38:07,821 epoch 64 - iter 333/373 - loss 0.01250491 - samples/sec: 21.77 - lr: 0.050000\n",
      "2022-05-08 16:38:15,045 epoch 64 - iter 370/373 - loss 0.01244804 - samples/sec: 20.69 - lr: 0.050000\n",
      "2022-05-08 16:38:15,655 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:38:15,656 EPOCH 64 done: loss 0.0124 - lr 0.050000\n",
      "2022-05-08 16:38:15,656 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 16:38:17,753 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:38:17,754 train mode resetting embeddings\n",
      "2022-05-08 16:38:17,760 train mode resetting embeddings\n",
      "2022-05-08 16:38:26,067 epoch 65 - iter 37/373 - loss 0.01217541 - samples/sec: 17.83 - lr: 0.050000\n",
      "2022-05-08 16:38:32,465 epoch 65 - iter 74/373 - loss 0.01129473 - samples/sec: 23.40 - lr: 0.050000\n",
      "2022-05-08 16:38:39,631 epoch 65 - iter 111/373 - loss 0.01081935 - samples/sec: 20.86 - lr: 0.050000\n",
      "2022-05-08 16:38:46,204 epoch 65 - iter 148/373 - loss 0.01153057 - samples/sec: 22.76 - lr: 0.050000\n",
      "2022-05-08 16:38:53,033 epoch 65 - iter 185/373 - loss 0.01182090 - samples/sec: 21.90 - lr: 0.050000\n",
      "2022-05-08 16:38:59,318 epoch 65 - iter 222/373 - loss 0.01148491 - samples/sec: 23.81 - lr: 0.050000\n",
      "2022-05-08 16:39:06,246 epoch 65 - iter 259/373 - loss 0.01157615 - samples/sec: 21.58 - lr: 0.050000\n",
      "2022-05-08 16:39:13,248 epoch 65 - iter 296/373 - loss 0.01165009 - samples/sec: 21.36 - lr: 0.050000\n",
      "2022-05-08 16:39:18,892 epoch 65 - iter 333/373 - loss 0.01153933 - samples/sec: 26.56 - lr: 0.050000\n",
      "2022-05-08 16:39:25,875 epoch 65 - iter 370/373 - loss 0.01168995 - samples/sec: 21.42 - lr: 0.050000\n",
      "2022-05-08 16:39:26,630 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:39:26,631 EPOCH 65 done: loss 0.0116 - lr 0.050000\n",
      "2022-05-08 16:39:26,632 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 16:39:28,923 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:39:28,924 train mode resetting embeddings\n",
      "2022-05-08 16:39:28,931 train mode resetting embeddings\n",
      "2022-05-08 16:39:35,677 epoch 66 - iter 37/373 - loss 0.01151847 - samples/sec: 21.97 - lr: 0.050000\n",
      "2022-05-08 16:39:42,241 epoch 66 - iter 74/373 - loss 0.01367644 - samples/sec: 22.79 - lr: 0.050000\n",
      "2022-05-08 16:39:49,570 epoch 66 - iter 111/373 - loss 0.01389321 - samples/sec: 20.39 - lr: 0.050000\n",
      "2022-05-08 16:39:57,127 epoch 66 - iter 148/373 - loss 0.01271391 - samples/sec: 19.77 - lr: 0.050000\n",
      "2022-05-08 16:40:03,889 epoch 66 - iter 185/373 - loss 0.01355489 - samples/sec: 22.18 - lr: 0.050000\n",
      "2022-05-08 16:40:10,866 epoch 66 - iter 222/373 - loss 0.01327109 - samples/sec: 21.42 - lr: 0.050000\n",
      "2022-05-08 16:40:17,479 epoch 66 - iter 259/373 - loss 0.01312268 - samples/sec: 22.61 - lr: 0.050000\n",
      "2022-05-08 16:40:24,231 epoch 66 - iter 296/373 - loss 0.01316170 - samples/sec: 22.14 - lr: 0.050000\n",
      "2022-05-08 16:40:31,357 epoch 66 - iter 333/373 - loss 0.01305124 - samples/sec: 20.98 - lr: 0.050000\n",
      "2022-05-08 16:40:38,653 epoch 66 - iter 370/373 - loss 0.01273970 - samples/sec: 20.48 - lr: 0.050000\n",
      "2022-05-08 16:40:39,029 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:40:39,031 EPOCH 66 done: loss 0.0128 - lr 0.050000\n",
      "2022-05-08 16:40:39,032 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 16:40:41,116 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:40:41,117 train mode resetting embeddings\n",
      "2022-05-08 16:40:41,124 train mode resetting embeddings\n",
      "2022-05-08 16:40:48,508 epoch 67 - iter 37/373 - loss 0.00881118 - samples/sec: 20.07 - lr: 0.050000\n",
      "2022-05-08 16:40:54,998 epoch 67 - iter 74/373 - loss 0.01276620 - samples/sec: 23.06 - lr: 0.050000\n",
      "2022-05-08 16:41:02,074 epoch 67 - iter 111/373 - loss 0.01174303 - samples/sec: 21.12 - lr: 0.050000\n",
      "2022-05-08 16:41:09,209 epoch 67 - iter 148/373 - loss 0.01112908 - samples/sec: 20.94 - lr: 0.050000\n",
      "2022-05-08 16:41:16,350 epoch 67 - iter 185/373 - loss 0.01138826 - samples/sec: 20.95 - lr: 0.050000\n",
      "2022-05-08 16:41:23,318 epoch 67 - iter 222/373 - loss 0.01161746 - samples/sec: 21.47 - lr: 0.050000\n",
      "2022-05-08 16:41:29,300 epoch 67 - iter 259/373 - loss 0.01166859 - samples/sec: 25.04 - lr: 0.050000\n",
      "2022-05-08 16:41:36,479 epoch 67 - iter 296/373 - loss 0.01231771 - samples/sec: 20.82 - lr: 0.050000\n",
      "2022-05-08 16:41:44,105 epoch 67 - iter 333/373 - loss 0.01207606 - samples/sec: 19.59 - lr: 0.050000\n",
      "2022-05-08 16:41:50,588 epoch 67 - iter 370/373 - loss 0.01202770 - samples/sec: 23.09 - lr: 0.050000\n",
      "2022-05-08 16:41:51,106 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:41:51,106 EPOCH 67 done: loss 0.0121 - lr 0.050000\n",
      "2022-05-08 16:41:51,107 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 16:41:53,527 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:41:53,528 train mode resetting embeddings\n",
      "2022-05-08 16:41:53,537 train mode resetting embeddings\n",
      "2022-05-08 16:42:00,848 epoch 68 - iter 37/373 - loss 0.01229464 - samples/sec: 20.27 - lr: 0.050000\n",
      "2022-05-08 16:42:07,666 epoch 68 - iter 74/373 - loss 0.00996649 - samples/sec: 21.92 - lr: 0.050000\n",
      "2022-05-08 16:42:13,879 epoch 68 - iter 111/373 - loss 0.00905439 - samples/sec: 24.08 - lr: 0.050000\n",
      "2022-05-08 16:42:19,866 epoch 68 - iter 148/373 - loss 0.00918363 - samples/sec: 25.02 - lr: 0.050000\n",
      "2022-05-08 16:42:26,028 epoch 68 - iter 185/373 - loss 0.00951821 - samples/sec: 24.30 - lr: 0.050000\n",
      "2022-05-08 16:42:33,098 epoch 68 - iter 222/373 - loss 0.00990021 - samples/sec: 21.14 - lr: 0.050000\n",
      "2022-05-08 16:42:39,695 epoch 68 - iter 259/373 - loss 0.00957069 - samples/sec: 22.66 - lr: 0.050000\n",
      "2022-05-08 16:42:45,888 epoch 68 - iter 296/373 - loss 0.00990959 - samples/sec: 24.18 - lr: 0.050000\n",
      "2022-05-08 16:42:53,206 epoch 68 - iter 333/373 - loss 0.01048101 - samples/sec: 20.42 - lr: 0.050000\n",
      "2022-05-08 16:43:01,299 epoch 68 - iter 370/373 - loss 0.01062561 - samples/sec: 18.45 - lr: 0.050000\n",
      "2022-05-08 16:43:01,815 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:43:01,816 EPOCH 68 done: loss 0.0106 - lr 0.050000\n",
      "2022-05-08 16:43:01,817 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 16:43:03,947 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:43:03,948 train mode resetting embeddings\n",
      "2022-05-08 16:43:03,954 train mode resetting embeddings\n",
      "2022-05-08 16:43:11,152 epoch 69 - iter 37/373 - loss 0.01360616 - samples/sec: 20.59 - lr: 0.050000\n",
      "2022-05-08 16:43:17,614 epoch 69 - iter 74/373 - loss 0.01461418 - samples/sec: 23.14 - lr: 0.050000\n",
      "2022-05-08 16:43:24,004 epoch 69 - iter 111/373 - loss 0.01238804 - samples/sec: 23.40 - lr: 0.050000\n",
      "2022-05-08 16:43:31,257 epoch 69 - iter 148/373 - loss 0.01201019 - samples/sec: 20.61 - lr: 0.050000\n",
      "2022-05-08 16:43:38,086 epoch 69 - iter 185/373 - loss 0.01161318 - samples/sec: 21.91 - lr: 0.050000\n",
      "2022-05-08 16:43:45,180 epoch 69 - iter 222/373 - loss 0.01194396 - samples/sec: 21.07 - lr: 0.050000\n",
      "2022-05-08 16:43:52,072 epoch 69 - iter 259/373 - loss 0.01194757 - samples/sec: 21.68 - lr: 0.050000\n",
      "2022-05-08 16:43:59,245 epoch 69 - iter 296/373 - loss 0.01235518 - samples/sec: 20.82 - lr: 0.050000\n",
      "2022-05-08 16:44:05,548 epoch 69 - iter 333/373 - loss 0.01203235 - samples/sec: 23.75 - lr: 0.050000\n",
      "2022-05-08 16:44:12,458 epoch 69 - iter 370/373 - loss 0.01201167 - samples/sec: 21.64 - lr: 0.050000\n",
      "2022-05-08 16:44:13,222 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:44:13,223 EPOCH 69 done: loss 0.0122 - lr 0.050000\n",
      "2022-05-08 16:44:13,223 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 16:44:15,572 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:44:15,573 train mode resetting embeddings\n",
      "2022-05-08 16:44:15,578 train mode resetting embeddings\n",
      "2022-05-08 16:44:22,045 epoch 70 - iter 37/373 - loss 0.01079456 - samples/sec: 22.91 - lr: 0.050000\n",
      "2022-05-08 16:44:28,767 epoch 70 - iter 74/373 - loss 0.00990853 - samples/sec: 22.24 - lr: 0.050000\n",
      "2022-05-08 16:44:35,325 epoch 70 - iter 111/373 - loss 0.01058438 - samples/sec: 22.81 - lr: 0.050000\n",
      "2022-05-08 16:44:42,773 epoch 70 - iter 148/373 - loss 0.01013470 - samples/sec: 20.09 - lr: 0.050000\n",
      "2022-05-08 16:44:49,969 epoch 70 - iter 185/373 - loss 0.01040932 - samples/sec: 20.76 - lr: 0.050000\n",
      "2022-05-08 16:44:57,257 epoch 70 - iter 222/373 - loss 0.01056620 - samples/sec: 20.50 - lr: 0.050000\n",
      "2022-05-08 16:45:03,915 epoch 70 - iter 259/373 - loss 0.01090627 - samples/sec: 22.49 - lr: 0.050000\n",
      "2022-05-08 16:45:10,049 epoch 70 - iter 296/373 - loss 0.01107669 - samples/sec: 24.42 - lr: 0.050000\n",
      "2022-05-08 16:45:16,464 epoch 70 - iter 333/373 - loss 0.01104182 - samples/sec: 23.32 - lr: 0.050000\n",
      "2022-05-08 16:45:23,167 epoch 70 - iter 370/373 - loss 0.01075928 - samples/sec: 22.35 - lr: 0.050000\n",
      "2022-05-08 16:45:23,932 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:45:23,932 EPOCH 70 done: loss 0.0111 - lr 0.050000\n",
      "2022-05-08 16:45:23,934 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 16:45:26,027 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:45:26,028 train mode resetting embeddings\n",
      "2022-05-08 16:45:26,034 train mode resetting embeddings\n",
      "2022-05-08 16:45:32,805 epoch 71 - iter 37/373 - loss 0.00875891 - samples/sec: 21.88 - lr: 0.050000\n",
      "2022-05-08 16:45:39,513 epoch 71 - iter 74/373 - loss 0.00769561 - samples/sec: 22.31 - lr: 0.050000\n",
      "2022-05-08 16:45:46,229 epoch 71 - iter 111/373 - loss 0.00863252 - samples/sec: 22.28 - lr: 0.050000\n",
      "2022-05-08 16:45:53,073 epoch 71 - iter 148/373 - loss 0.00938101 - samples/sec: 21.86 - lr: 0.050000\n",
      "2022-05-08 16:46:00,172 epoch 71 - iter 185/373 - loss 0.00965338 - samples/sec: 21.05 - lr: 0.050000\n",
      "2022-05-08 16:46:07,727 epoch 71 - iter 222/373 - loss 0.01011848 - samples/sec: 19.82 - lr: 0.050000\n",
      "2022-05-08 16:46:14,264 epoch 71 - iter 259/373 - loss 0.01039393 - samples/sec: 22.89 - lr: 0.050000\n",
      "2022-05-08 16:46:21,676 epoch 71 - iter 296/373 - loss 0.01031788 - samples/sec: 20.17 - lr: 0.050000\n",
      "2022-05-08 16:46:27,645 epoch 71 - iter 333/373 - loss 0.01044644 - samples/sec: 25.07 - lr: 0.050000\n",
      "2022-05-08 16:46:34,749 epoch 71 - iter 370/373 - loss 0.01048804 - samples/sec: 21.03 - lr: 0.050000\n",
      "2022-05-08 16:46:35,146 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:46:35,147 EPOCH 71 done: loss 0.0104 - lr 0.050000\n",
      "2022-05-08 16:46:35,148 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 16:46:37,523 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:46:37,524 train mode resetting embeddings\n",
      "2022-05-08 16:46:37,531 train mode resetting embeddings\n",
      "2022-05-08 16:46:44,528 epoch 72 - iter 37/373 - loss 0.01216154 - samples/sec: 21.17 - lr: 0.050000\n",
      "2022-05-08 16:46:50,368 epoch 72 - iter 74/373 - loss 0.00993804 - samples/sec: 25.63 - lr: 0.050000\n",
      "2022-05-08 16:46:56,634 epoch 72 - iter 111/373 - loss 0.00954853 - samples/sec: 23.87 - lr: 0.050000\n",
      "2022-05-08 16:47:03,660 epoch 72 - iter 148/373 - loss 0.00974358 - samples/sec: 21.26 - lr: 0.050000\n",
      "2022-05-08 16:47:11,213 epoch 72 - iter 185/373 - loss 0.01036968 - samples/sec: 19.78 - lr: 0.050000\n",
      "2022-05-08 16:47:18,158 epoch 72 - iter 222/373 - loss 0.01083015 - samples/sec: 21.54 - lr: 0.050000\n",
      "2022-05-08 16:47:24,416 epoch 72 - iter 259/373 - loss 0.01151820 - samples/sec: 23.91 - lr: 0.050000\n",
      "2022-05-08 16:47:31,414 epoch 72 - iter 296/373 - loss 0.01153503 - samples/sec: 21.37 - lr: 0.050000\n",
      "2022-05-08 16:47:38,338 epoch 72 - iter 333/373 - loss 0.01141146 - samples/sec: 21.62 - lr: 0.050000\n",
      "2022-05-08 16:47:44,814 epoch 72 - iter 370/373 - loss 0.01136170 - samples/sec: 23.11 - lr: 0.050000\n",
      "2022-05-08 16:47:45,403 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:47:45,403 EPOCH 72 done: loss 0.0114 - lr 0.050000\n",
      "2022-05-08 16:47:45,404 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 16:47:47,528 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:47:47,529 train mode resetting embeddings\n",
      "2022-05-08 16:47:47,534 train mode resetting embeddings\n",
      "2022-05-08 16:47:54,334 epoch 73 - iter 37/373 - loss 0.01404854 - samples/sec: 21.79 - lr: 0.050000\n",
      "2022-05-08 16:48:01,023 epoch 73 - iter 74/373 - loss 0.01125301 - samples/sec: 22.35 - lr: 0.050000\n",
      "2022-05-08 16:48:07,141 epoch 73 - iter 111/373 - loss 0.01159751 - samples/sec: 24.47 - lr: 0.050000\n",
      "2022-05-08 16:48:14,926 epoch 73 - iter 148/373 - loss 0.01019754 - samples/sec: 19.19 - lr: 0.050000\n",
      "2022-05-08 16:48:24,015 epoch 73 - iter 185/373 - loss 0.01052221 - samples/sec: 16.46 - lr: 0.050000\n",
      "2022-05-08 16:48:31,391 epoch 73 - iter 222/373 - loss 0.01048347 - samples/sec: 20.28 - lr: 0.050000\n",
      "2022-05-08 16:48:37,952 epoch 73 - iter 259/373 - loss 0.01018525 - samples/sec: 22.80 - lr: 0.050000\n",
      "2022-05-08 16:48:45,059 epoch 73 - iter 296/373 - loss 0.01010391 - samples/sec: 21.02 - lr: 0.050000\n",
      "2022-05-08 16:48:51,920 epoch 73 - iter 333/373 - loss 0.01026207 - samples/sec: 21.80 - lr: 0.050000\n",
      "2022-05-08 16:48:58,975 epoch 73 - iter 370/373 - loss 0.01042195 - samples/sec: 21.20 - lr: 0.050000\n",
      "2022-05-08 16:48:59,578 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:48:59,579 EPOCH 73 done: loss 0.0104 - lr 0.050000\n",
      "2022-05-08 16:48:59,580 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 16:49:01,971 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:49:01,972 train mode resetting embeddings\n",
      "2022-05-08 16:49:01,978 train mode resetting embeddings\n",
      "2022-05-08 16:49:09,383 epoch 74 - iter 37/373 - loss 0.00763317 - samples/sec: 20.01 - lr: 0.050000\n",
      "2022-05-08 16:49:16,584 epoch 74 - iter 74/373 - loss 0.00900566 - samples/sec: 20.76 - lr: 0.050000\n",
      "2022-05-08 16:49:23,853 epoch 74 - iter 111/373 - loss 0.00958309 - samples/sec: 20.57 - lr: 0.050000\n",
      "2022-05-08 16:49:30,762 epoch 74 - iter 148/373 - loss 0.01011780 - samples/sec: 21.65 - lr: 0.050000\n",
      "2022-05-08 16:49:37,607 epoch 74 - iter 185/373 - loss 0.01008680 - samples/sec: 21.85 - lr: 0.050000\n",
      "2022-05-08 16:49:43,938 epoch 74 - iter 222/373 - loss 0.01043222 - samples/sec: 23.66 - lr: 0.050000\n",
      "2022-05-08 16:49:49,462 epoch 74 - iter 259/373 - loss 0.01014183 - samples/sec: 27.14 - lr: 0.050000\n",
      "2022-05-08 16:49:56,413 epoch 74 - iter 296/373 - loss 0.00985523 - samples/sec: 21.52 - lr: 0.050000\n",
      "2022-05-08 16:50:03,132 epoch 74 - iter 333/373 - loss 0.00988434 - samples/sec: 22.26 - lr: 0.050000\n",
      "2022-05-08 16:50:10,238 epoch 74 - iter 370/373 - loss 0.01045419 - samples/sec: 21.03 - lr: 0.050000\n",
      "2022-05-08 16:50:10,875 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:50:10,876 EPOCH 74 done: loss 0.0104 - lr 0.050000\n",
      "2022-05-08 16:50:10,877 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 16:50:12,910 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:50:12,911 train mode resetting embeddings\n",
      "2022-05-08 16:50:12,916 train mode resetting embeddings\n",
      "2022-05-08 16:50:20,596 epoch 75 - iter 37/373 - loss 0.01110073 - samples/sec: 19.30 - lr: 0.050000\n",
      "2022-05-08 16:50:26,848 epoch 75 - iter 74/373 - loss 0.01069117 - samples/sec: 23.93 - lr: 0.050000\n",
      "2022-05-08 16:50:33,951 epoch 75 - iter 111/373 - loss 0.01057941 - samples/sec: 21.05 - lr: 0.050000\n",
      "2022-05-08 16:50:40,072 epoch 75 - iter 148/373 - loss 0.01039006 - samples/sec: 24.46 - lr: 0.050000\n",
      "2022-05-08 16:50:46,574 epoch 75 - iter 185/373 - loss 0.01070163 - samples/sec: 23.00 - lr: 0.050000\n",
      "2022-05-08 16:50:53,768 epoch 75 - iter 222/373 - loss 0.01140416 - samples/sec: 20.77 - lr: 0.050000\n",
      "2022-05-08 16:51:00,799 epoch 75 - iter 259/373 - loss 0.01113303 - samples/sec: 21.26 - lr: 0.050000\n",
      "2022-05-08 16:51:07,856 epoch 75 - iter 296/373 - loss 0.01104715 - samples/sec: 21.18 - lr: 0.050000\n",
      "2022-05-08 16:51:13,972 epoch 75 - iter 333/373 - loss 0.01116321 - samples/sec: 24.47 - lr: 0.050000\n",
      "2022-05-08 16:51:20,298 epoch 75 - iter 370/373 - loss 0.01113584 - samples/sec: 23.65 - lr: 0.050000\n",
      "2022-05-08 16:51:20,830 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:51:20,831 EPOCH 75 done: loss 0.0111 - lr 0.050000\n",
      "2022-05-08 16:51:20,833 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 16:51:23,323 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:51:23,324 train mode resetting embeddings\n",
      "2022-05-08 16:51:23,331 train mode resetting embeddings\n",
      "2022-05-08 16:51:31,167 epoch 76 - iter 37/373 - loss 0.01177070 - samples/sec: 18.91 - lr: 0.050000\n",
      "2022-05-08 16:51:37,764 epoch 76 - iter 74/373 - loss 0.01021865 - samples/sec: 22.68 - lr: 0.050000\n",
      "2022-05-08 16:51:43,963 epoch 76 - iter 111/373 - loss 0.01023580 - samples/sec: 24.15 - lr: 0.050000\n",
      "2022-05-08 16:51:49,862 epoch 76 - iter 148/373 - loss 0.01131153 - samples/sec: 25.40 - lr: 0.050000\n",
      "2022-05-08 16:51:57,019 epoch 76 - iter 185/373 - loss 0.01067364 - samples/sec: 20.88 - lr: 0.050000\n",
      "2022-05-08 16:52:03,636 epoch 76 - iter 222/373 - loss 0.01087464 - samples/sec: 22.60 - lr: 0.050000\n",
      "2022-05-08 16:52:10,031 epoch 76 - iter 259/373 - loss 0.01042949 - samples/sec: 23.39 - lr: 0.050000\n",
      "2022-05-08 16:52:16,725 epoch 76 - iter 296/373 - loss 0.01076814 - samples/sec: 22.33 - lr: 0.050000\n",
      "2022-05-08 16:52:23,157 epoch 76 - iter 333/373 - loss 0.01123511 - samples/sec: 23.26 - lr: 0.050000\n",
      "2022-05-08 16:52:32,132 epoch 76 - iter 370/373 - loss 0.01052165 - samples/sec: 16.62 - lr: 0.050000\n",
      "2022-05-08 16:52:33,065 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:52:33,066 EPOCH 76 done: loss 0.0105 - lr 0.050000\n",
      "2022-05-08 16:52:33,067 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 16:52:35,122 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:52:35,123 train mode resetting embeddings\n",
      "2022-05-08 16:52:35,129 train mode resetting embeddings\n",
      "2022-05-08 16:52:42,351 epoch 77 - iter 37/373 - loss 0.01082579 - samples/sec: 20.51 - lr: 0.050000\n",
      "2022-05-08 16:52:49,230 epoch 77 - iter 74/373 - loss 0.01121871 - samples/sec: 21.74 - lr: 0.050000\n",
      "2022-05-08 16:52:56,251 epoch 77 - iter 111/373 - loss 0.01098861 - samples/sec: 21.30 - lr: 0.050000\n",
      "2022-05-08 16:53:02,196 epoch 77 - iter 148/373 - loss 0.00997108 - samples/sec: 25.20 - lr: 0.050000\n",
      "2022-05-08 16:53:08,772 epoch 77 - iter 185/373 - loss 0.01014425 - samples/sec: 22.73 - lr: 0.050000\n",
      "2022-05-08 16:53:15,359 epoch 77 - iter 222/373 - loss 0.00996044 - samples/sec: 22.70 - lr: 0.050000\n",
      "2022-05-08 16:53:22,087 epoch 77 - iter 259/373 - loss 0.00945529 - samples/sec: 22.22 - lr: 0.050000\n",
      "2022-05-08 16:53:29,630 epoch 77 - iter 296/373 - loss 0.00962273 - samples/sec: 19.81 - lr: 0.050000\n",
      "2022-05-08 16:53:37,887 epoch 77 - iter 333/373 - loss 0.00994826 - samples/sec: 18.08 - lr: 0.050000\n",
      "2022-05-08 16:53:44,784 epoch 77 - iter 370/373 - loss 0.00981718 - samples/sec: 21.67 - lr: 0.050000\n",
      "2022-05-08 16:53:45,219 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:53:45,220 EPOCH 77 done: loss 0.0098 - lr 0.050000\n",
      "2022-05-08 16:53:45,221 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 16:53:47,551 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:53:47,552 train mode resetting embeddings\n",
      "2022-05-08 16:53:47,558 train mode resetting embeddings\n",
      "2022-05-08 16:53:54,509 epoch 78 - iter 37/373 - loss 0.01057423 - samples/sec: 21.31 - lr: 0.050000\n",
      "2022-05-08 16:54:02,763 epoch 78 - iter 74/373 - loss 0.01065547 - samples/sec: 18.08 - lr: 0.050000\n",
      "2022-05-08 16:54:10,404 epoch 78 - iter 111/373 - loss 0.00905377 - samples/sec: 19.55 - lr: 0.050000\n",
      "2022-05-08 16:54:16,379 epoch 78 - iter 148/373 - loss 0.00890583 - samples/sec: 25.07 - lr: 0.050000\n",
      "2022-05-08 16:54:23,636 epoch 78 - iter 185/373 - loss 0.00918936 - samples/sec: 20.60 - lr: 0.050000\n",
      "2022-05-08 16:54:31,188 epoch 78 - iter 222/373 - loss 0.00946851 - samples/sec: 19.78 - lr: 0.050000\n",
      "2022-05-08 16:54:38,894 epoch 78 - iter 259/373 - loss 0.00943860 - samples/sec: 19.39 - lr: 0.050000\n",
      "2022-05-08 16:54:44,829 epoch 78 - iter 296/373 - loss 0.00955390 - samples/sec: 25.23 - lr: 0.050000\n",
      "2022-05-08 16:54:51,373 epoch 78 - iter 333/373 - loss 0.00952391 - samples/sec: 22.89 - lr: 0.050000\n",
      "2022-05-08 16:54:57,964 epoch 78 - iter 370/373 - loss 0.00965329 - samples/sec: 22.69 - lr: 0.050000\n",
      "2022-05-08 16:54:58,550 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:54:58,551 EPOCH 78 done: loss 0.0096 - lr 0.050000\n",
      "2022-05-08 16:54:58,553 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 16:55:00,746 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:55:00,747 train mode resetting embeddings\n",
      "2022-05-08 16:55:00,752 train mode resetting embeddings\n",
      "2022-05-08 16:55:07,358 epoch 79 - iter 37/373 - loss 0.00622659 - samples/sec: 22.43 - lr: 0.050000\n",
      "2022-05-08 16:55:14,602 epoch 79 - iter 74/373 - loss 0.00694869 - samples/sec: 20.63 - lr: 0.050000\n",
      "2022-05-08 16:55:21,595 epoch 79 - iter 111/373 - loss 0.00834896 - samples/sec: 21.39 - lr: 0.050000\n",
      "2022-05-08 16:55:27,455 epoch 79 - iter 148/373 - loss 0.00811051 - samples/sec: 25.56 - lr: 0.050000\n",
      "2022-05-08 16:55:34,680 epoch 79 - iter 185/373 - loss 0.00797876 - samples/sec: 20.69 - lr: 0.050000\n",
      "2022-05-08 16:55:42,136 epoch 79 - iter 222/373 - loss 0.00832739 - samples/sec: 20.07 - lr: 0.050000\n",
      "2022-05-08 16:55:49,525 epoch 79 - iter 259/373 - loss 0.00913749 - samples/sec: 20.23 - lr: 0.050000\n",
      "2022-05-08 16:55:56,071 epoch 79 - iter 296/373 - loss 0.00929238 - samples/sec: 22.87 - lr: 0.050000\n",
      "2022-05-08 16:56:03,190 epoch 79 - iter 333/373 - loss 0.00965286 - samples/sec: 21.00 - lr: 0.050000\n",
      "2022-05-08 16:56:10,592 epoch 79 - iter 370/373 - loss 0.00956660 - samples/sec: 20.17 - lr: 0.050000\n",
      "2022-05-08 16:56:11,112 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:56:11,113 EPOCH 79 done: loss 0.0095 - lr 0.050000\n",
      "2022-05-08 16:56:11,113 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 16:56:13,511 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:56:13,512 train mode resetting embeddings\n",
      "2022-05-08 16:56:13,519 train mode resetting embeddings\n",
      "2022-05-08 16:56:20,260 epoch 80 - iter 37/373 - loss 0.01297935 - samples/sec: 21.99 - lr: 0.050000\n",
      "2022-05-08 16:56:28,455 epoch 80 - iter 74/373 - loss 0.01106477 - samples/sec: 18.23 - lr: 0.050000\n",
      "2022-05-08 16:56:35,730 epoch 80 - iter 111/373 - loss 0.01022349 - samples/sec: 20.56 - lr: 0.050000\n",
      "2022-05-08 16:56:42,628 epoch 80 - iter 148/373 - loss 0.01024022 - samples/sec: 21.67 - lr: 0.050000\n",
      "2022-05-08 16:56:48,939 epoch 80 - iter 185/373 - loss 0.01009739 - samples/sec: 23.76 - lr: 0.050000\n",
      "2022-05-08 16:56:56,762 epoch 80 - iter 222/373 - loss 0.00999179 - samples/sec: 19.09 - lr: 0.050000\n",
      "2022-05-08 16:57:03,592 epoch 80 - iter 259/373 - loss 0.00999514 - samples/sec: 21.89 - lr: 0.050000\n",
      "2022-05-08 16:57:10,204 epoch 80 - iter 296/373 - loss 0.01005269 - samples/sec: 22.62 - lr: 0.050000\n",
      "2022-05-08 16:57:17,003 epoch 80 - iter 333/373 - loss 0.00984199 - samples/sec: 21.99 - lr: 0.050000\n",
      "2022-05-08 16:57:23,729 epoch 80 - iter 370/373 - loss 0.00989583 - samples/sec: 22.22 - lr: 0.050000\n",
      "2022-05-08 16:57:24,268 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:57:24,268 EPOCH 80 done: loss 0.0100 - lr 0.050000\n",
      "2022-05-08 16:57:24,269 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 16:57:26,387 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:57:26,389 train mode resetting embeddings\n",
      "2022-05-08 16:57:26,395 train mode resetting embeddings\n",
      "2022-05-08 16:57:34,218 epoch 81 - iter 37/373 - loss 0.01140510 - samples/sec: 18.93 - lr: 0.050000\n",
      "2022-05-08 16:57:40,467 epoch 81 - iter 74/373 - loss 0.01032038 - samples/sec: 23.95 - lr: 0.050000\n",
      "2022-05-08 16:57:47,756 epoch 81 - iter 111/373 - loss 0.00939562 - samples/sec: 20.51 - lr: 0.050000\n",
      "2022-05-08 16:57:54,588 epoch 81 - iter 148/373 - loss 0.00929197 - samples/sec: 21.88 - lr: 0.050000\n",
      "2022-05-08 16:58:01,442 epoch 81 - iter 185/373 - loss 0.00907247 - samples/sec: 21.82 - lr: 0.050000\n",
      "2022-05-08 16:58:08,714 epoch 81 - iter 222/373 - loss 0.00927616 - samples/sec: 20.54 - lr: 0.050000\n",
      "2022-05-08 16:58:15,931 epoch 81 - iter 259/373 - loss 0.00924484 - samples/sec: 20.70 - lr: 0.050000\n",
      "2022-05-08 16:58:22,375 epoch 81 - iter 296/373 - loss 0.00914209 - samples/sec: 23.22 - lr: 0.050000\n",
      "2022-05-08 16:58:28,379 epoch 81 - iter 333/373 - loss 0.00892237 - samples/sec: 24.95 - lr: 0.050000\n",
      "2022-05-08 16:58:34,939 epoch 81 - iter 370/373 - loss 0.00891840 - samples/sec: 22.80 - lr: 0.050000\n",
      "2022-05-08 16:58:35,546 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:58:35,548 EPOCH 81 done: loss 0.0089 - lr 0.050000\n",
      "2022-05-08 16:58:35,548 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 16:58:37,934 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:58:37,935 train mode resetting embeddings\n",
      "2022-05-08 16:58:37,940 train mode resetting embeddings\n",
      "2022-05-08 16:58:44,863 epoch 82 - iter 37/373 - loss 0.00918841 - samples/sec: 21.41 - lr: 0.050000\n",
      "2022-05-08 16:58:51,652 epoch 82 - iter 74/373 - loss 0.00950871 - samples/sec: 22.02 - lr: 0.050000\n",
      "2022-05-08 16:58:58,758 epoch 82 - iter 111/373 - loss 0.01013118 - samples/sec: 21.03 - lr: 0.050000\n",
      "2022-05-08 16:59:05,231 epoch 82 - iter 148/373 - loss 0.01011296 - samples/sec: 23.11 - lr: 0.050000\n",
      "2022-05-08 16:59:12,162 epoch 82 - iter 185/373 - loss 0.01034690 - samples/sec: 21.59 - lr: 0.050000\n",
      "2022-05-08 16:59:18,747 epoch 82 - iter 222/373 - loss 0.01027488 - samples/sec: 22.71 - lr: 0.050000\n",
      "2022-05-08 16:59:26,411 epoch 82 - iter 259/373 - loss 0.01043129 - samples/sec: 19.50 - lr: 0.050000\n",
      "2022-05-08 16:59:33,323 epoch 82 - iter 296/373 - loss 0.01011130 - samples/sec: 21.62 - lr: 0.050000\n",
      "2022-05-08 16:59:40,638 epoch 82 - iter 333/373 - loss 0.00997563 - samples/sec: 20.43 - lr: 0.050000\n",
      "2022-05-08 16:59:47,912 epoch 82 - iter 370/373 - loss 0.00989630 - samples/sec: 20.57 - lr: 0.050000\n",
      "2022-05-08 16:59:48,387 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:59:48,388 EPOCH 82 done: loss 0.0099 - lr 0.050000\n",
      "2022-05-08 16:59:48,389 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 16:59:50,499 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 16:59:50,500 train mode resetting embeddings\n",
      "2022-05-08 16:59:50,506 train mode resetting embeddings\n",
      "2022-05-08 16:59:57,699 epoch 83 - iter 37/373 - loss 0.00917217 - samples/sec: 20.60 - lr: 0.050000\n",
      "2022-05-08 17:00:04,962 epoch 83 - iter 74/373 - loss 0.00958548 - samples/sec: 20.58 - lr: 0.050000\n",
      "2022-05-08 17:00:12,339 epoch 83 - iter 111/373 - loss 0.00974614 - samples/sec: 20.25 - lr: 0.050000\n",
      "2022-05-08 17:00:18,667 epoch 83 - iter 148/373 - loss 0.00961256 - samples/sec: 23.67 - lr: 0.050000\n",
      "2022-05-08 17:00:26,050 epoch 83 - iter 185/373 - loss 0.00953701 - samples/sec: 20.24 - lr: 0.050000\n",
      "2022-05-08 17:00:32,719 epoch 83 - iter 222/373 - loss 0.00975390 - samples/sec: 22.43 - lr: 0.050000\n",
      "2022-05-08 17:00:39,809 epoch 83 - iter 259/373 - loss 0.00964735 - samples/sec: 21.09 - lr: 0.050000\n",
      "2022-05-08 17:00:46,007 epoch 83 - iter 296/373 - loss 0.00982600 - samples/sec: 24.17 - lr: 0.050000\n",
      "2022-05-08 17:00:52,638 epoch 83 - iter 333/373 - loss 0.00960044 - samples/sec: 22.55 - lr: 0.050000\n",
      "2022-05-08 17:00:59,824 epoch 83 - iter 370/373 - loss 0.00961828 - samples/sec: 20.81 - lr: 0.050000\n",
      "2022-05-08 17:01:00,507 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:01:00,508 EPOCH 83 done: loss 0.0095 - lr 0.050000\n",
      "2022-05-08 17:01:00,509 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 17:01:02,873 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:01:02,874 train mode resetting embeddings\n",
      "2022-05-08 17:01:02,880 train mode resetting embeddings\n",
      "2022-05-08 17:01:10,215 epoch 84 - iter 37/373 - loss 0.00745041 - samples/sec: 20.20 - lr: 0.050000\n",
      "2022-05-08 17:01:17,508 epoch 84 - iter 74/373 - loss 0.00914070 - samples/sec: 20.50 - lr: 0.050000\n",
      "2022-05-08 17:01:24,139 epoch 84 - iter 111/373 - loss 0.00868040 - samples/sec: 22.57 - lr: 0.050000\n",
      "2022-05-08 17:01:31,135 epoch 84 - iter 148/373 - loss 0.00867233 - samples/sec: 21.36 - lr: 0.050000\n",
      "2022-05-08 17:01:38,142 epoch 84 - iter 185/373 - loss 0.00868429 - samples/sec: 21.33 - lr: 0.050000\n",
      "2022-05-08 17:01:45,675 epoch 84 - iter 222/373 - loss 0.00853760 - samples/sec: 19.83 - lr: 0.050000\n",
      "2022-05-08 17:01:51,938 epoch 84 - iter 259/373 - loss 0.00833970 - samples/sec: 23.90 - lr: 0.050000\n",
      "2022-05-08 17:01:58,360 epoch 84 - iter 296/373 - loss 0.00800671 - samples/sec: 23.30 - lr: 0.050000\n",
      "2022-05-08 17:02:04,794 epoch 84 - iter 333/373 - loss 0.00846552 - samples/sec: 23.24 - lr: 0.050000\n",
      "2022-05-08 17:02:11,522 epoch 84 - iter 370/373 - loss 0.00851821 - samples/sec: 22.24 - lr: 0.050000\n",
      "2022-05-08 17:02:12,281 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:02:12,282 EPOCH 84 done: loss 0.0085 - lr 0.050000\n",
      "2022-05-08 17:02:12,283 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 17:02:14,315 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:02:14,315 train mode resetting embeddings\n",
      "2022-05-08 17:02:14,321 train mode resetting embeddings\n",
      "2022-05-08 17:02:21,912 epoch 85 - iter 37/373 - loss 0.01271221 - samples/sec: 19.52 - lr: 0.050000\n",
      "2022-05-08 17:02:28,518 epoch 85 - iter 74/373 - loss 0.01104203 - samples/sec: 22.63 - lr: 0.050000\n",
      "2022-05-08 17:02:34,957 epoch 85 - iter 111/373 - loss 0.01009561 - samples/sec: 23.23 - lr: 0.050000\n",
      "2022-05-08 17:02:42,407 epoch 85 - iter 148/373 - loss 0.01011732 - samples/sec: 20.05 - lr: 0.050000\n",
      "2022-05-08 17:02:48,819 epoch 85 - iter 185/373 - loss 0.00978607 - samples/sec: 23.33 - lr: 0.050000\n",
      "2022-05-08 17:02:56,179 epoch 85 - iter 222/373 - loss 0.00964680 - samples/sec: 20.31 - lr: 0.050000\n",
      "2022-05-08 17:03:02,736 epoch 85 - iter 259/373 - loss 0.00958965 - samples/sec: 22.81 - lr: 0.050000\n",
      "2022-05-08 17:03:10,142 epoch 85 - iter 296/373 - loss 0.00935316 - samples/sec: 20.17 - lr: 0.050000\n",
      "2022-05-08 17:03:17,257 epoch 85 - iter 333/373 - loss 0.00940428 - samples/sec: 21.00 - lr: 0.050000\n",
      "2022-05-08 17:03:23,742 epoch 85 - iter 370/373 - loss 0.00938061 - samples/sec: 23.07 - lr: 0.050000\n",
      "2022-05-08 17:03:24,267 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:03:24,267 EPOCH 85 done: loss 0.0094 - lr 0.050000\n",
      "2022-05-08 17:03:24,268 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 17:03:26,645 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:03:26,646 train mode resetting embeddings\n",
      "2022-05-08 17:03:26,652 train mode resetting embeddings\n",
      "2022-05-08 17:03:34,136 epoch 86 - iter 37/373 - loss 0.01100937 - samples/sec: 19.80 - lr: 0.050000\n",
      "2022-05-08 17:03:41,460 epoch 86 - iter 74/373 - loss 0.00891656 - samples/sec: 20.39 - lr: 0.050000\n",
      "2022-05-08 17:03:48,160 epoch 86 - iter 111/373 - loss 0.00869098 - samples/sec: 22.32 - lr: 0.050000\n",
      "2022-05-08 17:03:55,086 epoch 86 - iter 148/373 - loss 0.00771956 - samples/sec: 21.58 - lr: 0.050000\n",
      "2022-05-08 17:04:01,738 epoch 86 - iter 185/373 - loss 0.00855980 - samples/sec: 22.48 - lr: 0.050000\n",
      "2022-05-08 17:04:09,144 epoch 86 - iter 222/373 - loss 0.00879397 - samples/sec: 20.16 - lr: 0.050000\n",
      "2022-05-08 17:04:15,799 epoch 86 - iter 259/373 - loss 0.00913571 - samples/sec: 22.48 - lr: 0.050000\n",
      "2022-05-08 17:04:22,247 epoch 86 - iter 296/373 - loss 0.00926634 - samples/sec: 23.24 - lr: 0.050000\n",
      "2022-05-08 17:04:29,903 epoch 86 - iter 333/373 - loss 0.00916941 - samples/sec: 19.51 - lr: 0.050000\n",
      "2022-05-08 17:04:36,267 epoch 86 - iter 370/373 - loss 0.00924051 - samples/sec: 23.51 - lr: 0.050000\n",
      "2022-05-08 17:04:37,031 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:04:37,032 EPOCH 86 done: loss 0.0092 - lr 0.050000\n",
      "2022-05-08 17:04:37,033 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 17:04:39,149 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:04:39,150 train mode resetting embeddings\n",
      "2022-05-08 17:04:39,156 train mode resetting embeddings\n",
      "2022-05-08 17:04:45,721 epoch 87 - iter 37/373 - loss 0.00602262 - samples/sec: 22.58 - lr: 0.050000\n",
      "2022-05-08 17:04:53,070 epoch 87 - iter 74/373 - loss 0.00768901 - samples/sec: 20.33 - lr: 0.050000\n",
      "2022-05-08 17:04:59,763 epoch 87 - iter 111/373 - loss 0.00908560 - samples/sec: 22.37 - lr: 0.050000\n",
      "2022-05-08 17:05:06,804 epoch 87 - iter 148/373 - loss 0.00862891 - samples/sec: 21.25 - lr: 0.050000\n",
      "2022-05-08 17:05:13,812 epoch 87 - iter 185/373 - loss 0.00888107 - samples/sec: 21.32 - lr: 0.050000\n",
      "2022-05-08 17:05:21,647 epoch 87 - iter 222/373 - loss 0.00875604 - samples/sec: 19.06 - lr: 0.050000\n",
      "2022-05-08 17:05:29,140 epoch 87 - iter 259/373 - loss 0.00887833 - samples/sec: 19.94 - lr: 0.050000\n",
      "2022-05-08 17:05:35,977 epoch 87 - iter 296/373 - loss 0.00856361 - samples/sec: 21.87 - lr: 0.050000\n",
      "2022-05-08 17:05:42,563 epoch 87 - iter 333/373 - loss 0.00858546 - samples/sec: 22.72 - lr: 0.050000\n",
      "2022-05-08 17:05:48,840 epoch 87 - iter 370/373 - loss 0.00908772 - samples/sec: 23.84 - lr: 0.050000\n",
      "2022-05-08 17:05:49,471 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:05:49,473 EPOCH 87 done: loss 0.0090 - lr 0.050000\n",
      "2022-05-08 17:05:49,473 BAD EPOCHS (no improvement): 3\n",
      "2022-05-08 17:05:51,571 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:05:51,572 train mode resetting embeddings\n",
      "2022-05-08 17:05:51,579 train mode resetting embeddings\n",
      "2022-05-08 17:05:58,798 epoch 88 - iter 37/373 - loss 0.01009121 - samples/sec: 20.53 - lr: 0.050000\n",
      "2022-05-08 17:06:05,677 epoch 88 - iter 74/373 - loss 0.01152139 - samples/sec: 21.73 - lr: 0.050000\n",
      "2022-05-08 17:06:11,757 epoch 88 - iter 111/373 - loss 0.01075768 - samples/sec: 24.64 - lr: 0.050000\n",
      "2022-05-08 17:06:18,483 epoch 88 - iter 148/373 - loss 0.00993717 - samples/sec: 22.24 - lr: 0.050000\n",
      "2022-05-08 17:06:25,080 epoch 88 - iter 185/373 - loss 0.00915919 - samples/sec: 22.66 - lr: 0.050000\n",
      "2022-05-08 17:06:32,175 epoch 88 - iter 222/373 - loss 0.00938872 - samples/sec: 21.07 - lr: 0.050000\n",
      "2022-05-08 17:06:38,878 epoch 88 - iter 259/373 - loss 0.00944674 - samples/sec: 22.30 - lr: 0.050000\n",
      "2022-05-08 17:06:46,637 epoch 88 - iter 296/373 - loss 0.00962813 - samples/sec: 19.24 - lr: 0.050000\n",
      "2022-05-08 17:06:54,016 epoch 88 - iter 333/373 - loss 0.00944945 - samples/sec: 20.25 - lr: 0.050000\n",
      "2022-05-08 17:07:01,562 epoch 88 - iter 370/373 - loss 0.00902498 - samples/sec: 19.80 - lr: 0.050000\n",
      "2022-05-08 17:07:02,134 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:07:02,135 EPOCH 88 done: loss 0.0090 - lr 0.050000\n",
      "2022-05-08 17:07:02,136 Epoch    88: reducing learning rate of group 0 to 2.5000e-02.\n",
      "2022-05-08 17:07:02,137 BAD EPOCHS (no improvement): 4\n",
      "2022-05-08 17:07:04,510 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:07:04,511 train mode resetting embeddings\n",
      "2022-05-08 17:07:04,517 train mode resetting embeddings\n",
      "2022-05-08 17:07:11,892 epoch 89 - iter 37/373 - loss 0.00810702 - samples/sec: 20.09 - lr: 0.025000\n",
      "2022-05-08 17:07:17,867 epoch 89 - iter 74/373 - loss 0.00656510 - samples/sec: 25.08 - lr: 0.025000\n",
      "2022-05-08 17:07:24,341 epoch 89 - iter 111/373 - loss 0.00728611 - samples/sec: 23.11 - lr: 0.025000\n",
      "2022-05-08 17:07:32,564 epoch 89 - iter 148/373 - loss 0.00663441 - samples/sec: 18.15 - lr: 0.025000\n",
      "2022-05-08 17:07:39,503 epoch 89 - iter 185/373 - loss 0.00691657 - samples/sec: 21.57 - lr: 0.025000\n",
      "2022-05-08 17:07:46,393 epoch 89 - iter 222/373 - loss 0.00737347 - samples/sec: 21.70 - lr: 0.025000\n",
      "2022-05-08 17:07:53,174 epoch 89 - iter 259/373 - loss 0.00769881 - samples/sec: 22.04 - lr: 0.025000\n",
      "2022-05-08 17:07:59,827 epoch 89 - iter 296/373 - loss 0.00758867 - samples/sec: 22.48 - lr: 0.025000\n",
      "2022-05-08 17:08:07,181 epoch 89 - iter 333/373 - loss 0.00760458 - samples/sec: 20.31 - lr: 0.025000\n",
      "2022-05-08 17:08:14,117 epoch 89 - iter 370/373 - loss 0.00810231 - samples/sec: 21.56 - lr: 0.025000\n",
      "2022-05-08 17:08:14,579 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:08:14,579 EPOCH 89 done: loss 0.0081 - lr 0.025000\n",
      "2022-05-08 17:08:14,580 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 17:08:16,688 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:08:16,688 train mode resetting embeddings\n",
      "2022-05-08 17:08:16,694 train mode resetting embeddings\n",
      "2022-05-08 17:08:24,830 epoch 90 - iter 37/373 - loss 0.00895361 - samples/sec: 18.21 - lr: 0.025000\n",
      "2022-05-08 17:08:31,458 epoch 90 - iter 74/373 - loss 0.00818727 - samples/sec: 22.57 - lr: 0.025000\n",
      "2022-05-08 17:08:37,910 epoch 90 - iter 111/373 - loss 0.00807997 - samples/sec: 23.21 - lr: 0.025000\n",
      "2022-05-08 17:08:45,006 epoch 90 - iter 148/373 - loss 0.00820766 - samples/sec: 21.06 - lr: 0.025000\n",
      "2022-05-08 17:08:51,477 epoch 90 - iter 185/373 - loss 0.00773100 - samples/sec: 23.11 - lr: 0.025000\n",
      "2022-05-08 17:08:58,809 epoch 90 - iter 222/373 - loss 0.00793543 - samples/sec: 20.39 - lr: 0.025000\n",
      "2022-05-08 17:09:05,656 epoch 90 - iter 259/373 - loss 0.00766493 - samples/sec: 21.83 - lr: 0.025000\n",
      "2022-05-08 17:09:12,712 epoch 90 - iter 296/373 - loss 0.00785665 - samples/sec: 21.18 - lr: 0.025000\n",
      "2022-05-08 17:09:20,068 epoch 90 - iter 333/373 - loss 0.00769622 - samples/sec: 20.31 - lr: 0.025000\n",
      "2022-05-08 17:09:26,329 epoch 90 - iter 370/373 - loss 0.00786406 - samples/sec: 23.89 - lr: 0.025000\n",
      "2022-05-08 17:09:26,961 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:09:26,962 EPOCH 90 done: loss 0.0078 - lr 0.025000\n",
      "2022-05-08 17:09:26,963 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 17:09:29,296 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:09:29,297 train mode resetting embeddings\n",
      "2022-05-08 17:09:29,303 train mode resetting embeddings\n",
      "2022-05-08 17:09:36,049 epoch 91 - iter 37/373 - loss 0.00769381 - samples/sec: 21.97 - lr: 0.025000\n",
      "2022-05-08 17:09:42,715 epoch 91 - iter 74/373 - loss 0.00899426 - samples/sec: 22.43 - lr: 0.025000\n",
      "2022-05-08 17:09:49,899 epoch 91 - iter 111/373 - loss 0.00820138 - samples/sec: 20.79 - lr: 0.025000\n",
      "2022-05-08 17:09:56,773 epoch 91 - iter 148/373 - loss 0.00842545 - samples/sec: 21.75 - lr: 0.025000\n",
      "2022-05-08 17:10:04,118 epoch 91 - iter 185/373 - loss 0.00844474 - samples/sec: 20.35 - lr: 0.025000\n",
      "2022-05-08 17:10:11,135 epoch 91 - iter 222/373 - loss 0.00849368 - samples/sec: 21.30 - lr: 0.025000\n",
      "2022-05-08 17:10:18,732 epoch 91 - iter 259/373 - loss 0.00822372 - samples/sec: 19.66 - lr: 0.025000\n",
      "2022-05-08 17:10:25,130 epoch 91 - iter 296/373 - loss 0.00786848 - samples/sec: 23.41 - lr: 0.025000\n",
      "2022-05-08 17:10:31,553 epoch 91 - iter 333/373 - loss 0.00772561 - samples/sec: 23.29 - lr: 0.025000\n",
      "2022-05-08 17:10:38,766 epoch 91 - iter 370/373 - loss 0.00759660 - samples/sec: 20.71 - lr: 0.025000\n",
      "2022-05-08 17:10:39,176 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:10:39,176 EPOCH 91 done: loss 0.0076 - lr 0.025000\n",
      "2022-05-08 17:10:39,177 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 17:10:41,255 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:10:41,256 train mode resetting embeddings\n",
      "2022-05-08 17:10:41,262 train mode resetting embeddings\n",
      "2022-05-08 17:10:48,682 epoch 92 - iter 37/373 - loss 0.00761976 - samples/sec: 19.97 - lr: 0.025000\n",
      "2022-05-08 17:10:55,391 epoch 92 - iter 74/373 - loss 0.00743245 - samples/sec: 22.28 - lr: 0.025000\n",
      "2022-05-08 17:11:02,692 epoch 92 - iter 111/373 - loss 0.00812269 - samples/sec: 20.46 - lr: 0.025000\n",
      "2022-05-08 17:11:09,171 epoch 92 - iter 148/373 - loss 0.00752242 - samples/sec: 23.08 - lr: 0.025000\n",
      "2022-05-08 17:11:16,663 epoch 92 - iter 185/373 - loss 0.00734500 - samples/sec: 19.95 - lr: 0.025000\n",
      "2022-05-08 17:11:25,069 epoch 92 - iter 222/373 - loss 0.00779319 - samples/sec: 17.75 - lr: 0.025000\n",
      "2022-05-08 17:11:31,889 epoch 92 - iter 259/373 - loss 0.00739947 - samples/sec: 21.94 - lr: 0.025000\n",
      "2022-05-08 17:11:39,418 epoch 92 - iter 296/373 - loss 0.00751899 - samples/sec: 19.84 - lr: 0.025000\n",
      "2022-05-08 17:11:45,084 epoch 92 - iter 333/373 - loss 0.00746772 - samples/sec: 26.44 - lr: 0.025000\n",
      "2022-05-08 17:11:51,781 epoch 92 - iter 370/373 - loss 0.00732059 - samples/sec: 22.32 - lr: 0.025000\n",
      "2022-05-08 17:11:52,235 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:11:52,236 EPOCH 92 done: loss 0.0073 - lr 0.025000\n",
      "2022-05-08 17:11:52,237 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 17:11:54,649 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:11:54,650 train mode resetting embeddings\n",
      "2022-05-08 17:11:54,655 train mode resetting embeddings\n",
      "2022-05-08 17:12:01,660 epoch 93 - iter 37/373 - loss 0.00585464 - samples/sec: 21.15 - lr: 0.025000\n",
      "2022-05-08 17:12:09,621 epoch 93 - iter 74/373 - loss 0.00713274 - samples/sec: 18.77 - lr: 0.025000\n",
      "2022-05-08 17:12:16,887 epoch 93 - iter 111/373 - loss 0.00651358 - samples/sec: 20.57 - lr: 0.025000\n",
      "2022-05-08 17:12:23,010 epoch 93 - iter 148/373 - loss 0.00718323 - samples/sec: 24.44 - lr: 0.025000\n",
      "2022-05-08 17:12:28,955 epoch 93 - iter 185/373 - loss 0.00768244 - samples/sec: 25.19 - lr: 0.025000\n",
      "2022-05-08 17:12:35,911 epoch 93 - iter 222/373 - loss 0.00770824 - samples/sec: 21.49 - lr: 0.025000\n",
      "2022-05-08 17:12:43,541 epoch 93 - iter 259/373 - loss 0.00704291 - samples/sec: 19.57 - lr: 0.025000\n",
      "2022-05-08 17:12:49,750 epoch 93 - iter 296/373 - loss 0.00738875 - samples/sec: 24.14 - lr: 0.025000\n",
      "2022-05-08 17:12:56,330 epoch 93 - iter 333/373 - loss 0.00775894 - samples/sec: 22.75 - lr: 0.025000\n",
      "2022-05-08 17:13:04,041 epoch 93 - iter 370/373 - loss 0.00778397 - samples/sec: 19.36 - lr: 0.025000\n",
      "2022-05-08 17:13:04,760 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:13:04,761 EPOCH 93 done: loss 0.0078 - lr 0.025000\n",
      "2022-05-08 17:13:04,762 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 17:13:06,847 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:13:06,848 train mode resetting embeddings\n",
      "2022-05-08 17:13:06,854 train mode resetting embeddings\n",
      "2022-05-08 17:13:14,259 epoch 94 - iter 37/373 - loss 0.00596192 - samples/sec: 20.01 - lr: 0.025000\n",
      "2022-05-08 17:13:20,805 epoch 94 - iter 74/373 - loss 0.00666675 - samples/sec: 22.89 - lr: 0.025000\n",
      "2022-05-08 17:13:27,593 epoch 94 - iter 111/373 - loss 0.00630434 - samples/sec: 22.03 - lr: 0.025000\n",
      "2022-05-08 17:13:34,955 epoch 94 - iter 148/373 - loss 0.00770294 - samples/sec: 20.30 - lr: 0.025000\n",
      "2022-05-08 17:13:42,015 epoch 94 - iter 185/373 - loss 0.00710059 - samples/sec: 21.19 - lr: 0.025000\n",
      "2022-05-08 17:13:48,400 epoch 94 - iter 222/373 - loss 0.00726658 - samples/sec: 23.43 - lr: 0.025000\n",
      "2022-05-08 17:13:55,437 epoch 94 - iter 259/373 - loss 0.00754402 - samples/sec: 21.26 - lr: 0.025000\n",
      "2022-05-08 17:14:02,520 epoch 94 - iter 296/373 - loss 0.00737294 - samples/sec: 21.09 - lr: 0.025000\n",
      "2022-05-08 17:14:08,781 epoch 94 - iter 333/373 - loss 0.00724441 - samples/sec: 23.90 - lr: 0.025000\n",
      "2022-05-08 17:14:14,808 epoch 94 - iter 370/373 - loss 0.00701601 - samples/sec: 24.84 - lr: 0.025000\n",
      "2022-05-08 17:14:15,375 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:14:15,376 EPOCH 94 done: loss 0.0072 - lr 0.025000\n",
      "2022-05-08 17:14:15,377 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 17:14:17,814 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:14:17,815 train mode resetting embeddings\n",
      "2022-05-08 17:14:17,822 train mode resetting embeddings\n",
      "2022-05-08 17:14:24,406 epoch 95 - iter 37/373 - loss 0.00946404 - samples/sec: 22.50 - lr: 0.025000\n",
      "2022-05-08 17:14:31,621 epoch 95 - iter 74/373 - loss 0.00816963 - samples/sec: 20.71 - lr: 0.025000\n",
      "2022-05-08 17:14:38,652 epoch 95 - iter 111/373 - loss 0.00802122 - samples/sec: 21.25 - lr: 0.025000\n",
      "2022-05-08 17:14:46,591 epoch 95 - iter 148/373 - loss 0.00723485 - samples/sec: 18.81 - lr: 0.025000\n",
      "2022-05-08 17:14:53,060 epoch 95 - iter 185/373 - loss 0.00764202 - samples/sec: 23.12 - lr: 0.025000\n",
      "2022-05-08 17:14:59,454 epoch 95 - iter 222/373 - loss 0.00726562 - samples/sec: 23.39 - lr: 0.025000\n",
      "2022-05-08 17:15:06,254 epoch 95 - iter 259/373 - loss 0.00732392 - samples/sec: 22.01 - lr: 0.025000\n",
      "2022-05-08 17:15:13,137 epoch 95 - iter 296/373 - loss 0.00731803 - samples/sec: 21.71 - lr: 0.025000\n",
      "2022-05-08 17:15:20,301 epoch 95 - iter 333/373 - loss 0.00774887 - samples/sec: 20.86 - lr: 0.025000\n",
      "2022-05-08 17:15:27,093 epoch 95 - iter 370/373 - loss 0.00761199 - samples/sec: 22.01 - lr: 0.025000\n",
      "2022-05-08 17:15:27,619 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:15:27,620 EPOCH 95 done: loss 0.0076 - lr 0.025000\n",
      "2022-05-08 17:15:27,621 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 17:15:29,742 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:15:29,743 train mode resetting embeddings\n",
      "2022-05-08 17:15:29,749 train mode resetting embeddings\n",
      "2022-05-08 17:15:36,699 epoch 96 - iter 37/373 - loss 0.00861303 - samples/sec: 21.32 - lr: 0.025000\n",
      "2022-05-08 17:15:44,181 epoch 96 - iter 74/373 - loss 0.00869591 - samples/sec: 19.97 - lr: 0.025000\n",
      "2022-05-08 17:15:50,341 epoch 96 - iter 111/373 - loss 0.00772901 - samples/sec: 24.33 - lr: 0.025000\n",
      "2022-05-08 17:15:57,022 epoch 96 - iter 148/373 - loss 0.00674912 - samples/sec: 22.39 - lr: 0.025000\n",
      "2022-05-08 17:16:04,142 epoch 96 - iter 185/373 - loss 0.00704476 - samples/sec: 20.99 - lr: 0.025000\n",
      "2022-05-08 17:16:11,434 epoch 96 - iter 222/373 - loss 0.00690222 - samples/sec: 20.48 - lr: 0.025000\n",
      "2022-05-08 17:16:18,526 epoch 96 - iter 259/373 - loss 0.00693872 - samples/sec: 21.08 - lr: 0.025000\n",
      "2022-05-08 17:16:25,655 epoch 96 - iter 296/373 - loss 0.00700849 - samples/sec: 20.98 - lr: 0.025000\n",
      "2022-05-08 17:16:32,228 epoch 96 - iter 333/373 - loss 0.00665475 - samples/sec: 22.75 - lr: 0.025000\n",
      "2022-05-08 17:16:38,769 epoch 96 - iter 370/373 - loss 0.00668644 - samples/sec: 22.87 - lr: 0.025000\n",
      "2022-05-08 17:16:39,522 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:16:39,523 EPOCH 96 done: loss 0.0066 - lr 0.025000\n",
      "2022-05-08 17:16:39,524 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 17:16:41,897 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:16:41,898 train mode resetting embeddings\n",
      "2022-05-08 17:16:41,903 train mode resetting embeddings\n",
      "2022-05-08 17:16:49,314 epoch 97 - iter 37/373 - loss 0.00710099 - samples/sec: 19.99 - lr: 0.025000\n",
      "2022-05-08 17:16:55,720 epoch 97 - iter 74/373 - loss 0.00824478 - samples/sec: 23.37 - lr: 0.025000\n",
      "2022-05-08 17:17:02,546 epoch 97 - iter 111/373 - loss 0.00933411 - samples/sec: 21.90 - lr: 0.025000\n",
      "2022-05-08 17:17:08,586 epoch 97 - iter 148/373 - loss 0.00866602 - samples/sec: 24.78 - lr: 0.025000\n",
      "2022-05-08 17:17:15,379 epoch 97 - iter 185/373 - loss 0.00840259 - samples/sec: 22.03 - lr: 0.025000\n",
      "2022-05-08 17:17:22,523 epoch 97 - iter 222/373 - loss 0.00805530 - samples/sec: 20.92 - lr: 0.025000\n",
      "2022-05-08 17:17:29,377 epoch 97 - iter 259/373 - loss 0.00809482 - samples/sec: 21.82 - lr: 0.025000\n",
      "2022-05-08 17:17:36,537 epoch 97 - iter 296/373 - loss 0.00782035 - samples/sec: 20.86 - lr: 0.025000\n",
      "2022-05-08 17:17:44,824 epoch 97 - iter 333/373 - loss 0.00761488 - samples/sec: 18.02 - lr: 0.025000\n",
      "2022-05-08 17:17:51,061 epoch 97 - iter 370/373 - loss 0.00778683 - samples/sec: 24.01 - lr: 0.025000\n",
      "2022-05-08 17:17:51,732 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:17:51,732 EPOCH 97 done: loss 0.0078 - lr 0.025000\n",
      "2022-05-08 17:17:51,733 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 17:17:53,940 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:17:53,941 train mode resetting embeddings\n",
      "2022-05-08 17:17:53,947 train mode resetting embeddings\n",
      "2022-05-08 17:18:02,315 epoch 98 - iter 37/373 - loss 0.00498564 - samples/sec: 17.70 - lr: 0.025000\n",
      "2022-05-08 17:18:09,915 epoch 98 - iter 74/373 - loss 0.00660693 - samples/sec: 19.65 - lr: 0.025000\n",
      "2022-05-08 17:18:17,696 epoch 98 - iter 111/373 - loss 0.00798551 - samples/sec: 19.19 - lr: 0.025000\n",
      "2022-05-08 17:18:23,654 epoch 98 - iter 148/373 - loss 0.00789111 - samples/sec: 25.13 - lr: 0.025000\n",
      "2022-05-08 17:18:30,044 epoch 98 - iter 185/373 - loss 0.00726246 - samples/sec: 23.41 - lr: 0.025000\n",
      "2022-05-08 17:18:35,691 epoch 98 - iter 222/373 - loss 0.00713479 - samples/sec: 26.53 - lr: 0.025000\n",
      "2022-05-08 17:18:42,772 epoch 98 - iter 259/373 - loss 0.00700519 - samples/sec: 21.10 - lr: 0.025000\n",
      "2022-05-08 17:18:50,078 epoch 98 - iter 296/373 - loss 0.00725636 - samples/sec: 20.45 - lr: 0.025000\n",
      "2022-05-08 17:18:56,915 epoch 98 - iter 333/373 - loss 0.00721121 - samples/sec: 21.87 - lr: 0.025000\n",
      "2022-05-08 17:19:02,984 epoch 98 - iter 370/373 - loss 0.00727232 - samples/sec: 24.66 - lr: 0.025000\n",
      "2022-05-08 17:19:03,426 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:19:03,427 EPOCH 98 done: loss 0.0072 - lr 0.025000\n",
      "2022-05-08 17:19:03,428 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 17:19:05,790 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:19:05,792 train mode resetting embeddings\n",
      "2022-05-08 17:19:05,798 train mode resetting embeddings\n",
      "2022-05-08 17:19:12,799 epoch 99 - iter 37/373 - loss 0.00822655 - samples/sec: 21.16 - lr: 0.025000\n",
      "2022-05-08 17:19:20,402 epoch 99 - iter 74/373 - loss 0.00874644 - samples/sec: 19.65 - lr: 0.025000\n",
      "2022-05-08 17:19:26,401 epoch 99 - iter 111/373 - loss 0.00749332 - samples/sec: 25.03 - lr: 0.025000\n",
      "2022-05-08 17:19:33,134 epoch 99 - iter 148/373 - loss 0.00762745 - samples/sec: 22.21 - lr: 0.025000\n",
      "2022-05-08 17:19:38,816 epoch 99 - iter 185/373 - loss 0.00763145 - samples/sec: 26.37 - lr: 0.025000\n",
      "2022-05-08 17:19:45,635 epoch 99 - iter 222/373 - loss 0.00706195 - samples/sec: 21.92 - lr: 0.025000\n",
      "2022-05-08 17:19:52,889 epoch 99 - iter 259/373 - loss 0.00672970 - samples/sec: 20.60 - lr: 0.025000\n",
      "2022-05-08 17:20:00,955 epoch 99 - iter 296/373 - loss 0.00656856 - samples/sec: 18.50 - lr: 0.025000\n",
      "2022-05-08 17:20:07,924 epoch 99 - iter 333/373 - loss 0.00691873 - samples/sec: 21.44 - lr: 0.025000\n",
      "2022-05-08 17:20:14,736 epoch 99 - iter 370/373 - loss 0.00699181 - samples/sec: 21.96 - lr: 0.025000\n",
      "2022-05-08 17:20:15,459 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:20:15,461 EPOCH 99 done: loss 0.0071 - lr 0.025000\n",
      "2022-05-08 17:20:15,461 BAD EPOCHS (no improvement): 3\n",
      "2022-05-08 17:20:17,549 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:20:17,550 train mode resetting embeddings\n",
      "2022-05-08 17:20:17,556 train mode resetting embeddings\n",
      "2022-05-08 17:20:25,321 epoch 100 - iter 37/373 - loss 0.00738306 - samples/sec: 19.08 - lr: 0.025000\n",
      "2022-05-08 17:20:32,168 epoch 100 - iter 74/373 - loss 0.00658898 - samples/sec: 21.84 - lr: 0.025000\n",
      "2022-05-08 17:20:40,213 epoch 100 - iter 111/373 - loss 0.00667385 - samples/sec: 18.56 - lr: 0.025000\n",
      "2022-05-08 17:20:46,694 epoch 100 - iter 148/373 - loss 0.00667256 - samples/sec: 23.07 - lr: 0.025000\n",
      "2022-05-08 17:20:53,600 epoch 100 - iter 185/373 - loss 0.00676457 - samples/sec: 21.64 - lr: 0.025000\n",
      "2022-05-08 17:20:59,460 epoch 100 - iter 222/373 - loss 0.00696708 - samples/sec: 25.55 - lr: 0.025000\n",
      "2022-05-08 17:21:07,200 epoch 100 - iter 259/373 - loss 0.00699777 - samples/sec: 19.30 - lr: 0.025000\n",
      "2022-05-08 17:21:12,862 epoch 100 - iter 296/373 - loss 0.00709627 - samples/sec: 26.46 - lr: 0.025000\n",
      "2022-05-08 17:21:20,311 epoch 100 - iter 333/373 - loss 0.00723052 - samples/sec: 20.05 - lr: 0.025000\n",
      "2022-05-08 17:21:27,002 epoch 100 - iter 370/373 - loss 0.00751307 - samples/sec: 22.35 - lr: 0.025000\n",
      "2022-05-08 17:21:27,595 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:21:27,597 EPOCH 100 done: loss 0.0076 - lr 0.025000\n",
      "2022-05-08 17:21:27,598 Epoch   100: reducing learning rate of group 0 to 1.2500e-02.\n",
      "2022-05-08 17:21:27,598 BAD EPOCHS (no improvement): 4\n",
      "2022-05-08 17:21:29,987 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:21:29,988 train mode resetting embeddings\n",
      "2022-05-08 17:21:29,994 train mode resetting embeddings\n",
      "2022-05-08 17:21:37,272 epoch 101 - iter 37/373 - loss 0.00862835 - samples/sec: 20.36 - lr: 0.012500\n",
      "2022-05-08 17:21:44,038 epoch 101 - iter 74/373 - loss 0.00762654 - samples/sec: 22.09 - lr: 0.012500\n",
      "2022-05-08 17:21:51,369 epoch 101 - iter 111/373 - loss 0.00727252 - samples/sec: 20.38 - lr: 0.012500\n",
      "2022-05-08 17:21:58,242 epoch 101 - iter 148/373 - loss 0.00667957 - samples/sec: 21.75 - lr: 0.012500\n",
      "2022-05-08 17:22:04,363 epoch 101 - iter 185/373 - loss 0.00649211 - samples/sec: 24.45 - lr: 0.012500\n",
      "2022-05-08 17:22:10,750 epoch 101 - iter 222/373 - loss 0.00690948 - samples/sec: 23.43 - lr: 0.012500\n",
      "2022-05-08 17:22:17,690 epoch 101 - iter 259/373 - loss 0.00697017 - samples/sec: 21.56 - lr: 0.012500\n",
      "2022-05-08 17:22:24,462 epoch 101 - iter 296/373 - loss 0.00700357 - samples/sec: 22.08 - lr: 0.012500\n",
      "2022-05-08 17:22:31,115 epoch 101 - iter 333/373 - loss 0.00671154 - samples/sec: 22.48 - lr: 0.012500\n",
      "2022-05-08 17:22:39,347 epoch 101 - iter 370/373 - loss 0.00643884 - samples/sec: 18.14 - lr: 0.012500\n",
      "2022-05-08 17:22:40,047 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:22:40,048 EPOCH 101 done: loss 0.0064 - lr 0.012500\n",
      "2022-05-08 17:22:40,049 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 17:22:42,193 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:22:42,193 train mode resetting embeddings\n",
      "2022-05-08 17:22:42,199 train mode resetting embeddings\n",
      "2022-05-08 17:22:48,735 epoch 102 - iter 37/373 - loss 0.00753172 - samples/sec: 22.67 - lr: 0.012500\n",
      "2022-05-08 17:22:55,691 epoch 102 - iter 74/373 - loss 0.00671343 - samples/sec: 21.49 - lr: 0.012500\n",
      "2022-05-08 17:23:02,396 epoch 102 - iter 111/373 - loss 0.00604552 - samples/sec: 22.29 - lr: 0.012500\n",
      "2022-05-08 17:23:08,914 epoch 102 - iter 148/373 - loss 0.00661537 - samples/sec: 22.96 - lr: 0.012500\n",
      "2022-05-08 17:23:16,311 epoch 102 - iter 185/373 - loss 0.00621025 - samples/sec: 20.19 - lr: 0.012500\n",
      "2022-05-08 17:23:23,742 epoch 102 - iter 222/373 - loss 0.00608609 - samples/sec: 20.11 - lr: 0.012500\n",
      "2022-05-08 17:23:30,207 epoch 102 - iter 259/373 - loss 0.00626762 - samples/sec: 23.14 - lr: 0.012500\n",
      "2022-05-08 17:23:36,976 epoch 102 - iter 296/373 - loss 0.00611677 - samples/sec: 22.09 - lr: 0.012500\n",
      "2022-05-08 17:23:44,164 epoch 102 - iter 333/373 - loss 0.00592738 - samples/sec: 20.79 - lr: 0.012500\n",
      "2022-05-08 17:23:51,258 epoch 102 - iter 370/373 - loss 0.00613641 - samples/sec: 21.06 - lr: 0.012500\n",
      "2022-05-08 17:23:51,810 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:23:51,812 EPOCH 102 done: loss 0.0061 - lr 0.012500\n",
      "2022-05-08 17:23:51,812 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 17:23:54,176 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:23:54,176 train mode resetting embeddings\n",
      "2022-05-08 17:23:54,182 train mode resetting embeddings\n",
      "2022-05-08 17:24:01,199 epoch 103 - iter 37/373 - loss 0.00781368 - samples/sec: 21.11 - lr: 0.012500\n",
      "2022-05-08 17:24:08,351 epoch 103 - iter 74/373 - loss 0.00554203 - samples/sec: 20.89 - lr: 0.012500\n",
      "2022-05-08 17:24:14,820 epoch 103 - iter 111/373 - loss 0.00624892 - samples/sec: 23.12 - lr: 0.012500\n",
      "2022-05-08 17:24:22,893 epoch 103 - iter 148/373 - loss 0.00648915 - samples/sec: 18.49 - lr: 0.012500\n",
      "2022-05-08 17:24:29,470 epoch 103 - iter 185/373 - loss 0.00715847 - samples/sec: 22.76 - lr: 0.012500\n",
      "2022-05-08 17:24:36,741 epoch 103 - iter 222/373 - loss 0.00699280 - samples/sec: 20.56 - lr: 0.012500\n",
      "2022-05-08 17:24:43,298 epoch 103 - iter 259/373 - loss 0.00668306 - samples/sec: 22.83 - lr: 0.012500\n",
      "2022-05-08 17:24:50,180 epoch 103 - iter 296/373 - loss 0.00667810 - samples/sec: 21.72 - lr: 0.012500\n",
      "2022-05-08 17:24:56,730 epoch 103 - iter 333/373 - loss 0.00671861 - samples/sec: 22.83 - lr: 0.012500\n",
      "2022-05-08 17:25:03,269 epoch 103 - iter 370/373 - loss 0.00646775 - samples/sec: 22.87 - lr: 0.012500\n",
      "2022-05-08 17:25:03,651 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:25:03,652 EPOCH 103 done: loss 0.0064 - lr 0.012500\n",
      "2022-05-08 17:25:03,653 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 17:25:05,725 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:25:05,726 train mode resetting embeddings\n",
      "2022-05-08 17:25:05,732 train mode resetting embeddings\n",
      "2022-05-08 17:25:11,781 epoch 104 - iter 37/373 - loss 0.00348389 - samples/sec: 24.51 - lr: 0.012500\n",
      "2022-05-08 17:25:19,393 epoch 104 - iter 74/373 - loss 0.00548685 - samples/sec: 19.62 - lr: 0.012500\n",
      "2022-05-08 17:25:25,531 epoch 104 - iter 111/373 - loss 0.00585069 - samples/sec: 24.38 - lr: 0.012500\n",
      "2022-05-08 17:25:32,013 epoch 104 - iter 148/373 - loss 0.00583262 - samples/sec: 23.07 - lr: 0.012500\n",
      "2022-05-08 17:25:39,659 epoch 104 - iter 185/373 - loss 0.00574777 - samples/sec: 19.55 - lr: 0.012500\n",
      "2022-05-08 17:25:45,828 epoch 104 - iter 222/373 - loss 0.00565481 - samples/sec: 24.26 - lr: 0.012500\n",
      "2022-05-08 17:25:52,955 epoch 104 - iter 259/373 - loss 0.00578319 - samples/sec: 20.98 - lr: 0.012500\n",
      "2022-05-08 17:25:59,990 epoch 104 - iter 296/373 - loss 0.00569233 - samples/sec: 21.24 - lr: 0.012500\n",
      "2022-05-08 17:26:07,385 epoch 104 - iter 333/373 - loss 0.00571960 - samples/sec: 20.20 - lr: 0.012500\n",
      "2022-05-08 17:26:14,869 epoch 104 - iter 370/373 - loss 0.00615326 - samples/sec: 19.97 - lr: 0.012500\n",
      "2022-05-08 17:26:15,580 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:26:15,582 EPOCH 104 done: loss 0.0062 - lr 0.012500\n",
      "2022-05-08 17:26:15,583 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 17:26:17,665 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:26:17,666 train mode resetting embeddings\n",
      "2022-05-08 17:26:17,671 train mode resetting embeddings\n",
      "2022-05-08 17:26:23,693 epoch 105 - iter 37/373 - loss 0.00561723 - samples/sec: 24.61 - lr: 0.012500\n",
      "2022-05-08 17:26:30,555 epoch 105 - iter 74/373 - loss 0.00723989 - samples/sec: 21.78 - lr: 0.012500\n",
      "2022-05-08 17:26:38,026 epoch 105 - iter 111/373 - loss 0.00746995 - samples/sec: 20.01 - lr: 0.012500\n",
      "2022-05-08 17:26:44,576 epoch 105 - iter 148/373 - loss 0.00728096 - samples/sec: 22.85 - lr: 0.012500\n",
      "2022-05-08 17:26:51,528 epoch 105 - iter 185/373 - loss 0.00694634 - samples/sec: 21.50 - lr: 0.012500\n",
      "2022-05-08 17:26:58,876 epoch 105 - iter 222/373 - loss 0.00675585 - samples/sec: 20.34 - lr: 0.012500\n",
      "2022-05-08 17:27:06,183 epoch 105 - iter 259/373 - loss 0.00639844 - samples/sec: 20.44 - lr: 0.012500\n",
      "2022-05-08 17:27:13,176 epoch 105 - iter 296/373 - loss 0.00641202 - samples/sec: 21.37 - lr: 0.012500\n",
      "2022-05-08 17:27:19,883 epoch 105 - iter 333/373 - loss 0.00631827 - samples/sec: 22.30 - lr: 0.012500\n",
      "2022-05-08 17:27:26,390 epoch 105 - iter 370/373 - loss 0.00641199 - samples/sec: 22.98 - lr: 0.012500\n",
      "2022-05-08 17:27:26,965 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:27:26,966 EPOCH 105 done: loss 0.0064 - lr 0.012500\n",
      "2022-05-08 17:27:26,966 BAD EPOCHS (no improvement): 3\n",
      "2022-05-08 17:27:29,334 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:27:29,335 train mode resetting embeddings\n",
      "2022-05-08 17:27:29,341 train mode resetting embeddings\n",
      "2022-05-08 17:27:35,754 epoch 106 - iter 37/373 - loss 0.00396221 - samples/sec: 23.11 - lr: 0.012500\n",
      "2022-05-08 17:27:42,672 epoch 106 - iter 74/373 - loss 0.00454912 - samples/sec: 21.61 - lr: 0.012500\n",
      "2022-05-08 17:27:48,921 epoch 106 - iter 111/373 - loss 0.00502522 - samples/sec: 23.96 - lr: 0.012500\n",
      "2022-05-08 17:27:56,273 epoch 106 - iter 148/373 - loss 0.00630658 - samples/sec: 20.32 - lr: 0.012500\n",
      "2022-05-08 17:28:02,923 epoch 106 - iter 185/373 - loss 0.00671046 - samples/sec: 22.49 - lr: 0.012500\n",
      "2022-05-08 17:28:09,312 epoch 106 - iter 222/373 - loss 0.00676678 - samples/sec: 23.41 - lr: 0.012500\n",
      "2022-05-08 17:28:16,056 epoch 106 - iter 259/373 - loss 0.00677273 - samples/sec: 22.17 - lr: 0.012500\n",
      "2022-05-08 17:28:23,778 epoch 106 - iter 296/373 - loss 0.00631370 - samples/sec: 19.34 - lr: 0.012500\n",
      "2022-05-08 17:28:31,400 epoch 106 - iter 333/373 - loss 0.00617231 - samples/sec: 19.59 - lr: 0.012500\n",
      "2022-05-08 17:28:37,636 epoch 106 - iter 370/373 - loss 0.00610657 - samples/sec: 24.01 - lr: 0.012500\n",
      "2022-05-08 17:28:38,159 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:28:38,159 EPOCH 106 done: loss 0.0061 - lr 0.012500\n",
      "2022-05-08 17:28:38,160 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 17:28:40,262 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:28:40,263 train mode resetting embeddings\n",
      "2022-05-08 17:28:40,269 train mode resetting embeddings\n",
      "2022-05-08 17:28:46,729 epoch 107 - iter 37/373 - loss 0.00488189 - samples/sec: 22.93 - lr: 0.012500\n",
      "2022-05-08 17:28:54,607 epoch 107 - iter 74/373 - loss 0.00573596 - samples/sec: 18.95 - lr: 0.012500\n",
      "2022-05-08 17:29:00,888 epoch 107 - iter 111/373 - loss 0.00586932 - samples/sec: 23.81 - lr: 0.012500\n",
      "2022-05-08 17:29:07,746 epoch 107 - iter 148/373 - loss 0.00569992 - samples/sec: 21.80 - lr: 0.012500\n",
      "2022-05-08 17:29:14,126 epoch 107 - iter 185/373 - loss 0.00527177 - samples/sec: 23.44 - lr: 0.012500\n",
      "2022-05-08 17:29:20,708 epoch 107 - iter 222/373 - loss 0.00568895 - samples/sec: 22.72 - lr: 0.012500\n",
      "2022-05-08 17:29:27,805 epoch 107 - iter 259/373 - loss 0.00600075 - samples/sec: 21.08 - lr: 0.012500\n",
      "2022-05-08 17:29:35,177 epoch 107 - iter 296/373 - loss 0.00616189 - samples/sec: 20.26 - lr: 0.012500\n",
      "2022-05-08 17:29:41,715 epoch 107 - iter 333/373 - loss 0.00616199 - samples/sec: 22.88 - lr: 0.012500\n",
      "2022-05-08 17:29:49,121 epoch 107 - iter 370/373 - loss 0.00597810 - samples/sec: 20.17 - lr: 0.012500\n",
      "2022-05-08 17:29:50,065 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:29:50,067 EPOCH 107 done: loss 0.0060 - lr 0.012500\n",
      "2022-05-08 17:29:50,068 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 17:29:52,435 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:29:52,436 train mode resetting embeddings\n",
      "2022-05-08 17:29:52,442 train mode resetting embeddings\n",
      "2022-05-08 17:29:58,419 epoch 108 - iter 37/373 - loss 0.00846153 - samples/sec: 24.79 - lr: 0.012500\n",
      "2022-05-08 17:30:05,679 epoch 108 - iter 74/373 - loss 0.00667463 - samples/sec: 20.59 - lr: 0.012500\n",
      "2022-05-08 17:30:12,501 epoch 108 - iter 111/373 - loss 0.00631836 - samples/sec: 21.92 - lr: 0.012500\n",
      "2022-05-08 17:30:18,897 epoch 108 - iter 148/373 - loss 0.00603195 - samples/sec: 23.40 - lr: 0.012500\n",
      "2022-05-08 17:30:25,645 epoch 108 - iter 185/373 - loss 0.00607549 - samples/sec: 22.15 - lr: 0.012500\n",
      "2022-05-08 17:30:32,309 epoch 108 - iter 222/373 - loss 0.00563750 - samples/sec: 22.44 - lr: 0.012500\n",
      "2022-05-08 17:30:39,529 epoch 108 - iter 259/373 - loss 0.00597849 - samples/sec: 20.70 - lr: 0.012500\n",
      "2022-05-08 17:30:46,556 epoch 108 - iter 296/373 - loss 0.00575203 - samples/sec: 21.26 - lr: 0.012500\n",
      "2022-05-08 17:30:53,250 epoch 108 - iter 333/373 - loss 0.00562688 - samples/sec: 22.34 - lr: 0.012500\n",
      "2022-05-08 17:30:59,743 epoch 108 - iter 370/373 - loss 0.00579812 - samples/sec: 23.05 - lr: 0.012500\n",
      "2022-05-08 17:31:00,270 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:31:00,271 EPOCH 108 done: loss 0.0058 - lr 0.012500\n",
      "2022-05-08 17:31:00,272 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 17:31:02,413 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:31:02,414 train mode resetting embeddings\n",
      "2022-05-08 17:31:02,420 train mode resetting embeddings\n",
      "2022-05-08 17:31:09,630 epoch 109 - iter 37/373 - loss 0.00782846 - samples/sec: 20.55 - lr: 0.012500\n",
      "2022-05-08 17:31:16,302 epoch 109 - iter 74/373 - loss 0.00686064 - samples/sec: 22.42 - lr: 0.012500\n",
      "2022-05-08 17:31:22,774 epoch 109 - iter 111/373 - loss 0.00640302 - samples/sec: 23.11 - lr: 0.012500\n",
      "2022-05-08 17:31:30,026 epoch 109 - iter 148/373 - loss 0.00614840 - samples/sec: 20.61 - lr: 0.012500\n",
      "2022-05-08 17:31:38,715 epoch 109 - iter 185/373 - loss 0.00590556 - samples/sec: 17.17 - lr: 0.012500\n",
      "2022-05-08 17:31:44,911 epoch 109 - iter 222/373 - loss 0.00581790 - samples/sec: 24.18 - lr: 0.012500\n",
      "2022-05-08 17:31:52,137 epoch 109 - iter 259/373 - loss 0.00578889 - samples/sec: 20.67 - lr: 0.012500\n",
      "2022-05-08 17:31:59,332 epoch 109 - iter 296/373 - loss 0.00598135 - samples/sec: 20.77 - lr: 0.012500\n",
      "2022-05-08 17:32:06,067 epoch 109 - iter 333/373 - loss 0.00587020 - samples/sec: 22.20 - lr: 0.012500\n",
      "2022-05-08 17:32:13,159 epoch 109 - iter 370/373 - loss 0.00571159 - samples/sec: 21.13 - lr: 0.012500\n",
      "2022-05-08 17:32:13,544 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:32:13,545 EPOCH 109 done: loss 0.0057 - lr 0.012500\n",
      "2022-05-08 17:32:13,546 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 17:32:15,948 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:32:15,948 train mode resetting embeddings\n",
      "2022-05-08 17:32:15,955 train mode resetting embeddings\n",
      "2022-05-08 17:32:22,696 epoch 110 - iter 37/373 - loss 0.00641728 - samples/sec: 21.99 - lr: 0.012500\n",
      "2022-05-08 17:32:29,002 epoch 110 - iter 74/373 - loss 0.00650133 - samples/sec: 23.75 - lr: 0.012500\n",
      "2022-05-08 17:32:35,025 epoch 110 - iter 111/373 - loss 0.00499815 - samples/sec: 24.89 - lr: 0.012500\n",
      "2022-05-08 17:32:43,280 epoch 110 - iter 148/373 - loss 0.00535754 - samples/sec: 18.08 - lr: 0.012500\n",
      "2022-05-08 17:32:49,609 epoch 110 - iter 185/373 - loss 0.00545091 - samples/sec: 23.65 - lr: 0.012500\n",
      "2022-05-08 17:32:56,251 epoch 110 - iter 222/373 - loss 0.00534550 - samples/sec: 22.53 - lr: 0.012500\n",
      "2022-05-08 17:33:02,682 epoch 110 - iter 259/373 - loss 0.00535524 - samples/sec: 23.26 - lr: 0.012500\n",
      "2022-05-08 17:33:09,673 epoch 110 - iter 296/373 - loss 0.00542896 - samples/sec: 21.37 - lr: 0.012500\n",
      "2022-05-08 17:33:17,864 epoch 110 - iter 333/373 - loss 0.00517724 - samples/sec: 18.24 - lr: 0.012500\n",
      "2022-05-08 17:33:25,051 epoch 110 - iter 370/373 - loss 0.00549773 - samples/sec: 20.83 - lr: 0.012500\n",
      "2022-05-08 17:33:25,840 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:33:25,841 EPOCH 110 done: loss 0.0056 - lr 0.012500\n",
      "2022-05-08 17:33:25,841 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 17:33:27,946 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:33:27,947 train mode resetting embeddings\n",
      "2022-05-08 17:33:27,952 train mode resetting embeddings\n",
      "2022-05-08 17:33:34,976 epoch 111 - iter 37/373 - loss 0.01021005 - samples/sec: 21.10 - lr: 0.012500\n",
      "2022-05-08 17:33:41,238 epoch 111 - iter 74/373 - loss 0.00851201 - samples/sec: 23.88 - lr: 0.012500\n",
      "2022-05-08 17:33:47,936 epoch 111 - iter 111/373 - loss 0.00780280 - samples/sec: 22.33 - lr: 0.012500\n",
      "2022-05-08 17:33:55,848 epoch 111 - iter 148/373 - loss 0.00784989 - samples/sec: 18.88 - lr: 0.012500\n",
      "2022-05-08 17:34:02,701 epoch 111 - iter 185/373 - loss 0.00760943 - samples/sec: 21.84 - lr: 0.012500\n",
      "2022-05-08 17:34:09,877 epoch 111 - iter 222/373 - loss 0.00712743 - samples/sec: 20.82 - lr: 0.012500\n",
      "2022-05-08 17:34:17,073 epoch 111 - iter 259/373 - loss 0.00676735 - samples/sec: 20.77 - lr: 0.012500\n",
      "2022-05-08 17:34:23,283 epoch 111 - iter 296/373 - loss 0.00653472 - samples/sec: 24.10 - lr: 0.012500\n",
      "2022-05-08 17:34:30,246 epoch 111 - iter 333/373 - loss 0.00621761 - samples/sec: 21.47 - lr: 0.012500\n",
      "2022-05-08 17:34:36,855 epoch 111 - iter 370/373 - loss 0.00594907 - samples/sec: 22.63 - lr: 0.012500\n",
      "2022-05-08 17:34:37,363 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:34:37,364 EPOCH 111 done: loss 0.0059 - lr 0.012500\n",
      "2022-05-08 17:34:37,365 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 17:34:39,782 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:34:39,783 train mode resetting embeddings\n",
      "2022-05-08 17:34:39,790 train mode resetting embeddings\n",
      "2022-05-08 17:34:46,656 epoch 112 - iter 37/373 - loss 0.00724943 - samples/sec: 21.58 - lr: 0.012500\n",
      "2022-05-08 17:34:52,803 epoch 112 - iter 74/373 - loss 0.00633568 - samples/sec: 24.40 - lr: 0.012500\n",
      "2022-05-08 17:35:00,062 epoch 112 - iter 111/373 - loss 0.00670493 - samples/sec: 20.58 - lr: 0.012500\n",
      "2022-05-08 17:35:06,959 epoch 112 - iter 148/373 - loss 0.00643671 - samples/sec: 21.68 - lr: 0.012500\n",
      "2022-05-08 17:35:13,491 epoch 112 - iter 185/373 - loss 0.00637102 - samples/sec: 22.89 - lr: 0.012500\n",
      "2022-05-08 17:35:20,432 epoch 112 - iter 222/373 - loss 0.00667648 - samples/sec: 21.53 - lr: 0.012500\n",
      "2022-05-08 17:35:26,789 epoch 112 - iter 259/373 - loss 0.00633026 - samples/sec: 23.53 - lr: 0.012500\n",
      "2022-05-08 17:35:33,555 epoch 112 - iter 296/373 - loss 0.00650674 - samples/sec: 22.10 - lr: 0.012500\n",
      "2022-05-08 17:35:41,312 epoch 112 - iter 333/373 - loss 0.00638422 - samples/sec: 19.25 - lr: 0.012500\n",
      "2022-05-08 17:35:49,143 epoch 112 - iter 370/373 - loss 0.00626319 - samples/sec: 19.07 - lr: 0.012500\n",
      "2022-05-08 17:35:49,829 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:35:49,830 EPOCH 112 done: loss 0.0063 - lr 0.012500\n",
      "2022-05-08 17:35:49,830 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 17:35:51,942 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:35:51,943 train mode resetting embeddings\n",
      "2022-05-08 17:35:51,949 train mode resetting embeddings\n",
      "2022-05-08 17:35:58,267 epoch 113 - iter 37/373 - loss 0.00673114 - samples/sec: 23.45 - lr: 0.012500\n",
      "2022-05-08 17:36:05,236 epoch 113 - iter 74/373 - loss 0.00634470 - samples/sec: 21.46 - lr: 0.012500\n",
      "2022-05-08 17:36:12,051 epoch 113 - iter 111/373 - loss 0.00581615 - samples/sec: 21.94 - lr: 0.012500\n",
      "2022-05-08 17:36:19,771 epoch 113 - iter 148/373 - loss 0.00533190 - samples/sec: 19.35 - lr: 0.012500\n",
      "2022-05-08 17:36:27,820 epoch 113 - iter 185/373 - loss 0.00599170 - samples/sec: 18.56 - lr: 0.012500\n",
      "2022-05-08 17:36:34,610 epoch 113 - iter 222/373 - loss 0.00605400 - samples/sec: 22.04 - lr: 0.012500\n",
      "2022-05-08 17:36:40,846 epoch 113 - iter 259/373 - loss 0.00609605 - samples/sec: 24.02 - lr: 0.012500\n",
      "2022-05-08 17:36:46,961 epoch 113 - iter 296/373 - loss 0.00606882 - samples/sec: 24.48 - lr: 0.012500\n",
      "2022-05-08 17:36:53,655 epoch 113 - iter 333/373 - loss 0.00604137 - samples/sec: 22.34 - lr: 0.012500\n",
      "2022-05-08 17:37:00,821 epoch 113 - iter 370/373 - loss 0.00589140 - samples/sec: 20.85 - lr: 0.012500\n",
      "2022-05-08 17:37:01,532 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:37:01,533 EPOCH 113 done: loss 0.0059 - lr 0.012500\n",
      "2022-05-08 17:37:01,534 BAD EPOCHS (no improvement): 3\n",
      "2022-05-08 17:37:03,931 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:37:03,932 train mode resetting embeddings\n",
      "2022-05-08 17:37:03,939 train mode resetting embeddings\n",
      "2022-05-08 17:37:11,444 epoch 114 - iter 37/373 - loss 0.00756545 - samples/sec: 19.74 - lr: 0.012500\n",
      "2022-05-08 17:37:18,160 epoch 114 - iter 74/373 - loss 0.00575856 - samples/sec: 22.28 - lr: 0.012500\n",
      "2022-05-08 17:37:25,543 epoch 114 - iter 111/373 - loss 0.00638160 - samples/sec: 20.24 - lr: 0.012500\n",
      "2022-05-08 17:37:32,470 epoch 114 - iter 148/373 - loss 0.00668668 - samples/sec: 21.58 - lr: 0.012500\n",
      "2022-05-08 17:37:39,062 epoch 114 - iter 185/373 - loss 0.00682929 - samples/sec: 22.70 - lr: 0.012500\n",
      "2022-05-08 17:37:46,193 epoch 114 - iter 222/373 - loss 0.00665727 - samples/sec: 20.95 - lr: 0.012500\n",
      "2022-05-08 17:37:52,886 epoch 114 - iter 259/373 - loss 0.00667285 - samples/sec: 22.35 - lr: 0.012500\n",
      "2022-05-08 17:38:00,392 epoch 114 - iter 296/373 - loss 0.00645781 - samples/sec: 19.90 - lr: 0.012500\n",
      "2022-05-08 17:38:06,985 epoch 114 - iter 333/373 - loss 0.00620821 - samples/sec: 22.68 - lr: 0.012500\n",
      "2022-05-08 17:38:12,993 epoch 114 - iter 370/373 - loss 0.00604580 - samples/sec: 24.92 - lr: 0.012500\n",
      "2022-05-08 17:38:13,656 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:38:13,656 EPOCH 114 done: loss 0.0061 - lr 0.012500\n",
      "2022-05-08 17:38:13,657 Epoch   114: reducing learning rate of group 0 to 6.2500e-03.\n",
      "2022-05-08 17:38:13,658 BAD EPOCHS (no improvement): 4\n",
      "2022-05-08 17:38:15,770 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:38:15,771 train mode resetting embeddings\n",
      "2022-05-08 17:38:15,777 train mode resetting embeddings\n",
      "2022-05-08 17:38:22,474 epoch 115 - iter 37/373 - loss 0.00480327 - samples/sec: 22.13 - lr: 0.006250\n",
      "2022-05-08 17:38:29,080 epoch 115 - iter 74/373 - loss 0.00648357 - samples/sec: 22.63 - lr: 0.006250\n",
      "2022-05-08 17:38:35,571 epoch 115 - iter 111/373 - loss 0.00813078 - samples/sec: 23.05 - lr: 0.006250\n",
      "2022-05-08 17:38:42,609 epoch 115 - iter 148/373 - loss 0.00714766 - samples/sec: 21.24 - lr: 0.006250\n",
      "2022-05-08 17:38:49,764 epoch 115 - iter 185/373 - loss 0.00651659 - samples/sec: 20.88 - lr: 0.006250\n",
      "2022-05-08 17:38:56,495 epoch 115 - iter 222/373 - loss 0.00600015 - samples/sec: 22.23 - lr: 0.006250\n",
      "2022-05-08 17:39:03,805 epoch 115 - iter 259/373 - loss 0.00629849 - samples/sec: 20.44 - lr: 0.006250\n",
      "2022-05-08 17:39:11,138 epoch 115 - iter 296/373 - loss 0.00615760 - samples/sec: 20.39 - lr: 0.006250\n",
      "2022-05-08 17:39:18,137 epoch 115 - iter 333/373 - loss 0.00623865 - samples/sec: 21.36 - lr: 0.006250\n",
      "2022-05-08 17:39:24,973 epoch 115 - iter 370/373 - loss 0.00608210 - samples/sec: 21.87 - lr: 0.006250\n",
      "2022-05-08 17:39:25,684 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:39:25,685 EPOCH 115 done: loss 0.0061 - lr 0.006250\n",
      "2022-05-08 17:39:25,686 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 17:39:28,090 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:39:28,092 train mode resetting embeddings\n",
      "2022-05-08 17:39:28,098 train mode resetting embeddings\n",
      "2022-05-08 17:39:36,094 epoch 116 - iter 37/373 - loss 0.00624675 - samples/sec: 18.53 - lr: 0.006250\n",
      "2022-05-08 17:39:42,160 epoch 116 - iter 74/373 - loss 0.00672924 - samples/sec: 24.67 - lr: 0.006250\n",
      "2022-05-08 17:39:48,746 epoch 116 - iter 111/373 - loss 0.00636498 - samples/sec: 22.72 - lr: 0.006250\n",
      "2022-05-08 17:39:55,864 epoch 116 - iter 148/373 - loss 0.00678331 - samples/sec: 21.02 - lr: 0.006250\n",
      "2022-05-08 17:40:02,204 epoch 116 - iter 185/373 - loss 0.00645606 - samples/sec: 23.59 - lr: 0.006250\n",
      "2022-05-08 17:40:09,913 epoch 116 - iter 222/373 - loss 0.00727496 - samples/sec: 19.41 - lr: 0.006250\n",
      "2022-05-08 17:40:17,721 epoch 116 - iter 259/373 - loss 0.00691683 - samples/sec: 19.13 - lr: 0.006250\n",
      "2022-05-08 17:40:25,276 epoch 116 - iter 296/373 - loss 0.00660119 - samples/sec: 19.78 - lr: 0.006250\n",
      "2022-05-08 17:40:32,353 epoch 116 - iter 333/373 - loss 0.00684262 - samples/sec: 21.11 - lr: 0.006250\n",
      "2022-05-08 17:40:38,943 epoch 116 - iter 370/373 - loss 0.00672220 - samples/sec: 22.70 - lr: 0.006250\n",
      "2022-05-08 17:40:39,374 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:40:39,375 EPOCH 116 done: loss 0.0067 - lr 0.006250\n",
      "2022-05-08 17:40:39,376 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 17:40:41,472 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:40:41,473 train mode resetting embeddings\n",
      "2022-05-08 17:40:41,478 train mode resetting embeddings\n",
      "2022-05-08 17:40:49,867 epoch 117 - iter 37/373 - loss 0.00583878 - samples/sec: 17.66 - lr: 0.006250\n",
      "2022-05-08 17:40:56,974 epoch 117 - iter 74/373 - loss 0.00498013 - samples/sec: 21.03 - lr: 0.006250\n",
      "2022-05-08 17:41:04,755 epoch 117 - iter 111/373 - loss 0.00588695 - samples/sec: 19.20 - lr: 0.006250\n",
      "2022-05-08 17:41:10,752 epoch 117 - iter 148/373 - loss 0.00555639 - samples/sec: 25.02 - lr: 0.006250\n",
      "2022-05-08 17:41:17,337 epoch 117 - iter 185/373 - loss 0.00550370 - samples/sec: 22.74 - lr: 0.006250\n",
      "2022-05-08 17:41:23,677 epoch 117 - iter 222/373 - loss 0.00546156 - samples/sec: 23.60 - lr: 0.006250\n",
      "2022-05-08 17:41:31,333 epoch 117 - iter 259/373 - loss 0.00538358 - samples/sec: 19.51 - lr: 0.006250\n",
      "2022-05-08 17:41:38,440 epoch 117 - iter 296/373 - loss 0.00508395 - samples/sec: 21.03 - lr: 0.006250\n",
      "2022-05-08 17:41:45,270 epoch 117 - iter 333/373 - loss 0.00525102 - samples/sec: 21.93 - lr: 0.006250\n",
      "2022-05-08 17:41:53,105 epoch 117 - iter 370/373 - loss 0.00523903 - samples/sec: 19.06 - lr: 0.006250\n",
      "2022-05-08 17:41:53,696 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:41:53,696 EPOCH 117 done: loss 0.0052 - lr 0.006250\n",
      "2022-05-08 17:41:53,697 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 17:41:56,059 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:41:56,060 train mode resetting embeddings\n",
      "2022-05-08 17:41:56,065 train mode resetting embeddings\n",
      "2022-05-08 17:42:03,205 epoch 118 - iter 37/373 - loss 0.00461462 - samples/sec: 20.75 - lr: 0.006250\n",
      "2022-05-08 17:42:09,984 epoch 118 - iter 74/373 - loss 0.00490969 - samples/sec: 22.06 - lr: 0.006250\n",
      "2022-05-08 17:42:16,038 epoch 118 - iter 111/373 - loss 0.00489906 - samples/sec: 24.75 - lr: 0.006250\n",
      "2022-05-08 17:42:23,402 epoch 118 - iter 148/373 - loss 0.00528387 - samples/sec: 20.32 - lr: 0.006250\n",
      "2022-05-08 17:42:30,441 epoch 118 - iter 185/373 - loss 0.00519728 - samples/sec: 21.23 - lr: 0.006250\n",
      "2022-05-08 17:42:37,458 epoch 118 - iter 222/373 - loss 0.00519572 - samples/sec: 21.30 - lr: 0.006250\n",
      "2022-05-08 17:42:44,235 epoch 118 - iter 259/373 - loss 0.00536323 - samples/sec: 22.06 - lr: 0.006250\n",
      "2022-05-08 17:42:50,825 epoch 118 - iter 296/373 - loss 0.00527394 - samples/sec: 22.69 - lr: 0.006250\n",
      "2022-05-08 17:42:58,116 epoch 118 - iter 333/373 - loss 0.00511769 - samples/sec: 20.49 - lr: 0.006250\n",
      "2022-05-08 17:43:05,120 epoch 118 - iter 370/373 - loss 0.00528482 - samples/sec: 21.34 - lr: 0.006250\n",
      "2022-05-08 17:43:05,895 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:43:05,896 EPOCH 118 done: loss 0.0053 - lr 0.006250\n",
      "2022-05-08 17:43:05,896 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 17:43:07,988 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:43:07,989 train mode resetting embeddings\n",
      "2022-05-08 17:43:07,995 train mode resetting embeddings\n",
      "2022-05-08 17:43:14,336 epoch 119 - iter 37/373 - loss 0.00510245 - samples/sec: 23.37 - lr: 0.006250\n",
      "2022-05-08 17:43:21,576 epoch 119 - iter 74/373 - loss 0.00467028 - samples/sec: 20.66 - lr: 0.006250\n",
      "2022-05-08 17:43:28,385 epoch 119 - iter 111/373 - loss 0.00428041 - samples/sec: 21.98 - lr: 0.006250\n",
      "2022-05-08 17:43:35,527 epoch 119 - iter 148/373 - loss 0.00446414 - samples/sec: 20.92 - lr: 0.006250\n",
      "2022-05-08 17:43:41,952 epoch 119 - iter 185/373 - loss 0.00460571 - samples/sec: 23.29 - lr: 0.006250\n",
      "2022-05-08 17:43:48,834 epoch 119 - iter 222/373 - loss 0.00473577 - samples/sec: 21.73 - lr: 0.006250\n",
      "2022-05-08 17:43:55,372 epoch 119 - iter 259/373 - loss 0.00496107 - samples/sec: 22.87 - lr: 0.006250\n",
      "2022-05-08 17:44:02,323 epoch 119 - iter 296/373 - loss 0.00522773 - samples/sec: 21.51 - lr: 0.006250\n",
      "2022-05-08 17:44:08,946 epoch 119 - iter 333/373 - loss 0.00540328 - samples/sec: 22.58 - lr: 0.006250\n",
      "2022-05-08 17:44:16,766 epoch 119 - iter 370/373 - loss 0.00519596 - samples/sec: 19.10 - lr: 0.006250\n",
      "2022-05-08 17:44:17,467 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:44:17,469 EPOCH 119 done: loss 0.0052 - lr 0.006250\n",
      "2022-05-08 17:44:17,469 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 17:44:19,894 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:44:19,895 train mode resetting embeddings\n",
      "2022-05-08 17:44:19,901 train mode resetting embeddings\n",
      "2022-05-08 17:44:26,757 epoch 120 - iter 37/373 - loss 0.00270391 - samples/sec: 21.61 - lr: 0.006250\n",
      "2022-05-08 17:44:34,777 epoch 120 - iter 74/373 - loss 0.00709226 - samples/sec: 18.61 - lr: 0.006250\n",
      "2022-05-08 17:44:41,421 epoch 120 - iter 111/373 - loss 0.00725217 - samples/sec: 22.52 - lr: 0.006250\n",
      "2022-05-08 17:44:48,846 epoch 120 - iter 148/373 - loss 0.00665582 - samples/sec: 20.12 - lr: 0.006250\n",
      "2022-05-08 17:44:54,924 epoch 120 - iter 185/373 - loss 0.00651327 - samples/sec: 24.66 - lr: 0.006250\n",
      "2022-05-08 17:45:01,683 epoch 120 - iter 222/373 - loss 0.00652252 - samples/sec: 22.14 - lr: 0.006250\n",
      "2022-05-08 17:45:07,278 epoch 120 - iter 259/373 - loss 0.00653660 - samples/sec: 26.82 - lr: 0.006250\n",
      "2022-05-08 17:45:14,506 epoch 120 - iter 296/373 - loss 0.00645020 - samples/sec: 20.67 - lr: 0.006250\n",
      "2022-05-08 17:45:22,282 epoch 120 - iter 333/373 - loss 0.00607210 - samples/sec: 19.22 - lr: 0.006250\n",
      "2022-05-08 17:45:29,806 epoch 120 - iter 370/373 - loss 0.00604635 - samples/sec: 19.85 - lr: 0.006250\n",
      "2022-05-08 17:45:30,395 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:45:30,395 EPOCH 120 done: loss 0.0061 - lr 0.006250\n",
      "2022-05-08 17:45:30,397 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 17:45:32,474 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:45:32,475 train mode resetting embeddings\n",
      "2022-05-08 17:45:32,480 train mode resetting embeddings\n",
      "2022-05-08 17:45:39,385 epoch 121 - iter 37/373 - loss 0.00713023 - samples/sec: 21.46 - lr: 0.006250\n",
      "2022-05-08 17:45:46,453 epoch 121 - iter 74/373 - loss 0.00724872 - samples/sec: 21.14 - lr: 0.006250\n",
      "2022-05-08 17:45:53,637 epoch 121 - iter 111/373 - loss 0.00677193 - samples/sec: 20.80 - lr: 0.006250\n",
      "2022-05-08 17:45:59,795 epoch 121 - iter 148/373 - loss 0.00566805 - samples/sec: 24.31 - lr: 0.006250\n",
      "2022-05-08 17:46:06,579 epoch 121 - iter 185/373 - loss 0.00555243 - samples/sec: 22.04 - lr: 0.006250\n",
      "2022-05-08 17:46:12,898 epoch 121 - iter 222/373 - loss 0.00580424 - samples/sec: 23.68 - lr: 0.006250\n",
      "2022-05-08 17:46:19,282 epoch 121 - iter 259/373 - loss 0.00592160 - samples/sec: 23.45 - lr: 0.006250\n",
      "2022-05-08 17:46:26,413 epoch 121 - iter 296/373 - loss 0.00558308 - samples/sec: 20.97 - lr: 0.006250\n",
      "2022-05-08 17:46:33,567 epoch 121 - iter 333/373 - loss 0.00543972 - samples/sec: 20.88 - lr: 0.006250\n",
      "2022-05-08 17:46:41,380 epoch 121 - iter 370/373 - loss 0.00549425 - samples/sec: 19.11 - lr: 0.006250\n",
      "2022-05-08 17:46:42,043 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:46:42,045 EPOCH 121 done: loss 0.0055 - lr 0.006250\n",
      "2022-05-08 17:46:42,046 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 17:46:44,143 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:46:44,143 train mode resetting embeddings\n",
      "2022-05-08 17:46:44,149 train mode resetting embeddings\n",
      "2022-05-08 17:46:51,245 epoch 122 - iter 37/373 - loss 0.00449402 - samples/sec: 20.88 - lr: 0.006250\n",
      "2022-05-08 17:46:57,984 epoch 122 - iter 74/373 - loss 0.00490833 - samples/sec: 22.19 - lr: 0.006250\n",
      "2022-05-08 17:47:05,004 epoch 122 - iter 111/373 - loss 0.00534999 - samples/sec: 21.29 - lr: 0.006250\n",
      "2022-05-08 17:47:11,359 epoch 122 - iter 148/373 - loss 0.00522593 - samples/sec: 23.55 - lr: 0.006250\n",
      "2022-05-08 17:47:18,942 epoch 122 - iter 185/373 - loss 0.00516283 - samples/sec: 19.70 - lr: 0.006250\n",
      "2022-05-08 17:47:25,726 epoch 122 - iter 222/373 - loss 0.00516288 - samples/sec: 22.03 - lr: 0.006250\n",
      "2022-05-08 17:47:32,929 epoch 122 - iter 259/373 - loss 0.00539340 - samples/sec: 20.74 - lr: 0.006250\n",
      "2022-05-08 17:47:40,419 epoch 122 - iter 296/373 - loss 0.00527136 - samples/sec: 19.95 - lr: 0.006250\n",
      "2022-05-08 17:47:47,276 epoch 122 - iter 333/373 - loss 0.00544709 - samples/sec: 21.82 - lr: 0.006250\n",
      "2022-05-08 17:47:54,133 epoch 122 - iter 370/373 - loss 0.00544467 - samples/sec: 21.82 - lr: 0.006250\n",
      "2022-05-08 17:47:54,890 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:47:54,891 EPOCH 122 done: loss 0.0054 - lr 0.006250\n",
      "2022-05-08 17:47:54,892 BAD EPOCHS (no improvement): 3\n",
      "2022-05-08 17:47:57,321 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:47:57,322 train mode resetting embeddings\n",
      "2022-05-08 17:47:57,328 train mode resetting embeddings\n",
      "2022-05-08 17:48:03,731 epoch 123 - iter 37/373 - loss 0.00526407 - samples/sec: 23.15 - lr: 0.006250\n",
      "2022-05-08 17:48:10,670 epoch 123 - iter 74/373 - loss 0.00518566 - samples/sec: 21.54 - lr: 0.006250\n",
      "2022-05-08 17:48:18,257 epoch 123 - iter 111/373 - loss 0.00534820 - samples/sec: 19.68 - lr: 0.006250\n",
      "2022-05-08 17:48:24,830 epoch 123 - iter 148/373 - loss 0.00515546 - samples/sec: 22.77 - lr: 0.006250\n",
      "2022-05-08 17:48:32,268 epoch 123 - iter 185/373 - loss 0.00520378 - samples/sec: 20.08 - lr: 0.006250\n",
      "2022-05-08 17:48:39,506 epoch 123 - iter 222/373 - loss 0.00509436 - samples/sec: 20.64 - lr: 0.006250\n",
      "2022-05-08 17:48:46,429 epoch 123 - iter 259/373 - loss 0.00529959 - samples/sec: 21.59 - lr: 0.006250\n",
      "2022-05-08 17:48:52,996 epoch 123 - iter 296/373 - loss 0.00539065 - samples/sec: 22.77 - lr: 0.006250\n",
      "2022-05-08 17:48:59,858 epoch 123 - iter 333/373 - loss 0.00525165 - samples/sec: 21.78 - lr: 0.006250\n",
      "2022-05-08 17:49:06,958 epoch 123 - iter 370/373 - loss 0.00530442 - samples/sec: 21.04 - lr: 0.006250\n",
      "2022-05-08 17:49:07,504 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:49:07,505 EPOCH 123 done: loss 0.0053 - lr 0.006250\n",
      "2022-05-08 17:49:07,506 Epoch   123: reducing learning rate of group 0 to 3.1250e-03.\n",
      "2022-05-08 17:49:07,506 BAD EPOCHS (no improvement): 4\n",
      "2022-05-08 17:49:09,611 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:49:09,612 train mode resetting embeddings\n",
      "2022-05-08 17:49:09,617 train mode resetting embeddings\n",
      "2022-05-08 17:49:16,509 epoch 124 - iter 37/373 - loss 0.00587485 - samples/sec: 21.49 - lr: 0.003125\n",
      "2022-05-08 17:49:23,610 epoch 124 - iter 74/373 - loss 0.00605357 - samples/sec: 21.06 - lr: 0.003125\n",
      "2022-05-08 17:49:29,911 epoch 124 - iter 111/373 - loss 0.00540728 - samples/sec: 23.76 - lr: 0.003125\n",
      "2022-05-08 17:49:37,014 epoch 124 - iter 148/373 - loss 0.00488417 - samples/sec: 21.03 - lr: 0.003125\n",
      "2022-05-08 17:49:44,212 epoch 124 - iter 185/373 - loss 0.00487109 - samples/sec: 20.79 - lr: 0.003125\n",
      "2022-05-08 17:49:50,825 epoch 124 - iter 222/373 - loss 0.00466490 - samples/sec: 22.62 - lr: 0.003125\n",
      "2022-05-08 17:49:58,322 epoch 124 - iter 259/373 - loss 0.00497873 - samples/sec: 19.92 - lr: 0.003125\n",
      "2022-05-08 17:50:05,443 epoch 124 - iter 296/373 - loss 0.00517045 - samples/sec: 20.99 - lr: 0.003125\n",
      "2022-05-08 17:50:12,849 epoch 124 - iter 333/373 - loss 0.00491511 - samples/sec: 20.16 - lr: 0.003125\n",
      "2022-05-08 17:50:19,327 epoch 124 - iter 370/373 - loss 0.00510744 - samples/sec: 23.09 - lr: 0.003125\n",
      "2022-05-08 17:50:20,248 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:50:20,249 EPOCH 124 done: loss 0.0051 - lr 0.003125\n",
      "2022-05-08 17:50:20,250 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 17:50:22,606 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:50:22,607 train mode resetting embeddings\n",
      "2022-05-08 17:50:22,613 train mode resetting embeddings\n",
      "2022-05-08 17:50:29,440 epoch 125 - iter 37/373 - loss 0.00579362 - samples/sec: 21.71 - lr: 0.003125\n",
      "2022-05-08 17:50:37,192 epoch 125 - iter 74/373 - loss 0.00579431 - samples/sec: 19.25 - lr: 0.003125\n",
      "2022-05-08 17:50:43,982 epoch 125 - iter 111/373 - loss 0.00630371 - samples/sec: 22.01 - lr: 0.003125\n",
      "2022-05-08 17:50:50,168 epoch 125 - iter 148/373 - loss 0.00625478 - samples/sec: 24.19 - lr: 0.003125\n",
      "2022-05-08 17:50:56,112 epoch 125 - iter 185/373 - loss 0.00608377 - samples/sec: 25.20 - lr: 0.003125\n",
      "2022-05-08 17:51:02,671 epoch 125 - iter 222/373 - loss 0.00559637 - samples/sec: 22.81 - lr: 0.003125\n",
      "2022-05-08 17:51:09,547 epoch 125 - iter 259/373 - loss 0.00574888 - samples/sec: 21.73 - lr: 0.003125\n",
      "2022-05-08 17:51:17,589 epoch 125 - iter 296/373 - loss 0.00597098 - samples/sec: 18.56 - lr: 0.003125\n",
      "2022-05-08 17:51:24,122 epoch 125 - iter 333/373 - loss 0.00591243 - samples/sec: 22.89 - lr: 0.003125\n",
      "2022-05-08 17:51:32,332 epoch 125 - iter 370/373 - loss 0.00602112 - samples/sec: 18.17 - lr: 0.003125\n",
      "2022-05-08 17:51:32,639 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:51:32,640 EPOCH 125 done: loss 0.0060 - lr 0.003125\n",
      "2022-05-08 17:51:32,641 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 17:51:34,709 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:51:34,710 train mode resetting embeddings\n",
      "2022-05-08 17:51:34,716 train mode resetting embeddings\n",
      "2022-05-08 17:51:42,213 epoch 126 - iter 37/373 - loss 0.00641317 - samples/sec: 19.76 - lr: 0.003125\n",
      "2022-05-08 17:51:49,297 epoch 126 - iter 74/373 - loss 0.00547996 - samples/sec: 21.09 - lr: 0.003125\n",
      "2022-05-08 17:51:55,502 epoch 126 - iter 111/373 - loss 0.00540332 - samples/sec: 24.10 - lr: 0.003125\n",
      "2022-05-08 17:52:02,300 epoch 126 - iter 148/373 - loss 0.00538786 - samples/sec: 22.00 - lr: 0.003125\n",
      "2022-05-08 17:52:08,869 epoch 126 - iter 185/373 - loss 0.00508164 - samples/sec: 22.77 - lr: 0.003125\n",
      "2022-05-08 17:52:16,975 epoch 126 - iter 222/373 - loss 0.00518691 - samples/sec: 18.42 - lr: 0.003125\n",
      "2022-05-08 17:52:24,428 epoch 126 - iter 259/373 - loss 0.00538018 - samples/sec: 20.08 - lr: 0.003125\n",
      "2022-05-08 17:52:33,397 epoch 126 - iter 296/373 - loss 0.00541252 - samples/sec: 16.65 - lr: 0.003125\n",
      "2022-05-08 17:52:41,319 epoch 126 - iter 333/373 - loss 0.00545180 - samples/sec: 18.87 - lr: 0.003125\n",
      "2022-05-08 17:52:49,199 epoch 126 - iter 370/373 - loss 0.00542365 - samples/sec: 18.96 - lr: 0.003125\n",
      "2022-05-08 17:52:50,145 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:52:50,146 EPOCH 126 done: loss 0.0054 - lr 0.003125\n",
      "2022-05-08 17:52:50,146 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 17:52:52,920 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:52:52,921 train mode resetting embeddings\n",
      "2022-05-08 17:52:52,927 train mode resetting embeddings\n",
      "2022-05-08 17:53:01,210 epoch 127 - iter 37/373 - loss 0.00623875 - samples/sec: 17.89 - lr: 0.003125\n",
      "2022-05-08 17:53:08,220 epoch 127 - iter 74/373 - loss 0.00475787 - samples/sec: 21.35 - lr: 0.003125\n",
      "2022-05-08 17:53:16,128 epoch 127 - iter 111/373 - loss 0.00532099 - samples/sec: 18.91 - lr: 0.003125\n",
      "2022-05-08 17:53:24,907 epoch 127 - iter 148/373 - loss 0.00500911 - samples/sec: 17.01 - lr: 0.003125\n",
      "2022-05-08 17:53:32,672 epoch 127 - iter 185/373 - loss 0.00521392 - samples/sec: 19.26 - lr: 0.003125\n",
      "2022-05-08 17:53:40,601 epoch 127 - iter 222/373 - loss 0.00527499 - samples/sec: 18.87 - lr: 0.003125\n",
      "2022-05-08 17:53:48,188 epoch 127 - iter 259/373 - loss 0.00491358 - samples/sec: 19.73 - lr: 0.003125\n",
      "2022-05-08 17:53:56,866 epoch 127 - iter 296/373 - loss 0.00509789 - samples/sec: 17.22 - lr: 0.003125\n",
      "2022-05-08 17:54:04,127 epoch 127 - iter 333/373 - loss 0.00495505 - samples/sec: 20.58 - lr: 0.003125\n",
      "2022-05-08 17:54:11,367 epoch 127 - iter 370/373 - loss 0.00501860 - samples/sec: 20.72 - lr: 0.003125\n",
      "2022-05-08 17:54:12,014 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:54:12,015 EPOCH 127 done: loss 0.0050 - lr 0.003125\n",
      "2022-05-08 17:54:12,016 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 17:54:14,085 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:54:14,087 train mode resetting embeddings\n",
      "2022-05-08 17:54:14,093 train mode resetting embeddings\n",
      "2022-05-08 17:54:20,659 epoch 128 - iter 37/373 - loss 0.00312855 - samples/sec: 22.56 - lr: 0.003125\n",
      "2022-05-08 17:54:28,731 epoch 128 - iter 74/373 - loss 0.00635248 - samples/sec: 18.49 - lr: 0.003125\n",
      "2022-05-08 17:54:35,354 epoch 128 - iter 111/373 - loss 0.00636502 - samples/sec: 22.64 - lr: 0.003125\n",
      "2022-05-08 17:54:42,080 epoch 128 - iter 148/373 - loss 0.00549402 - samples/sec: 22.23 - lr: 0.003125\n",
      "2022-05-08 17:54:48,585 epoch 128 - iter 185/373 - loss 0.00474757 - samples/sec: 23.00 - lr: 0.003125\n",
      "2022-05-08 17:54:56,103 epoch 128 - iter 222/373 - loss 0.00484069 - samples/sec: 19.88 - lr: 0.003125\n",
      "2022-05-08 17:55:03,429 epoch 128 - iter 259/373 - loss 0.00466957 - samples/sec: 20.39 - lr: 0.003125\n",
      "2022-05-08 17:55:11,043 epoch 128 - iter 296/373 - loss 0.00471168 - samples/sec: 19.62 - lr: 0.003125\n",
      "2022-05-08 17:55:17,850 epoch 128 - iter 333/373 - loss 0.00480895 - samples/sec: 21.96 - lr: 0.003125\n",
      "2022-05-08 17:55:25,020 epoch 128 - iter 370/373 - loss 0.00471810 - samples/sec: 20.84 - lr: 0.003125\n",
      "2022-05-08 17:55:25,574 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:55:25,575 EPOCH 128 done: loss 0.0047 - lr 0.003125\n",
      "2022-05-08 17:55:25,575 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 17:55:28,013 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:55:28,014 train mode resetting embeddings\n",
      "2022-05-08 17:55:28,020 train mode resetting embeddings\n",
      "2022-05-08 17:55:34,565 epoch 129 - iter 37/373 - loss 0.00642097 - samples/sec: 22.64 - lr: 0.003125\n",
      "2022-05-08 17:55:41,184 epoch 129 - iter 74/373 - loss 0.00744551 - samples/sec: 22.60 - lr: 0.003125\n",
      "2022-05-08 17:55:48,865 epoch 129 - iter 111/373 - loss 0.00778807 - samples/sec: 19.46 - lr: 0.003125\n",
      "2022-05-08 17:55:56,331 epoch 129 - iter 148/373 - loss 0.00680166 - samples/sec: 20.00 - lr: 0.003125\n",
      "2022-05-08 17:56:03,156 epoch 129 - iter 185/373 - loss 0.00660741 - samples/sec: 21.93 - lr: 0.003125\n",
      "2022-05-08 17:56:10,606 epoch 129 - iter 222/373 - loss 0.00655805 - samples/sec: 20.05 - lr: 0.003125\n",
      "2022-05-08 17:56:16,907 epoch 129 - iter 259/373 - loss 0.00650996 - samples/sec: 23.74 - lr: 0.003125\n",
      "2022-05-08 17:56:22,689 epoch 129 - iter 296/373 - loss 0.00661028 - samples/sec: 25.90 - lr: 0.003125\n",
      "2022-05-08 17:56:30,474 epoch 129 - iter 333/373 - loss 0.00655131 - samples/sec: 19.18 - lr: 0.003125\n",
      "2022-05-08 17:56:37,087 epoch 129 - iter 370/373 - loss 0.00641381 - samples/sec: 22.61 - lr: 0.003125\n",
      "2022-05-08 17:56:37,855 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:56:37,856 EPOCH 129 done: loss 0.0064 - lr 0.003125\n",
      "2022-05-08 17:56:37,858 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 17:56:39,962 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:56:39,962 train mode resetting embeddings\n",
      "2022-05-08 17:56:39,968 train mode resetting embeddings\n",
      "2022-05-08 17:56:47,763 epoch 130 - iter 37/373 - loss 0.00411004 - samples/sec: 19.00 - lr: 0.003125\n",
      "2022-05-08 17:56:54,503 epoch 130 - iter 74/373 - loss 0.00470723 - samples/sec: 22.18 - lr: 0.003125\n",
      "2022-05-08 17:57:02,662 epoch 130 - iter 111/373 - loss 0.00461223 - samples/sec: 18.29 - lr: 0.003125\n",
      "2022-05-08 17:57:09,161 epoch 130 - iter 148/373 - loss 0.00478202 - samples/sec: 23.03 - lr: 0.003125\n",
      "2022-05-08 17:57:16,209 epoch 130 - iter 185/373 - loss 0.00514076 - samples/sec: 21.21 - lr: 0.003125\n",
      "2022-05-08 17:57:23,281 epoch 130 - iter 222/373 - loss 0.00524121 - samples/sec: 21.13 - lr: 0.003125\n",
      "2022-05-08 17:57:29,980 epoch 130 - iter 259/373 - loss 0.00536052 - samples/sec: 22.31 - lr: 0.003125\n",
      "2022-05-08 17:57:36,068 epoch 130 - iter 296/373 - loss 0.00551523 - samples/sec: 24.58 - lr: 0.003125\n",
      "2022-05-08 17:57:42,784 epoch 130 - iter 333/373 - loss 0.00536197 - samples/sec: 22.27 - lr: 0.003125\n",
      "2022-05-08 17:57:49,305 epoch 130 - iter 370/373 - loss 0.00559040 - samples/sec: 22.94 - lr: 0.003125\n",
      "2022-05-08 17:57:49,770 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:57:49,771 EPOCH 130 done: loss 0.0056 - lr 0.003125\n",
      "2022-05-08 17:57:49,772 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 17:57:52,213 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:57:52,214 train mode resetting embeddings\n",
      "2022-05-08 17:57:52,220 train mode resetting embeddings\n",
      "2022-05-08 17:57:58,200 epoch 131 - iter 37/373 - loss 0.00484002 - samples/sec: 24.78 - lr: 0.003125\n",
      "2022-05-08 17:58:06,368 epoch 131 - iter 74/373 - loss 0.00420236 - samples/sec: 18.27 - lr: 0.003125\n",
      "2022-05-08 17:58:13,699 epoch 131 - iter 111/373 - loss 0.00471104 - samples/sec: 20.38 - lr: 0.003125\n",
      "2022-05-08 17:58:21,823 epoch 131 - iter 148/373 - loss 0.00512284 - samples/sec: 18.38 - lr: 0.003125\n",
      "2022-05-08 17:58:28,248 epoch 131 - iter 185/373 - loss 0.00464553 - samples/sec: 23.28 - lr: 0.003125\n",
      "2022-05-08 17:58:35,287 epoch 131 - iter 222/373 - loss 0.00441990 - samples/sec: 21.23 - lr: 0.003125\n",
      "2022-05-08 17:58:40,678 epoch 131 - iter 259/373 - loss 0.00435766 - samples/sec: 27.80 - lr: 0.003125\n",
      "2022-05-08 17:58:47,150 epoch 131 - iter 296/373 - loss 0.00429167 - samples/sec: 23.11 - lr: 0.003125\n",
      "2022-05-08 17:58:55,012 epoch 131 - iter 333/373 - loss 0.00450430 - samples/sec: 18.99 - lr: 0.003125\n",
      "2022-05-08 17:59:02,366 epoch 131 - iter 370/373 - loss 0.00467755 - samples/sec: 20.31 - lr: 0.003125\n",
      "2022-05-08 17:59:03,050 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:59:03,051 EPOCH 131 done: loss 0.0047 - lr 0.003125\n",
      "2022-05-08 17:59:03,052 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 17:59:05,137 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 17:59:05,138 train mode resetting embeddings\n",
      "2022-05-08 17:59:05,143 train mode resetting embeddings\n",
      "2022-05-08 17:59:12,713 epoch 132 - iter 37/373 - loss 0.00348670 - samples/sec: 19.57 - lr: 0.003125\n",
      "2022-05-08 17:59:20,304 epoch 132 - iter 74/373 - loss 0.00387927 - samples/sec: 19.70 - lr: 0.003125\n",
      "2022-05-08 17:59:27,730 epoch 132 - iter 111/373 - loss 0.00422727 - samples/sec: 20.12 - lr: 0.003125\n",
      "2022-05-08 17:59:35,405 epoch 132 - iter 148/373 - loss 0.00434920 - samples/sec: 19.47 - lr: 0.003125\n",
      "2022-05-08 17:59:42,500 epoch 132 - iter 185/373 - loss 0.00486475 - samples/sec: 21.06 - lr: 0.003125\n",
      "2022-05-08 17:59:49,413 epoch 132 - iter 222/373 - loss 0.00492825 - samples/sec: 21.62 - lr: 0.003125\n",
      "2022-05-08 17:59:56,600 epoch 132 - iter 259/373 - loss 0.00544216 - samples/sec: 20.78 - lr: 0.003125\n",
      "2022-05-08 18:00:03,258 epoch 132 - iter 296/373 - loss 0.00546809 - samples/sec: 22.47 - lr: 0.003125\n",
      "2022-05-08 18:00:09,002 epoch 132 - iter 333/373 - loss 0.00575249 - samples/sec: 26.07 - lr: 0.003125\n",
      "2022-05-08 18:00:14,305 epoch 132 - iter 370/373 - loss 0.00558082 - samples/sec: 28.27 - lr: 0.003125\n",
      "2022-05-08 18:00:15,087 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:00:15,088 EPOCH 132 done: loss 0.0056 - lr 0.003125\n",
      "2022-05-08 18:00:15,089 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 18:00:17,404 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:00:17,405 train mode resetting embeddings\n",
      "2022-05-08 18:00:17,410 train mode resetting embeddings\n",
      "2022-05-08 18:00:25,630 epoch 133 - iter 37/373 - loss 0.00554144 - samples/sec: 18.02 - lr: 0.003125\n",
      "2022-05-08 18:00:32,269 epoch 133 - iter 74/373 - loss 0.00622400 - samples/sec: 22.54 - lr: 0.003125\n",
      "2022-05-08 18:00:38,833 epoch 133 - iter 111/373 - loss 0.00647518 - samples/sec: 22.78 - lr: 0.003125\n",
      "2022-05-08 18:00:46,513 epoch 133 - iter 148/373 - loss 0.00618152 - samples/sec: 19.46 - lr: 0.003125\n",
      "2022-05-08 18:00:53,257 epoch 133 - iter 185/373 - loss 0.00596701 - samples/sec: 22.16 - lr: 0.003125\n",
      "2022-05-08 18:00:59,817 epoch 133 - iter 222/373 - loss 0.00579350 - samples/sec: 22.79 - lr: 0.003125\n",
      "2022-05-08 18:01:06,335 epoch 133 - iter 259/373 - loss 0.00576773 - samples/sec: 22.95 - lr: 0.003125\n",
      "2022-05-08 18:01:12,796 epoch 133 - iter 296/373 - loss 0.00584953 - samples/sec: 23.16 - lr: 0.003125\n",
      "2022-05-08 18:01:19,485 epoch 133 - iter 333/373 - loss 0.00559298 - samples/sec: 22.38 - lr: 0.003125\n",
      "2022-05-08 18:01:26,170 epoch 133 - iter 370/373 - loss 0.00545286 - samples/sec: 22.36 - lr: 0.003125\n",
      "2022-05-08 18:01:26,573 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:01:26,575 EPOCH 133 done: loss 0.0054 - lr 0.003125\n",
      "2022-05-08 18:01:26,576 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 18:01:28,717 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:01:28,718 train mode resetting embeddings\n",
      "2022-05-08 18:01:28,723 train mode resetting embeddings\n",
      "2022-05-08 18:01:36,135 epoch 134 - iter 37/373 - loss 0.00285873 - samples/sec: 19.99 - lr: 0.003125\n",
      "2022-05-08 18:01:43,562 epoch 134 - iter 74/373 - loss 0.00353538 - samples/sec: 20.11 - lr: 0.003125\n",
      "2022-05-08 18:01:50,581 epoch 134 - iter 111/373 - loss 0.00513463 - samples/sec: 21.28 - lr: 0.003125\n",
      "2022-05-08 18:01:57,245 epoch 134 - iter 148/373 - loss 0.00515350 - samples/sec: 22.44 - lr: 0.003125\n",
      "2022-05-08 18:02:03,688 epoch 134 - iter 185/373 - loss 0.00500449 - samples/sec: 23.22 - lr: 0.003125\n",
      "2022-05-08 18:02:10,357 epoch 134 - iter 222/373 - loss 0.00524326 - samples/sec: 22.41 - lr: 0.003125\n",
      "2022-05-08 18:02:17,584 epoch 134 - iter 259/373 - loss 0.00526832 - samples/sec: 20.67 - lr: 0.003125\n",
      "2022-05-08 18:02:25,449 epoch 134 - iter 296/373 - loss 0.00514417 - samples/sec: 18.99 - lr: 0.003125\n",
      "2022-05-08 18:02:32,302 epoch 134 - iter 333/373 - loss 0.00482621 - samples/sec: 21.81 - lr: 0.003125\n",
      "2022-05-08 18:02:40,386 epoch 134 - iter 370/373 - loss 0.00476455 - samples/sec: 18.48 - lr: 0.003125\n",
      "2022-05-08 18:02:40,889 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:02:40,890 EPOCH 134 done: loss 0.0048 - lr 0.003125\n",
      "2022-05-08 18:02:40,891 BAD EPOCHS (no improvement): 3\n",
      "2022-05-08 18:02:43,592 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:02:43,593 train mode resetting embeddings\n",
      "2022-05-08 18:02:43,600 train mode resetting embeddings\n",
      "2022-05-08 18:02:50,959 epoch 135 - iter 37/373 - loss 0.00533348 - samples/sec: 20.13 - lr: 0.003125\n",
      "2022-05-08 18:02:58,684 epoch 135 - iter 74/373 - loss 0.00570335 - samples/sec: 19.35 - lr: 0.003125\n",
      "2022-05-08 18:03:06,803 epoch 135 - iter 111/373 - loss 0.00555847 - samples/sec: 18.41 - lr: 0.003125\n",
      "2022-05-08 18:03:14,572 epoch 135 - iter 148/373 - loss 0.00547190 - samples/sec: 19.25 - lr: 0.003125\n",
      "2022-05-08 18:03:23,453 epoch 135 - iter 185/373 - loss 0.00534878 - samples/sec: 16.81 - lr: 0.003125\n",
      "2022-05-08 18:03:31,180 epoch 135 - iter 222/373 - loss 0.00524814 - samples/sec: 19.35 - lr: 0.003125\n",
      "2022-05-08 18:03:38,402 epoch 135 - iter 259/373 - loss 0.00537379 - samples/sec: 20.73 - lr: 0.003125\n",
      "2022-05-08 18:03:45,994 epoch 135 - iter 296/373 - loss 0.00529249 - samples/sec: 19.68 - lr: 0.003125\n",
      "2022-05-08 18:03:53,773 epoch 135 - iter 333/373 - loss 0.00539203 - samples/sec: 19.23 - lr: 0.003125\n",
      "2022-05-08 18:04:02,324 epoch 135 - iter 370/373 - loss 0.00542779 - samples/sec: 17.47 - lr: 0.003125\n",
      "2022-05-08 18:04:02,927 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:04:02,928 EPOCH 135 done: loss 0.0054 - lr 0.003125\n",
      "2022-05-08 18:04:02,928 Epoch   135: reducing learning rate of group 0 to 1.5625e-03.\n",
      "2022-05-08 18:04:02,929 BAD EPOCHS (no improvement): 4\n",
      "2022-05-08 18:04:05,402 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:04:05,403 train mode resetting embeddings\n",
      "2022-05-08 18:04:05,410 train mode resetting embeddings\n",
      "2022-05-08 18:04:13,159 epoch 136 - iter 37/373 - loss 0.00696612 - samples/sec: 19.12 - lr: 0.001563\n",
      "2022-05-08 18:04:20,560 epoch 136 - iter 74/373 - loss 0.00599420 - samples/sec: 20.22 - lr: 0.001563\n",
      "2022-05-08 18:04:26,668 epoch 136 - iter 111/373 - loss 0.00559304 - samples/sec: 24.56 - lr: 0.001563\n",
      "2022-05-08 18:04:34,262 epoch 136 - iter 148/373 - loss 0.00519644 - samples/sec: 19.68 - lr: 0.001563\n",
      "2022-05-08 18:04:42,289 epoch 136 - iter 185/373 - loss 0.00474193 - samples/sec: 18.61 - lr: 0.001563\n",
      "2022-05-08 18:04:49,459 epoch 136 - iter 222/373 - loss 0.00517456 - samples/sec: 20.85 - lr: 0.001563\n",
      "2022-05-08 18:04:57,628 epoch 136 - iter 259/373 - loss 0.00513651 - samples/sec: 18.30 - lr: 0.001563\n",
      "2022-05-08 18:05:05,748 epoch 136 - iter 296/373 - loss 0.00509526 - samples/sec: 18.39 - lr: 0.001563\n",
      "2022-05-08 18:05:13,953 epoch 136 - iter 333/373 - loss 0.00520939 - samples/sec: 18.21 - lr: 0.001563\n",
      "2022-05-08 18:05:21,985 epoch 136 - iter 370/373 - loss 0.00509226 - samples/sec: 18.61 - lr: 0.001563\n",
      "2022-05-08 18:05:22,411 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:05:22,412 EPOCH 136 done: loss 0.0051 - lr 0.001563\n",
      "2022-05-08 18:05:22,413 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 18:05:24,676 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:05:24,677 train mode resetting embeddings\n",
      "2022-05-08 18:05:24,683 train mode resetting embeddings\n",
      "2022-05-08 18:05:33,107 epoch 137 - iter 37/373 - loss 0.00562826 - samples/sec: 17.59 - lr: 0.001563\n",
      "2022-05-08 18:05:41,417 epoch 137 - iter 74/373 - loss 0.00562484 - samples/sec: 17.97 - lr: 0.001563\n",
      "2022-05-08 18:05:48,316 epoch 137 - iter 111/373 - loss 0.00570169 - samples/sec: 21.71 - lr: 0.001563\n",
      "2022-05-08 18:05:55,957 epoch 137 - iter 148/373 - loss 0.00558450 - samples/sec: 19.56 - lr: 0.001563\n",
      "2022-05-08 18:06:03,647 epoch 137 - iter 185/373 - loss 0.00532163 - samples/sec: 19.44 - lr: 0.001563\n",
      "2022-05-08 18:06:11,806 epoch 137 - iter 222/373 - loss 0.00558806 - samples/sec: 18.32 - lr: 0.001563\n",
      "2022-05-08 18:06:18,431 epoch 137 - iter 259/373 - loss 0.00551682 - samples/sec: 22.59 - lr: 0.001563\n",
      "2022-05-08 18:06:25,205 epoch 137 - iter 296/373 - loss 0.00558911 - samples/sec: 22.08 - lr: 0.001563\n",
      "2022-05-08 18:06:33,160 epoch 137 - iter 333/373 - loss 0.00586566 - samples/sec: 18.78 - lr: 0.001563\n",
      "2022-05-08 18:06:40,317 epoch 137 - iter 370/373 - loss 0.00560086 - samples/sec: 20.89 - lr: 0.001563\n",
      "2022-05-08 18:06:40,756 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:06:40,757 EPOCH 137 done: loss 0.0056 - lr 0.001563\n",
      "2022-05-08 18:06:40,759 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 18:06:43,284 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:06:43,285 train mode resetting embeddings\n",
      "2022-05-08 18:06:43,291 train mode resetting embeddings\n",
      "2022-05-08 18:06:51,480 epoch 138 - iter 37/373 - loss 0.00657412 - samples/sec: 18.09 - lr: 0.001563\n",
      "2022-05-08 18:06:58,072 epoch 138 - iter 74/373 - loss 0.00643189 - samples/sec: 22.70 - lr: 0.001563\n",
      "2022-05-08 18:07:05,105 epoch 138 - iter 111/373 - loss 0.00570983 - samples/sec: 21.28 - lr: 0.001563\n",
      "2022-05-08 18:07:13,164 epoch 138 - iter 148/373 - loss 0.00578365 - samples/sec: 18.55 - lr: 0.001563\n",
      "2022-05-08 18:07:21,154 epoch 138 - iter 185/373 - loss 0.00555940 - samples/sec: 18.71 - lr: 0.001563\n",
      "2022-05-08 18:07:28,995 epoch 138 - iter 222/373 - loss 0.00526033 - samples/sec: 19.08 - lr: 0.001563\n",
      "2022-05-08 18:07:35,688 epoch 138 - iter 259/373 - loss 0.00509450 - samples/sec: 22.39 - lr: 0.001563\n",
      "2022-05-08 18:07:45,028 epoch 138 - iter 296/373 - loss 0.00509329 - samples/sec: 15.99 - lr: 0.001563\n",
      "2022-05-08 18:07:53,854 epoch 138 - iter 333/373 - loss 0.00498136 - samples/sec: 16.92 - lr: 0.001563\n",
      "2022-05-08 18:08:02,584 epoch 138 - iter 370/373 - loss 0.00510679 - samples/sec: 17.15 - lr: 0.001563\n",
      "2022-05-08 18:08:03,171 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:08:03,172 EPOCH 138 done: loss 0.0051 - lr 0.001563\n",
      "2022-05-08 18:08:03,172 BAD EPOCHS (no improvement): 3\n",
      "2022-05-08 18:08:05,538 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:08:05,539 train mode resetting embeddings\n",
      "2022-05-08 18:08:05,544 train mode resetting embeddings\n",
      "2022-05-08 18:08:13,496 epoch 139 - iter 37/373 - loss 0.00811191 - samples/sec: 18.64 - lr: 0.001563\n",
      "2022-05-08 18:08:21,866 epoch 139 - iter 74/373 - loss 0.00587028 - samples/sec: 17.86 - lr: 0.001563\n",
      "2022-05-08 18:08:30,461 epoch 139 - iter 111/373 - loss 0.00568982 - samples/sec: 17.37 - lr: 0.001563\n",
      "2022-05-08 18:08:38,582 epoch 139 - iter 148/373 - loss 0.00503823 - samples/sec: 18.39 - lr: 0.001563\n",
      "2022-05-08 18:08:46,488 epoch 139 - iter 185/373 - loss 0.00487138 - samples/sec: 18.91 - lr: 0.001563\n",
      "2022-05-08 18:08:54,077 epoch 139 - iter 222/373 - loss 0.00512836 - samples/sec: 19.73 - lr: 0.001563\n",
      "2022-05-08 18:09:01,533 epoch 139 - iter 259/373 - loss 0.00525201 - samples/sec: 20.04 - lr: 0.001563\n",
      "2022-05-08 18:09:10,169 epoch 139 - iter 296/373 - loss 0.00538352 - samples/sec: 17.29 - lr: 0.001563\n",
      "2022-05-08 18:09:17,697 epoch 139 - iter 333/373 - loss 0.00565695 - samples/sec: 19.87 - lr: 0.001563\n",
      "2022-05-08 18:09:25,205 epoch 139 - iter 370/373 - loss 0.00539738 - samples/sec: 19.91 - lr: 0.001563\n",
      "2022-05-08 18:09:26,104 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:09:26,105 EPOCH 139 done: loss 0.0054 - lr 0.001563\n",
      "2022-05-08 18:09:26,106 Epoch   139: reducing learning rate of group 0 to 7.8125e-04.\n",
      "2022-05-08 18:09:26,106 BAD EPOCHS (no improvement): 4\n",
      "2022-05-08 18:09:28,783 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:09:28,784 train mode resetting embeddings\n",
      "2022-05-08 18:09:28,792 train mode resetting embeddings\n",
      "2022-05-08 18:09:36,081 epoch 140 - iter 37/373 - loss 0.00676640 - samples/sec: 20.33 - lr: 0.000781\n",
      "2022-05-08 18:09:42,604 epoch 140 - iter 74/373 - loss 0.00597345 - samples/sec: 22.95 - lr: 0.000781\n",
      "2022-05-08 18:09:50,314 epoch 140 - iter 111/373 - loss 0.00576681 - samples/sec: 19.38 - lr: 0.000781\n",
      "2022-05-08 18:09:58,583 epoch 140 - iter 148/373 - loss 0.00527893 - samples/sec: 18.10 - lr: 0.000781\n",
      "2022-05-08 18:10:06,552 epoch 140 - iter 185/373 - loss 0.00520257 - samples/sec: 18.75 - lr: 0.000781\n",
      "2022-05-08 18:10:14,775 epoch 140 - iter 222/373 - loss 0.00541911 - samples/sec: 18.16 - lr: 0.000781\n",
      "2022-05-08 18:10:23,515 epoch 140 - iter 259/373 - loss 0.00547687 - samples/sec: 17.09 - lr: 0.000781\n",
      "2022-05-08 18:10:32,282 epoch 140 - iter 296/373 - loss 0.00532720 - samples/sec: 17.03 - lr: 0.000781\n",
      "2022-05-08 18:10:40,859 epoch 140 - iter 333/373 - loss 0.00527893 - samples/sec: 17.42 - lr: 0.000781\n",
      "2022-05-08 18:10:48,635 epoch 140 - iter 370/373 - loss 0.00520479 - samples/sec: 19.24 - lr: 0.000781\n",
      "2022-05-08 18:10:49,191 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:10:49,192 EPOCH 140 done: loss 0.0053 - lr 0.000781\n",
      "2022-05-08 18:10:49,193 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 18:10:51,510 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:10:51,511 train mode resetting embeddings\n",
      "2022-05-08 18:10:51,517 train mode resetting embeddings\n",
      "2022-05-08 18:10:59,189 epoch 141 - iter 37/373 - loss 0.00562511 - samples/sec: 19.31 - lr: 0.000781\n",
      "2022-05-08 18:11:07,048 epoch 141 - iter 74/373 - loss 0.00737209 - samples/sec: 19.02 - lr: 0.000781\n",
      "2022-05-08 18:11:15,060 epoch 141 - iter 111/373 - loss 0.00653909 - samples/sec: 18.64 - lr: 0.000781\n",
      "2022-05-08 18:11:24,090 epoch 141 - iter 148/373 - loss 0.00549369 - samples/sec: 16.54 - lr: 0.000781\n",
      "2022-05-08 18:11:31,543 epoch 141 - iter 185/373 - loss 0.00548625 - samples/sec: 20.07 - lr: 0.000781\n",
      "2022-05-08 18:11:39,004 epoch 141 - iter 222/373 - loss 0.00569355 - samples/sec: 20.03 - lr: 0.000781\n",
      "2022-05-08 18:11:47,326 epoch 141 - iter 259/373 - loss 0.00582748 - samples/sec: 17.98 - lr: 0.000781\n",
      "2022-05-08 18:11:55,310 epoch 141 - iter 296/373 - loss 0.00566197 - samples/sec: 18.72 - lr: 0.000781\n",
      "2022-05-08 18:12:02,567 epoch 141 - iter 333/373 - loss 0.00556402 - samples/sec: 20.62 - lr: 0.000781\n",
      "2022-05-08 18:12:09,998 epoch 141 - iter 370/373 - loss 0.00544825 - samples/sec: 20.13 - lr: 0.000781\n",
      "2022-05-08 18:12:10,660 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:12:10,661 EPOCH 141 done: loss 0.0054 - lr 0.000781\n",
      "2022-05-08 18:12:10,661 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 18:12:13,207 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:12:13,208 train mode resetting embeddings\n",
      "2022-05-08 18:12:13,214 train mode resetting embeddings\n",
      "2022-05-08 18:12:20,722 epoch 142 - iter 37/373 - loss 0.00659784 - samples/sec: 19.73 - lr: 0.000781\n",
      "2022-05-08 18:12:28,202 epoch 142 - iter 74/373 - loss 0.00540212 - samples/sec: 19.98 - lr: 0.000781\n",
      "2022-05-08 18:12:36,057 epoch 142 - iter 111/373 - loss 0.00534054 - samples/sec: 19.03 - lr: 0.000781\n",
      "2022-05-08 18:12:43,599 epoch 142 - iter 148/373 - loss 0.00517092 - samples/sec: 19.85 - lr: 0.000781\n",
      "2022-05-08 18:12:50,176 epoch 142 - iter 185/373 - loss 0.00541015 - samples/sec: 22.78 - lr: 0.000781\n",
      "2022-05-08 18:12:58,105 epoch 142 - iter 222/373 - loss 0.00498982 - samples/sec: 18.87 - lr: 0.000781\n",
      "2022-05-08 18:13:05,069 epoch 142 - iter 259/373 - loss 0.00504104 - samples/sec: 21.50 - lr: 0.000781\n",
      "2022-05-08 18:13:12,457 epoch 142 - iter 296/373 - loss 0.00494452 - samples/sec: 20.23 - lr: 0.000781\n",
      "2022-05-08 18:13:20,385 epoch 142 - iter 333/373 - loss 0.00512273 - samples/sec: 18.84 - lr: 0.000781\n",
      "2022-05-08 18:13:27,863 epoch 142 - iter 370/373 - loss 0.00518972 - samples/sec: 19.98 - lr: 0.000781\n",
      "2022-05-08 18:13:28,924 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:13:28,926 EPOCH 142 done: loss 0.0052 - lr 0.000781\n",
      "2022-05-08 18:13:28,926 BAD EPOCHS (no improvement): 3\n",
      "2022-05-08 18:13:31,172 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:13:31,174 train mode resetting embeddings\n",
      "2022-05-08 18:13:31,179 train mode resetting embeddings\n",
      "2022-05-08 18:13:37,747 epoch 143 - iter 37/373 - loss 0.00508366 - samples/sec: 22.56 - lr: 0.000781\n",
      "2022-05-08 18:13:45,000 epoch 143 - iter 74/373 - loss 0.00569520 - samples/sec: 20.63 - lr: 0.000781\n",
      "2022-05-08 18:13:52,658 epoch 143 - iter 111/373 - loss 0.00506019 - samples/sec: 19.52 - lr: 0.000781\n",
      "2022-05-08 18:14:00,315 epoch 143 - iter 148/373 - loss 0.00471373 - samples/sec: 19.53 - lr: 0.000781\n",
      "2022-05-08 18:14:07,442 epoch 143 - iter 185/373 - loss 0.00525388 - samples/sec: 21.01 - lr: 0.000781\n",
      "2022-05-08 18:14:14,908 epoch 143 - iter 222/373 - loss 0.00520613 - samples/sec: 20.03 - lr: 0.000781\n",
      "2022-05-08 18:14:23,035 epoch 143 - iter 259/373 - loss 0.00520749 - samples/sec: 18.39 - lr: 0.000781\n",
      "2022-05-08 18:14:30,924 epoch 143 - iter 296/373 - loss 0.00505148 - samples/sec: 18.95 - lr: 0.000781\n",
      "2022-05-08 18:14:39,131 epoch 143 - iter 333/373 - loss 0.00491812 - samples/sec: 18.21 - lr: 0.000781\n",
      "2022-05-08 18:14:46,897 epoch 143 - iter 370/373 - loss 0.00472934 - samples/sec: 19.24 - lr: 0.000781\n",
      "2022-05-08 18:14:47,373 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:14:47,374 EPOCH 143 done: loss 0.0048 - lr 0.000781\n",
      "2022-05-08 18:14:47,374 Epoch   143: reducing learning rate of group 0 to 3.9063e-04.\n",
      "2022-05-08 18:14:47,375 BAD EPOCHS (no improvement): 4\n",
      "2022-05-08 18:14:49,935 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:14:49,936 train mode resetting embeddings\n",
      "2022-05-08 18:14:49,942 train mode resetting embeddings\n",
      "2022-05-08 18:14:57,079 epoch 144 - iter 37/373 - loss 0.00473991 - samples/sec: 20.76 - lr: 0.000391\n",
      "2022-05-08 18:15:03,586 epoch 144 - iter 74/373 - loss 0.00471126 - samples/sec: 23.04 - lr: 0.000391\n",
      "2022-05-08 18:15:11,840 epoch 144 - iter 111/373 - loss 0.00556845 - samples/sec: 18.10 - lr: 0.000391\n",
      "2022-05-08 18:15:19,086 epoch 144 - iter 148/373 - loss 0.00575023 - samples/sec: 20.63 - lr: 0.000391\n",
      "2022-05-08 18:15:26,992 epoch 144 - iter 185/373 - loss 0.00543788 - samples/sec: 18.90 - lr: 0.000391\n",
      "2022-05-08 18:15:35,309 epoch 144 - iter 222/373 - loss 0.00557154 - samples/sec: 17.97 - lr: 0.000391\n",
      "2022-05-08 18:15:42,053 epoch 144 - iter 259/373 - loss 0.00589975 - samples/sec: 22.20 - lr: 0.000391\n",
      "2022-05-08 18:15:49,665 epoch 144 - iter 296/373 - loss 0.00578229 - samples/sec: 19.62 - lr: 0.000391\n",
      "2022-05-08 18:15:57,865 epoch 144 - iter 333/373 - loss 0.00562295 - samples/sec: 18.22 - lr: 0.000391\n",
      "2022-05-08 18:16:04,968 epoch 144 - iter 370/373 - loss 0.00556700 - samples/sec: 21.06 - lr: 0.000391\n",
      "2022-05-08 18:16:05,838 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:16:05,839 EPOCH 144 done: loss 0.0055 - lr 0.000391\n",
      "2022-05-08 18:16:05,840 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 18:16:08,139 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:16:08,141 train mode resetting embeddings\n",
      "2022-05-08 18:16:08,149 train mode resetting embeddings\n",
      "2022-05-08 18:16:16,301 epoch 145 - iter 37/373 - loss 0.00570184 - samples/sec: 18.18 - lr: 0.000391\n",
      "2022-05-08 18:16:23,498 epoch 145 - iter 74/373 - loss 0.00503661 - samples/sec: 20.81 - lr: 0.000391\n",
      "2022-05-08 18:16:32,000 epoch 145 - iter 111/373 - loss 0.00529406 - samples/sec: 17.56 - lr: 0.000391\n",
      "2022-05-08 18:16:39,660 epoch 145 - iter 148/373 - loss 0.00550096 - samples/sec: 19.51 - lr: 0.000391\n",
      "2022-05-08 18:16:45,814 epoch 145 - iter 185/373 - loss 0.00547152 - samples/sec: 24.34 - lr: 0.000391\n",
      "2022-05-08 18:16:55,299 epoch 145 - iter 222/373 - loss 0.00550531 - samples/sec: 15.72 - lr: 0.000391\n",
      "2022-05-08 18:17:02,636 epoch 145 - iter 259/373 - loss 0.00606702 - samples/sec: 20.38 - lr: 0.000391\n",
      "2022-05-08 18:17:11,022 epoch 145 - iter 296/373 - loss 0.00598029 - samples/sec: 17.79 - lr: 0.000391\n",
      "2022-05-08 18:17:17,791 epoch 145 - iter 333/373 - loss 0.00567661 - samples/sec: 22.09 - lr: 0.000391\n",
      "2022-05-08 18:17:25,049 epoch 145 - iter 370/373 - loss 0.00570512 - samples/sec: 20.61 - lr: 0.000391\n",
      "2022-05-08 18:17:25,533 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:17:25,534 EPOCH 145 done: loss 0.0057 - lr 0.000391\n",
      "2022-05-08 18:17:25,534 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 18:17:28,086 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:17:28,087 train mode resetting embeddings\n",
      "2022-05-08 18:17:28,093 train mode resetting embeddings\n",
      "2022-05-08 18:17:35,270 epoch 146 - iter 37/373 - loss 0.00681969 - samples/sec: 20.65 - lr: 0.000391\n",
      "2022-05-08 18:17:41,844 epoch 146 - iter 74/373 - loss 0.00682078 - samples/sec: 22.78 - lr: 0.000391\n",
      "2022-05-08 18:17:48,775 epoch 146 - iter 111/373 - loss 0.00615424 - samples/sec: 21.60 - lr: 0.000391\n",
      "2022-05-08 18:17:55,791 epoch 146 - iter 148/373 - loss 0.00644372 - samples/sec: 21.32 - lr: 0.000391\n",
      "2022-05-08 18:18:03,955 epoch 146 - iter 185/373 - loss 0.00641763 - samples/sec: 18.31 - lr: 0.000391\n",
      "2022-05-08 18:18:13,445 epoch 146 - iter 222/373 - loss 0.00604504 - samples/sec: 15.71 - lr: 0.000391\n",
      "2022-05-08 18:18:20,937 epoch 146 - iter 259/373 - loss 0.00581690 - samples/sec: 19.96 - lr: 0.000391\n",
      "2022-05-08 18:18:28,406 epoch 146 - iter 296/373 - loss 0.00584989 - samples/sec: 20.03 - lr: 0.000391\n",
      "2022-05-08 18:18:35,825 epoch 146 - iter 333/373 - loss 0.00572927 - samples/sec: 20.16 - lr: 0.000391\n",
      "2022-05-08 18:18:43,258 epoch 146 - iter 370/373 - loss 0.00549330 - samples/sec: 20.10 - lr: 0.000391\n",
      "2022-05-08 18:18:43,781 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:18:43,781 EPOCH 146 done: loss 0.0055 - lr 0.000391\n",
      "2022-05-08 18:18:43,782 BAD EPOCHS (no improvement): 3\n",
      "2022-05-08 18:18:46,016 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:18:46,016 train mode resetting embeddings\n",
      "2022-05-08 18:18:46,023 train mode resetting embeddings\n",
      "2022-05-08 18:18:54,010 epoch 147 - iter 37/373 - loss 0.00527768 - samples/sec: 18.55 - lr: 0.000391\n",
      "2022-05-08 18:19:01,228 epoch 147 - iter 74/373 - loss 0.00500338 - samples/sec: 20.72 - lr: 0.000391\n",
      "2022-05-08 18:19:08,894 epoch 147 - iter 111/373 - loss 0.00564012 - samples/sec: 19.50 - lr: 0.000391\n",
      "2022-05-08 18:19:16,791 epoch 147 - iter 148/373 - loss 0.00550101 - samples/sec: 18.92 - lr: 0.000391\n",
      "2022-05-08 18:19:22,913 epoch 147 - iter 185/373 - loss 0.00507593 - samples/sec: 24.47 - lr: 0.000391\n",
      "2022-05-08 18:19:30,192 epoch 147 - iter 222/373 - loss 0.00577588 - samples/sec: 20.55 - lr: 0.000391\n",
      "2022-05-08 18:19:37,825 epoch 147 - iter 259/373 - loss 0.00583213 - samples/sec: 19.57 - lr: 0.000391\n",
      "2022-05-08 18:19:45,982 epoch 147 - iter 296/373 - loss 0.00570801 - samples/sec: 18.31 - lr: 0.000391\n",
      "2022-05-08 18:19:53,751 epoch 147 - iter 333/373 - loss 0.00553701 - samples/sec: 19.23 - lr: 0.000391\n",
      "2022-05-08 18:20:01,413 epoch 147 - iter 370/373 - loss 0.00554786 - samples/sec: 19.52 - lr: 0.000391\n",
      "2022-05-08 18:20:02,279 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:20:02,280 EPOCH 147 done: loss 0.0055 - lr 0.000391\n",
      "2022-05-08 18:20:02,281 Epoch   147: reducing learning rate of group 0 to 1.9531e-04.\n",
      "2022-05-08 18:20:02,282 BAD EPOCHS (no improvement): 4\n",
      "2022-05-08 18:20:04,651 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:20:04,652 train mode resetting embeddings\n",
      "2022-05-08 18:20:04,658 train mode resetting embeddings\n",
      "2022-05-08 18:20:12,264 epoch 148 - iter 37/373 - loss 0.00707882 - samples/sec: 19.49 - lr: 0.000195\n",
      "2022-05-08 18:20:20,450 epoch 148 - iter 74/373 - loss 0.00658238 - samples/sec: 18.24 - lr: 0.000195\n",
      "2022-05-08 18:20:26,468 epoch 148 - iter 111/373 - loss 0.00694300 - samples/sec: 24.88 - lr: 0.000195\n",
      "2022-05-08 18:20:34,346 epoch 148 - iter 148/373 - loss 0.00683777 - samples/sec: 18.96 - lr: 0.000195\n",
      "2022-05-08 18:20:41,447 epoch 148 - iter 185/373 - loss 0.00642004 - samples/sec: 21.05 - lr: 0.000195\n",
      "2022-05-08 18:20:48,279 epoch 148 - iter 222/373 - loss 0.00598401 - samples/sec: 21.91 - lr: 0.000195\n",
      "2022-05-08 18:20:56,279 epoch 148 - iter 259/373 - loss 0.00611468 - samples/sec: 18.67 - lr: 0.000195\n",
      "2022-05-08 18:21:04,283 epoch 148 - iter 296/373 - loss 0.00574965 - samples/sec: 18.68 - lr: 0.000195\n",
      "2022-05-08 18:21:12,142 epoch 148 - iter 333/373 - loss 0.00557737 - samples/sec: 19.02 - lr: 0.000195\n",
      "2022-05-08 18:21:19,248 epoch 148 - iter 370/373 - loss 0.00548064 - samples/sec: 21.07 - lr: 0.000195\n",
      "2022-05-08 18:21:20,289 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:21:20,291 EPOCH 148 done: loss 0.0055 - lr 0.000195\n",
      "2022-05-08 18:21:20,291 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 18:21:22,851 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:21:22,852 train mode resetting embeddings\n",
      "2022-05-08 18:21:22,859 train mode resetting embeddings\n",
      "2022-05-08 18:21:30,316 epoch 149 - iter 37/373 - loss 0.00435283 - samples/sec: 19.87 - lr: 0.000195\n",
      "2022-05-08 18:21:37,451 epoch 149 - iter 74/373 - loss 0.00443218 - samples/sec: 20.95 - lr: 0.000195\n",
      "2022-05-08 18:21:45,604 epoch 149 - iter 111/373 - loss 0.00400963 - samples/sec: 18.32 - lr: 0.000195\n",
      "2022-05-08 18:21:53,665 epoch 149 - iter 148/373 - loss 0.00420579 - samples/sec: 18.54 - lr: 0.000195\n",
      "2022-05-08 18:22:01,679 epoch 149 - iter 185/373 - loss 0.00403346 - samples/sec: 18.66 - lr: 0.000195\n",
      "2022-05-08 18:22:09,131 epoch 149 - iter 222/373 - loss 0.00424974 - samples/sec: 20.06 - lr: 0.000195\n",
      "2022-05-08 18:22:16,330 epoch 149 - iter 259/373 - loss 0.00455920 - samples/sec: 20.78 - lr: 0.000195\n",
      "2022-05-08 18:22:22,805 epoch 149 - iter 296/373 - loss 0.00450610 - samples/sec: 23.13 - lr: 0.000195\n",
      "2022-05-08 18:22:29,359 epoch 149 - iter 333/373 - loss 0.00469004 - samples/sec: 22.83 - lr: 0.000195\n",
      "2022-05-08 18:22:37,065 epoch 149 - iter 370/373 - loss 0.00460043 - samples/sec: 19.38 - lr: 0.000195\n",
      "2022-05-08 18:22:37,589 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:22:37,590 EPOCH 149 done: loss 0.0047 - lr 0.000195\n",
      "2022-05-08 18:22:37,591 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 18:22:39,844 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:22:39,845 train mode resetting embeddings\n",
      "2022-05-08 18:22:39,851 train mode resetting embeddings\n",
      "2022-05-08 18:22:47,360 epoch 150 - iter 37/373 - loss 0.00491465 - samples/sec: 19.73 - lr: 0.000195\n",
      "2022-05-08 18:22:54,460 epoch 150 - iter 74/373 - loss 0.00552479 - samples/sec: 21.08 - lr: 0.000195\n",
      "2022-05-08 18:23:01,798 epoch 150 - iter 111/373 - loss 0.00527822 - samples/sec: 20.39 - lr: 0.000195\n",
      "2022-05-08 18:23:09,552 epoch 150 - iter 148/373 - loss 0.00495429 - samples/sec: 19.29 - lr: 0.000195\n",
      "2022-05-08 18:23:17,208 epoch 150 - iter 185/373 - loss 0.00462721 - samples/sec: 19.52 - lr: 0.000195\n",
      "2022-05-08 18:23:24,785 epoch 150 - iter 222/373 - loss 0.00473761 - samples/sec: 19.75 - lr: 0.000195\n",
      "2022-05-08 18:23:32,236 epoch 150 - iter 259/373 - loss 0.00492620 - samples/sec: 20.07 - lr: 0.000195\n",
      "2022-05-08 18:23:38,863 epoch 150 - iter 296/373 - loss 0.00479872 - samples/sec: 22.58 - lr: 0.000195\n",
      "2022-05-08 18:23:47,292 epoch 150 - iter 333/373 - loss 0.00469287 - samples/sec: 17.72 - lr: 0.000195\n",
      "2022-05-08 18:23:54,799 epoch 150 - iter 370/373 - loss 0.00458328 - samples/sec: 19.91 - lr: 0.000195\n",
      "2022-05-08 18:23:55,404 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:23:55,405 EPOCH 150 done: loss 0.0046 - lr 0.000195\n",
      "2022-05-08 18:23:55,406 BAD EPOCHS (no improvement): 0\n",
      "2022-05-08 18:23:57,961 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:23:57,962 train mode resetting embeddings\n",
      "2022-05-08 18:23:57,968 train mode resetting embeddings\n",
      "2022-05-08 18:24:05,408 epoch 151 - iter 37/373 - loss 0.00898818 - samples/sec: 19.92 - lr: 0.000195\n",
      "2022-05-08 18:24:12,645 epoch 151 - iter 74/373 - loss 0.00817222 - samples/sec: 20.67 - lr: 0.000195\n",
      "2022-05-08 18:24:21,096 epoch 151 - iter 111/373 - loss 0.00725219 - samples/sec: 17.67 - lr: 0.000195\n",
      "2022-05-08 18:24:27,809 epoch 151 - iter 148/373 - loss 0.00668131 - samples/sec: 22.30 - lr: 0.000195\n",
      "2022-05-08 18:24:34,442 epoch 151 - iter 185/373 - loss 0.00628835 - samples/sec: 22.54 - lr: 0.000195\n",
      "2022-05-08 18:24:41,633 epoch 151 - iter 222/373 - loss 0.00622909 - samples/sec: 20.78 - lr: 0.000195\n",
      "2022-05-08 18:24:47,832 epoch 151 - iter 259/373 - loss 0.00624676 - samples/sec: 24.17 - lr: 0.000195\n",
      "2022-05-08 18:24:54,175 epoch 151 - iter 296/373 - loss 0.00615471 - samples/sec: 23.59 - lr: 0.000195\n",
      "2022-05-08 18:25:01,272 epoch 151 - iter 333/373 - loss 0.00587044 - samples/sec: 21.06 - lr: 0.000195\n",
      "2022-05-08 18:25:08,091 epoch 151 - iter 370/373 - loss 0.00581469 - samples/sec: 21.92 - lr: 0.000195\n",
      "2022-05-08 18:25:08,628 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:25:08,629 EPOCH 151 done: loss 0.0058 - lr 0.000195\n",
      "2022-05-08 18:25:08,630 BAD EPOCHS (no improvement): 1\n",
      "2022-05-08 18:25:10,687 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:25:10,688 train mode resetting embeddings\n",
      "2022-05-08 18:25:10,695 train mode resetting embeddings\n",
      "2022-05-08 18:25:18,368 epoch 152 - iter 37/373 - loss 0.00674962 - samples/sec: 19.31 - lr: 0.000195\n",
      "2022-05-08 18:25:26,284 epoch 152 - iter 74/373 - loss 0.00568701 - samples/sec: 18.86 - lr: 0.000195\n",
      "2022-05-08 18:25:32,577 epoch 152 - iter 111/373 - loss 0.00470583 - samples/sec: 23.78 - lr: 0.000195\n",
      "2022-05-08 18:25:39,359 epoch 152 - iter 148/373 - loss 0.00492984 - samples/sec: 22.06 - lr: 0.000195\n",
      "2022-05-08 18:25:45,335 epoch 152 - iter 185/373 - loss 0.00468594 - samples/sec: 25.04 - lr: 0.000195\n",
      "2022-05-08 18:25:52,214 epoch 152 - iter 222/373 - loss 0.00469702 - samples/sec: 21.72 - lr: 0.000195\n",
      "2022-05-08 18:25:58,094 epoch 152 - iter 259/373 - loss 0.00516209 - samples/sec: 25.46 - lr: 0.000195\n",
      "2022-05-08 18:26:04,823 epoch 152 - iter 296/373 - loss 0.00516665 - samples/sec: 22.23 - lr: 0.000195\n",
      "2022-05-08 18:26:10,981 epoch 152 - iter 333/373 - loss 0.00521518 - samples/sec: 24.32 - lr: 0.000195\n",
      "2022-05-08 18:26:17,946 epoch 152 - iter 370/373 - loss 0.00518823 - samples/sec: 21.46 - lr: 0.000195\n",
      "2022-05-08 18:26:18,467 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:26:18,468 EPOCH 152 done: loss 0.0053 - lr 0.000195\n",
      "2022-05-08 18:26:18,470 BAD EPOCHS (no improvement): 2\n",
      "2022-05-08 18:26:20,762 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:26:20,764 train mode resetting embeddings\n",
      "2022-05-08 18:26:20,769 train mode resetting embeddings\n",
      "2022-05-08 18:26:27,430 epoch 153 - iter 37/373 - loss 0.00635861 - samples/sec: 22.25 - lr: 0.000195\n",
      "2022-05-08 18:26:34,476 epoch 153 - iter 74/373 - loss 0.00485071 - samples/sec: 21.22 - lr: 0.000195\n",
      "2022-05-08 18:26:41,669 epoch 153 - iter 111/373 - loss 0.00535237 - samples/sec: 20.76 - lr: 0.000195\n",
      "2022-05-08 18:26:48,395 epoch 153 - iter 148/373 - loss 0.00488030 - samples/sec: 22.25 - lr: 0.000195\n",
      "2022-05-08 18:26:53,998 epoch 153 - iter 185/373 - loss 0.00485427 - samples/sec: 26.74 - lr: 0.000195\n",
      "2022-05-08 18:27:00,898 epoch 153 - iter 222/373 - loss 0.00497098 - samples/sec: 21.67 - lr: 0.000195\n",
      "2022-05-08 18:27:07,178 epoch 153 - iter 259/373 - loss 0.00523481 - samples/sec: 23.82 - lr: 0.000195\n",
      "2022-05-08 18:27:13,011 epoch 153 - iter 296/373 - loss 0.00526730 - samples/sec: 25.67 - lr: 0.000195\n",
      "2022-05-08 18:27:20,916 epoch 153 - iter 333/373 - loss 0.00510797 - samples/sec: 18.88 - lr: 0.000195\n",
      "2022-05-08 18:27:27,773 epoch 153 - iter 370/373 - loss 0.00514848 - samples/sec: 21.79 - lr: 0.000195\n",
      "2022-05-08 18:27:28,283 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:27:28,284 EPOCH 153 done: loss 0.0052 - lr 0.000195\n",
      "2022-05-08 18:27:28,284 BAD EPOCHS (no improvement): 3\n",
      "2022-05-08 18:27:30,314 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:27:30,314 train mode resetting embeddings\n",
      "2022-05-08 18:27:30,321 train mode resetting embeddings\n",
      "2022-05-08 18:27:37,699 epoch 154 - iter 37/373 - loss 0.00751781 - samples/sec: 20.09 - lr: 0.000195\n",
      "2022-05-08 18:27:43,999 epoch 154 - iter 74/373 - loss 0.00626356 - samples/sec: 23.75 - lr: 0.000195\n",
      "2022-05-08 18:27:50,271 epoch 154 - iter 111/373 - loss 0.00575402 - samples/sec: 23.86 - lr: 0.000195\n",
      "2022-05-08 18:27:57,055 epoch 154 - iter 148/373 - loss 0.00520489 - samples/sec: 22.04 - lr: 0.000195\n",
      "2022-05-08 18:28:03,414 epoch 154 - iter 185/373 - loss 0.00544483 - samples/sec: 23.52 - lr: 0.000195\n",
      "2022-05-08 18:28:09,714 epoch 154 - iter 222/373 - loss 0.00562130 - samples/sec: 23.76 - lr: 0.000195\n",
      "2022-05-08 18:28:15,432 epoch 154 - iter 259/373 - loss 0.00569281 - samples/sec: 26.19 - lr: 0.000195\n",
      "2022-05-08 18:28:22,829 epoch 154 - iter 296/373 - loss 0.00542101 - samples/sec: 20.19 - lr: 0.000195\n",
      "2022-05-08 18:28:29,992 epoch 154 - iter 333/373 - loss 0.00553035 - samples/sec: 20.85 - lr: 0.000195\n",
      "2022-05-08 18:28:37,876 epoch 154 - iter 370/373 - loss 0.00538252 - samples/sec: 18.94 - lr: 0.000195\n",
      "2022-05-08 18:28:38,216 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:28:38,217 EPOCH 154 done: loss 0.0054 - lr 0.000195\n",
      "2022-05-08 18:28:38,218 Epoch   154: reducing learning rate of group 0 to 9.7656e-05.\n",
      "2022-05-08 18:28:38,218 BAD EPOCHS (no improvement): 4\n",
      "2022-05-08 18:28:40,510 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:28:40,511 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:28:40,511 learning rate too small - quitting training!\n",
      "2022-05-08 18:28:40,512 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:28:42,560 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:28:42,561 Testing using last state of model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 135/135 [00:46<00:00,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-08 18:29:28,679 Evaluating as a multi-label problem: False\n",
      "2022-05-08 18:29:28,724 0.7768\t0.6135\t0.6856\t0.5251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-08 18:29:28,725 \n",
      "Results:\n",
      "- F-score (micro) 0.6856\n",
      "- F-score (macro) 0.6947\n",
      "- Accuracy 0.5251\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         DIS     0.7720    0.6034    0.6774       769\n",
      "        DRUG     0.7917    0.6468    0.7119       235\n",
      "\n",
      "   micro avg     0.7768    0.6135    0.6856      1004\n",
      "   macro avg     0.7819    0.6251    0.6947      1004\n",
      "weighted avg     0.7766    0.6135    0.6855      1004\n",
      "\n",
      "2022-05-08 18:29:28,727 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:29:28,742 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:29:28,745 WARNING: No LOSS found for test split in this data.\n",
      "2022-05-08 18:29:28,749 Are you sure you want to plot LOSS and not another value?\n",
      "2022-05-08 18:29:28,752 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:29:28,811 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-08 18:29:28,811 WARNING: No F1 found for test split in this data.\n",
      "2022-05-08 18:29:28,812 Are you sure you want to plot F1 and not another value?\n",
      "2022-05-08 18:29:28,813 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-08 18:29:29,448 Loss and F1 plots are saved in ..\\resources\\taggers\\FA_MedRed_pooled-flair\\training.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = 'MedRed'\n",
    "selected_embeddings = defaultdict(lambda: 0, {'pooled-flair':1})\n",
    "train(model, selected_embeddings)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9c1d87ea85a731d0848cf4e607dbb57360506d747d84c6cb1b30d4e34e6c1fb7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
