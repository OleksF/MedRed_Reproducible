{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the trained model to generate embeddings for new sentences, from the 500k Reddit dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment config. Note that the spacy model en_core_web_lg needs to first be downloaded separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import SequenceTagger\n",
    "from flair.data import Sentence \n",
    "import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # included for convenience to help find correct paths\n",
    "# import os\n",
    "# os.getcwd()\n",
    "# os.listdir(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top level project directory containing code, data, .gitignore, etc\n",
    "MEDRED_REPRODUCIBLE_DIR = \"../\"\n",
    "REDDIT_IN = MEDRED_REPRODUCIBLE_DIR + \"data/validation/Reddit/all_sr.csv\"\n",
    "REDDIT_SYMPTOM_TAGGED_TOKENS_OUT = MEDRED_REPRODUCIBLE_DIR + \"data/validation/Reddit/NER_Reddit_pred_dis.csv\"\n",
    "REDDIT_DRUG_TAGGED_TOKENS_OUT = MEDRED_REPRODUCIBLE_DIR + \"data/validation/Reddit/NER_Reddit_pred_drug.csv\"\n",
    "EMBEDDING_MODEL_PATH = MEDRED_REPRODUCIBLE_DIR + \"resources/taggers/FA_MedRed_glove_roberta/final-model.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model_path):\n",
    "\t'''\n",
    "\tMakes predictions on the Reddit 500k corpus given a model.\n",
    "\tOutputs two tagged files by type:\n",
    "\t1. *_dis.csv for symptoms\n",
    "\t2. *_drug.csv for drugs\n",
    "\t'''\n",
    "\n",
    "\t# load the model you trained\n",
    "\tmodel = SequenceTagger.load(model_path)\n",
    "\t# load data\n",
    "\tdata = pd.read_csv(REDDIT_IN) # cols: ,year,month,subreddit,body,clean_body,post_index\n",
    "\t# process and write...\n",
    "\twith open(REDDIT_DRUG_TAGGED_TOKENS_OUT, 'w', encoding=\"utf-8\") as f_drug:\n",
    "\t\twith open(REDDIT_SYMPTOM_TAGGED_TOKENS_OUT, 'w', encoding=\"utf-8\") as f_dis:\n",
    "\t\t\theader = \"subreddit,post_index,matched,score,start_pos,end_pos\\n\"\n",
    "\t\t\tf_dis.write(header)\n",
    "\t\t\tf_drug.write(header)\n",
    "\t\t\t# iterate across rows (with the tqdm progress bar)\n",
    "\t\t\tfor _, row in tqdm.tqdm(data.iterrows(), total=data.shape[0]):\n",
    "\t\t\t\tsentence = Sentence(str(row['body']))\n",
    "\t\t\t\t# predict tags (generates them into sentence object)\n",
    "\t\t\t\tmodel.predict(sentence) # generates prediction into sentence object\n",
    "\t\t\t\t# write each entity\n",
    "\t\t\t\tfor el in sentence.get_spans(\"ner\"):\n",
    "\t\t\t\t\t# write to appropriate file\n",
    "\t\t\t\t\t# values written are the subreddit (), post_index (an ID)\n",
    "\t\t\t\t\t# note for future adaptation: text writes token text, not sentence (for sentence, can el.sentence.text)\n",
    "\t\t\t\t\tif el.tag == 'DIS':\n",
    "\t\t\t\t\t\tf_dis.write(row['subreddit']+','+row['post_index']+',\"'+\\\n",
    "\t\t\t\t\t\t\tel.text.replace('\\n', ' ').replace('\\t', ' ')+'\",'+str(el.score)+','+str(el.start_position)+','+str(el.end_position)+'\\n')\n",
    "\t\t\t\t\telif el.tag == 'DRUG':\n",
    "\t\t\t\t\t\tf_drug.write(row['subreddit']+','+row['post_index']+',\"'+\\\n",
    "\t\t\t\t\t\t\tel.text.replace('\\n', ' ')+'\",'+str(el.score)+','+str(el.start_position)+','+str(el.end_position)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run predictions. Takes ~6hrs 40min with a single RTX 2080Ti GPU and i7-8700k CPU.\n",
    "\n",
    "On termination, will throw StopIteration error; _do not panic!_ The marked up files should generate anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-07 15:58:31,468 loading file ../resources/taggers/FA_MedRed_glove_roberta/final-model.pt\n",
      "2022-05-07 15:58:33,785 SequenceTagger predicts: Dictionary with 11 tags: O, S-DIS, B-DIS, E-DIS, I-DIS, S-DRUG, B-DRUG, E-DRUG, I-DRUG, <START>, <STOP>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 493041/496958 [6:41:06<03:11, 20.49it/s]   \n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24136/2055245324.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEMBEDDING_MODEL_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24136/2544643425.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(model_path)\u001b[0m\n\u001b[0;32m     21\u001b[0m                                 \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'body'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                                 \u001b[1;31m# predict tags (generates them into sentence object)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m                                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# generates prediction into sentence object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m                                 \u001b[1;31m# write each entity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m                                 \u001b[1;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_spans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ner\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alex\\anaconda3\\lib\\site-packages\\flair\\models\\sequence_tagger_model.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, sentences, mini_batch_size, return_probabilities_for_all_classes, verbose, label_name, return_loss, embedding_storage_mode)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m                 \u001b[1;31m# get features from forward propagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 478\u001b[1;33m                 \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgold_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    479\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m                 \u001b[1;31m# remove previously predicted labels of this type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alex\\anaconda3\\lib\\site-packages\\flair\\models\\sequence_tagger_model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m    280\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m             \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[1;31m# make a zero-padded tensor for the whole sentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alex\\anaconda3\\lib\\site-packages\\flair\\embeddings\\token.py\u001b[0m in \u001b[0;36membed\u001b[1;34m(self, sentences, static_embeddings)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0membedding\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m             \u001b[0membedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alex\\anaconda3\\lib\\site-packages\\flair\\embeddings\\base.py\u001b[0m in \u001b[0;36membed\u001b[1;34m(self, data_points)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_everything_embedded\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_points\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatic_embeddings\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_embeddings_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_points\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata_points\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alex\\anaconda3\\lib\\site-packages\\flair\\embeddings\\base.py\u001b[0m in \u001b[0;36m_add_embeddings_internal\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m    764\u001b[0m             \u001b[0mexpanded_sentences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 766\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_embeddings_to_sentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpanded_sentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    767\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m         \u001b[1;31m# move embeddings from context back to original sentence (if using context)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alex\\anaconda3\\lib\\site-packages\\flair\\embeddings\\base.py\u001b[0m in \u001b[0;36m_add_embeddings_to_sentences\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_add_embeddings_to_sentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mSentence\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 675\u001b[1;33m         \u001b[0mtokenized_sentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_token_subtoken_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubtoken_lengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gather_tokenized_strings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m         \u001b[1;31m# encode inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alex\\anaconda3\\lib\\site-packages\\flair\\embeddings\\base.py\u001b[0m in \u001b[0;36m_gather_tokenized_strings\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m    546\u001b[0m                 \u001b[1;31m# determine into how many subtokens each token is split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m                 all_token_subtoken_lengths.append(\n\u001b[1;32m--> 548\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reconstruct_tokens_from_subtokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubtokenized_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    549\u001b[0m                 )\n\u001b[0;32m    550\u001b[0m             \u001b[0msubtoken_lengths\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubtokenized_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alex\\anaconda3\\lib\\site-packages\\flair\\embeddings\\base.py\u001b[0m in \u001b[0;36m_reconstruct_tokens_from_subtokens\u001b[1;34m(self, sentence, subtokens)\u001b[0m\n\u001b[0;32m    480\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m                     \u001b[0mtoken_subtoken_lengths\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m                     \u001b[0mtoken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m                     \u001b[0mtoken_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_processed_token_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mtoken_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predict(EMBEDDING_MODEL_PATH)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9c1d87ea85a731d0848cf4e607dbb57360506d747d84c6cb1b30d4e34e6c1fb7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
